{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“UNIKEY_COMP5046_Ass1.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/JPEN6856_COMP5046_Ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD",
        "colab_type": "text"
      },
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO",
        "colab_type": "text"
      },
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab_type": "code",
        "outputId": "4f912414-2723-49c5-9a28-d232c1b3fee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1vF3FqgBC1Y-RPefeVmY8zetdZG1jmHzT'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_train.csv')\n",
        "\n",
        "id = '1XhaV8YMuQeSwozQww8PeyiWMJfia13G6'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('imdb_test.csv')\n",
        "\n",
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"imdb_train.csv\")\n",
        "df_test = pd.read_csv(\"imdb_test.csv\")\n",
        "\n",
        "reviews_train = df_train['review'].tolist()\n",
        "sentiments_train = df_train['sentiment'].tolist()\n",
        "reviews_test = df_test['review'].tolist()\n",
        "sentiments_test = df_test['sentiment'].tolist()\n",
        "\n",
        "print(\"Training set number:\",len(reviews_train))\n",
        "print(\"Testing set number:\",len(reviews_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set number: 25000\n",
            "Testing set number: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "source": [
        "The first step when building a neural network model is getting your data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word and sentiments with an integer. We'll also want to clean it up a bit before doing that.\n",
        "\n",
        "Here are the processing steps that we'll want to take:\n",
        ">\n",
        "* Firstly, we will get rid of periods and extraneous punctuation -> In this case, punctuations cannot indicates whether a review is postive or negative, rather, it indicates the manutitudes of the sentiment. Thus, it's not useful for our purpose given we are not trying to predictive the magnutitudes, hence we will get rid of them.  \n",
        "* Secondly, we will convert all the characters to lower-case -> Upper-case and lower-case might change the meaning of a sentence, but the overall sentiment of the reviews are unlikely to be impacted by this. Thus we should treat upper and lower words indifferently, and group them together.\n",
        "* Then we will break the sentences into words and encodes them into integers.\n",
        "* Then we will break the sentiments into binary numeric classes, 1 for postive and 0 for negative\n",
        "* Then we will get rid of extremely long or short reviews -> the outliers\n",
        "\n",
        "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsIXp9W0TBez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "contractions_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "def expand_contractions(s, contractions_dict=contractions_dict):\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "    return contractions_re.sub(replace, s)\n",
        "expand_contractions('You don\\'t need a library')\n",
        "\n",
        "\n",
        "def remove_punctuation_re(x):\n",
        "    x = re.sub(r'[^\\w\\s]','',x)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m_yIj0VtFn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# get rid of punctuation and convert them to lower case for training dataset\n",
        "for i in range(len(reviews_train)):\n",
        "  review_tmp = []\n",
        "  reviews_train_temp = remove_punctuation_re(expand_contractions(reviews_train[i])).lower()\n",
        "  for word in reviews_train_temp.split():\n",
        "    review_tmp.append(word)\n",
        "  reviews_train[i] = ' '.join(review_tmp)\n",
        "  \n",
        "# get rid of punctuation and convert them to lower case for test dataset\n",
        "for i in range(len(reviews_test)):\n",
        "  review_tmp = []\n",
        "  reviews_test_temp = remove_punctuation_re(expand_contractions(reviews_test[i])).lower()\n",
        "  for word in reviews_test_temp.split():\n",
        "    review_tmp.append(word)\n",
        "  reviews_test[i] = ' '.join(review_tmp)\n",
        "  \n",
        "# create a list that contains the all the words from both training and test dataset\n",
        "reviews = reviews_test + reviews_train\n",
        "all_text = ' '.join([w for w in reviews])\n",
        "words = all_text.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Z7nuS5XQwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
        "\n",
        "## use the dict to tokenize each words in reviews.split()\n",
        "## store the tokenized reviews in reviews_ints_train & reviews_ints_test\n",
        "reviews_ints_train = []\n",
        "for review in reviews_train:\n",
        "    reviews_ints_train.append([vocab_to_int[word] for word in review.split()])\n",
        "\n",
        "reviews_ints_test = []\n",
        "for review in reviews_test:\n",
        "    reviews_ints_test.append([vocab_to_int[word] for word in review.split()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsZdDW72hF3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1=positive, 0=negative label conversion\n",
        "import numpy as np \n",
        "\n",
        "sentiments_train = np.array([1 if sentiment == 'positive' else 0 for sentiment in sentiments_train])\n",
        "sentiments_test = np.array([1 if sentiment == 'positive' else 0 for sentiment in sentiments_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-",
        "colab_type": "text"
      },
      "source": [
        "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cM4rlYkHefJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg",
        "colab_type": "text"
      },
      "source": [
        "*You are required to describe which preprocessing techniques were used with justification of your decision.*\n",
        "\n",
        "**Important**: If you are going to use the code from lab3 word2vec preprocessing. Please note that `word_list = list(set(word_list)) ` has randomness. So to make sure the word_list is the same every time you run it, you can put `word_list.sort()` after that line of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTkGebr2oEIU",
        "colab_type": "code",
        "outputId": "030d733c-07eb-4706-db19-05c7c81fbc8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Making window size 5 skip-gram\n",
        "skip_grams_target = []\n",
        "skip_grams_context = []\n",
        "vocab_size = len(vocab_to_int)\n",
        "\n",
        "for i in range(5, len(words) - 5):\n",
        "    # (context, target) : ([target index - 3, target index + 3], target)\n",
        "    target = vocab_to_int[words[i]]\n",
        "    context_temp = []\n",
        "    for word in words[i - 5:i]:\n",
        "        context_temp.append(vocab_to_int[word])\n",
        "    for word in words[i+1:i + 6]:\n",
        "        context_temp.append(vocab_to_int[word])\n",
        "    context = context_temp\n",
        "\n",
        "    # skipgrams - (target, context[0]), (target, context[1])..\n",
        "    for w in context:\n",
        "        skip_grams_target.append(target)\n",
        "        skip_grams_context.append(w)\n",
        "\n",
        "print(len(skip_grams_target))\n",
        "print(len(skip_grams_context))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "116452030\n",
            "116452030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weoNbf62wLiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Batches\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "batch_size = 1000\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(np.array(skip_grams_target)), torch.from_numpy(np.array(skip_grams_context)))\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2. Build Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS",
        "colab_type": "text"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#hyperparameters\n",
        "learning_rate = 0.005\n",
        "embedding_size = 300\n",
        "voc_size = len(vocab_to_int)\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.linear1 = nn.Linear(voc_size, embedding_size,bias=False)\n",
        "        self.linear2 = nn.Linear(embedding_size, voc_size,bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.linear1(x)\n",
        "        out = self.linear2(hidden)\n",
        "        return out\n",
        "\n",
        "skip_gram_model = SkipGram().to(device)\n",
        "criterion = nn.CrossEntropyLoss() #please note we are using \"CrossEntropyLoss\" here\n",
        "optimiser = optim.Adam(skip_gram_model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "colab_type": "code",
        "outputId": "251a5a2a-8dc3-4056-e109-6d6ec3dade47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from datetime import datetime\n",
        "print(datetime.now())\n",
        "\n",
        "data_len = len(skip_grams_target)\n",
        "iter_count = 0\n",
        "\n",
        "for epoch in range(5):\n",
        "      \n",
        "     for inputs, labels in train_loader:\n",
        "\n",
        "       iter_count += 1\n",
        "       inputs_list = []\n",
        "       \n",
        "       for i in range(len(inputs)):\n",
        "        input_temp = [0]*vocab_size\n",
        "        input_temp[inputs[i]] = 1\n",
        "        inputs_list.append(input_temp)\n",
        "       \n",
        "       inputs_array = np.array(inputs_list)\n",
        "       inputs_torch = torch.from_numpy(inputs_array).float().to(device)\n",
        "       labels_torch = labels.to(device)\n",
        "       \n",
        "       skip_gram_model.train()\n",
        "       # zero the parameter gradients\n",
        "       optimiser.zero_grad()\n",
        "\n",
        "       # forward + backward + optimize\n",
        "       outputs = skip_gram_model(inputs_torch)\n",
        "       loss = criterion(outputs, labels_torch) # We don't need to calcualte logsoftmax here\n",
        "       loss.backward()\n",
        "       optimiser.step()\n",
        "\n",
        "       #if epoch % 10 == 0: \n",
        "       print('Iteration: %d, loss: %.4f' %(iter_count, loss))\n",
        "\n",
        "print(datetime.now())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-06 02:26:08.081091\n",
            "Iteration: 1, loss: 12.1050\n",
            "Iteration: 2, loss: 12.1043\n",
            "Iteration: 3, loss: 12.1027\n",
            "Iteration: 4, loss: 12.0994\n",
            "Iteration: 5, loss: 12.0945\n",
            "Iteration: 6, loss: 12.0850\n",
            "Iteration: 7, loss: 12.0743\n",
            "Iteration: 8, loss: 12.0576\n",
            "Iteration: 9, loss: 12.0386\n",
            "Iteration: 10, loss: 12.0120\n",
            "Iteration: 11, loss: 11.9783\n",
            "Iteration: 12, loss: 11.9396\n",
            "Iteration: 13, loss: 11.8654\n",
            "Iteration: 14, loss: 11.8074\n",
            "Iteration: 15, loss: 11.7393\n",
            "Iteration: 16, loss: 11.6479\n",
            "Iteration: 17, loss: 11.5357\n",
            "Iteration: 18, loss: 11.4517\n",
            "Iteration: 19, loss: 11.2752\n",
            "Iteration: 20, loss: 11.1599\n",
            "Iteration: 21, loss: 11.0341\n",
            "Iteration: 22, loss: 10.8051\n",
            "Iteration: 23, loss: 10.6357\n",
            "Iteration: 24, loss: 10.3818\n",
            "Iteration: 25, loss: 10.2235\n",
            "Iteration: 26, loss: 10.0304\n",
            "Iteration: 27, loss: 9.6375\n",
            "Iteration: 28, loss: 9.7263\n",
            "Iteration: 29, loss: 9.3786\n",
            "Iteration: 30, loss: 9.0928\n",
            "Iteration: 31, loss: 9.0269\n",
            "Iteration: 32, loss: 9.0866\n",
            "Iteration: 33, loss: 9.0329\n",
            "Iteration: 34, loss: 8.9225\n",
            "Iteration: 35, loss: 8.9086\n",
            "Iteration: 36, loss: 9.0776\n",
            "Iteration: 37, loss: 8.7391\n",
            "Iteration: 38, loss: 8.9757\n",
            "Iteration: 39, loss: 8.8515\n",
            "Iteration: 40, loss: 8.7437\n",
            "Iteration: 41, loss: 8.5309\n",
            "Iteration: 42, loss: 8.7663\n",
            "Iteration: 43, loss: 8.5652\n",
            "Iteration: 44, loss: 8.7771\n",
            "Iteration: 45, loss: 8.5260\n",
            "Iteration: 46, loss: 8.6743\n",
            "Iteration: 47, loss: 8.5125\n",
            "Iteration: 48, loss: 8.4513\n",
            "Iteration: 49, loss: 8.5104\n",
            "Iteration: 50, loss: 8.3742\n",
            "Iteration: 51, loss: 8.4895\n",
            "Iteration: 52, loss: 8.4267\n",
            "Iteration: 53, loss: 8.2781\n",
            "Iteration: 54, loss: 8.2585\n",
            "Iteration: 55, loss: 8.3844\n",
            "Iteration: 56, loss: 8.2113\n",
            "Iteration: 57, loss: 8.2315\n",
            "Iteration: 58, loss: 8.3623\n",
            "Iteration: 59, loss: 8.2060\n",
            "Iteration: 60, loss: 8.2685\n",
            "Iteration: 61, loss: 8.1608\n",
            "Iteration: 62, loss: 8.3328\n",
            "Iteration: 63, loss: 8.2289\n",
            "Iteration: 64, loss: 8.4295\n",
            "Iteration: 65, loss: 8.0936\n",
            "Iteration: 66, loss: 8.2059\n",
            "Iteration: 67, loss: 8.3029\n",
            "Iteration: 68, loss: 7.9169\n",
            "Iteration: 69, loss: 8.1117\n",
            "Iteration: 70, loss: 8.2421\n",
            "Iteration: 71, loss: 8.0260\n",
            "Iteration: 72, loss: 8.1180\n",
            "Iteration: 73, loss: 8.1484\n",
            "Iteration: 74, loss: 7.9380\n",
            "Iteration: 75, loss: 8.2262\n",
            "Iteration: 76, loss: 8.2124\n",
            "Iteration: 77, loss: 7.8781\n",
            "Iteration: 78, loss: 8.0878\n",
            "Iteration: 79, loss: 8.1031\n",
            "Iteration: 80, loss: 7.8807\n",
            "Iteration: 81, loss: 8.0391\n",
            "Iteration: 82, loss: 8.1350\n",
            "Iteration: 83, loss: 7.9157\n",
            "Iteration: 84, loss: 7.7742\n",
            "Iteration: 85, loss: 8.0532\n",
            "Iteration: 86, loss: 7.8574\n",
            "Iteration: 87, loss: 7.8373\n",
            "Iteration: 88, loss: 8.1118\n",
            "Iteration: 89, loss: 8.0120\n",
            "Iteration: 90, loss: 8.0577\n",
            "Iteration: 91, loss: 7.7549\n",
            "Iteration: 92, loss: 7.9370\n",
            "Iteration: 93, loss: 7.8662\n",
            "Iteration: 94, loss: 7.8893\n",
            "Iteration: 95, loss: 7.8869\n",
            "Iteration: 96, loss: 7.8978\n",
            "Iteration: 97, loss: 7.9200\n",
            "Iteration: 98, loss: 7.7428\n",
            "Iteration: 99, loss: 7.7838\n",
            "Iteration: 100, loss: 7.8121\n",
            "Iteration: 101, loss: 7.9673\n",
            "Iteration: 102, loss: 7.7519\n",
            "Iteration: 103, loss: 7.8303\n",
            "Iteration: 104, loss: 7.6893\n",
            "Iteration: 105, loss: 7.8108\n",
            "Iteration: 106, loss: 7.7729\n",
            "Iteration: 107, loss: 7.7571\n",
            "Iteration: 108, loss: 7.9246\n",
            "Iteration: 109, loss: 7.7393\n",
            "Iteration: 110, loss: 7.8010\n",
            "Iteration: 111, loss: 8.0042\n",
            "Iteration: 112, loss: 7.7805\n",
            "Iteration: 113, loss: 7.8309\n",
            "Iteration: 114, loss: 7.8348\n",
            "Iteration: 115, loss: 7.8099\n",
            "Iteration: 116, loss: 7.8267\n",
            "Iteration: 117, loss: 7.6834\n",
            "Iteration: 118, loss: 7.9030\n",
            "Iteration: 119, loss: 7.7002\n",
            "Iteration: 120, loss: 7.7023\n",
            "Iteration: 121, loss: 7.9138\n",
            "Iteration: 122, loss: 7.8249\n",
            "Iteration: 123, loss: 7.8822\n",
            "Iteration: 124, loss: 7.7178\n",
            "Iteration: 125, loss: 7.7508\n",
            "Iteration: 126, loss: 7.6727\n",
            "Iteration: 127, loss: 7.6317\n",
            "Iteration: 128, loss: 7.8052\n",
            "Iteration: 129, loss: 7.6356\n",
            "Iteration: 130, loss: 7.6886\n",
            "Iteration: 131, loss: 7.8685\n",
            "Iteration: 132, loss: 7.8111\n",
            "Iteration: 133, loss: 7.7642\n",
            "Iteration: 134, loss: 7.6905\n",
            "Iteration: 135, loss: 7.6919\n",
            "Iteration: 136, loss: 7.6089\n",
            "Iteration: 137, loss: 7.7062\n",
            "Iteration: 138, loss: 7.8395\n",
            "Iteration: 139, loss: 7.5856\n",
            "Iteration: 140, loss: 7.8698\n",
            "Iteration: 141, loss: 7.5468\n",
            "Iteration: 142, loss: 7.6699\n",
            "Iteration: 143, loss: 7.6036\n",
            "Iteration: 144, loss: 7.7733\n",
            "Iteration: 145, loss: 7.6137\n",
            "Iteration: 146, loss: 7.7647\n",
            "Iteration: 147, loss: 7.6428\n",
            "Iteration: 148, loss: 7.6473\n",
            "Iteration: 149, loss: 7.8790\n",
            "Iteration: 150, loss: 7.7881\n",
            "Iteration: 151, loss: 7.8027\n",
            "Iteration: 152, loss: 7.7205\n",
            "Iteration: 153, loss: 7.6261\n",
            "Iteration: 154, loss: 7.8084\n",
            "Iteration: 155, loss: 7.8436\n",
            "Iteration: 156, loss: 7.7446\n",
            "Iteration: 157, loss: 7.8207\n",
            "Iteration: 158, loss: 7.7727\n",
            "Iteration: 159, loss: 7.5926\n",
            "Iteration: 160, loss: 7.5704\n",
            "Iteration: 161, loss: 7.7645\n",
            "Iteration: 162, loss: 7.6038\n",
            "Iteration: 163, loss: 7.7678\n",
            "Iteration: 164, loss: 7.6663\n",
            "Iteration: 165, loss: 7.7901\n",
            "Iteration: 166, loss: 7.7275\n",
            "Iteration: 167, loss: 7.6237\n",
            "Iteration: 168, loss: 7.7259\n",
            "Iteration: 169, loss: 7.8762\n",
            "Iteration: 170, loss: 7.6526\n",
            "Iteration: 171, loss: 7.6955\n",
            "Iteration: 172, loss: 7.8518\n",
            "Iteration: 173, loss: 7.5792\n",
            "Iteration: 174, loss: 7.7375\n",
            "Iteration: 175, loss: 7.7193\n",
            "Iteration: 176, loss: 7.8128\n",
            "Iteration: 177, loss: 7.5116\n",
            "Iteration: 178, loss: 7.6448\n",
            "Iteration: 179, loss: 7.7894\n",
            "Iteration: 180, loss: 7.5542\n",
            "Iteration: 181, loss: 7.4989\n",
            "Iteration: 182, loss: 7.5530\n",
            "Iteration: 183, loss: 7.6263\n",
            "Iteration: 184, loss: 7.5557\n",
            "Iteration: 185, loss: 7.4856\n",
            "Iteration: 186, loss: 7.4500\n",
            "Iteration: 187, loss: 7.5943\n",
            "Iteration: 188, loss: 7.6870\n",
            "Iteration: 189, loss: 7.6721\n",
            "Iteration: 190, loss: 7.6979\n",
            "Iteration: 191, loss: 7.5064\n",
            "Iteration: 192, loss: 7.5883\n",
            "Iteration: 193, loss: 7.5309\n",
            "Iteration: 194, loss: 7.6804\n",
            "Iteration: 195, loss: 7.5523\n",
            "Iteration: 196, loss: 7.8361\n",
            "Iteration: 197, loss: 7.7473\n",
            "Iteration: 198, loss: 7.5900\n",
            "Iteration: 199, loss: 7.7008\n",
            "Iteration: 200, loss: 7.7160\n",
            "Iteration: 201, loss: 7.7342\n",
            "Iteration: 202, loss: 7.7520\n",
            "Iteration: 203, loss: 7.7374\n",
            "Iteration: 204, loss: 7.4355\n",
            "Iteration: 205, loss: 7.5762\n",
            "Iteration: 206, loss: 7.4032\n",
            "Iteration: 207, loss: 7.8358\n",
            "Iteration: 208, loss: 7.5312\n",
            "Iteration: 209, loss: 7.7148\n",
            "Iteration: 210, loss: 7.5679\n",
            "Iteration: 211, loss: 7.6903\n",
            "Iteration: 212, loss: 7.4362\n",
            "Iteration: 213, loss: 7.7747\n",
            "Iteration: 214, loss: 7.7547\n",
            "Iteration: 215, loss: 7.8556\n",
            "Iteration: 216, loss: 7.5406\n",
            "Iteration: 217, loss: 7.6811\n",
            "Iteration: 218, loss: 7.6089\n",
            "Iteration: 219, loss: 7.7705\n",
            "Iteration: 220, loss: 7.6649\n",
            "Iteration: 221, loss: 7.7178\n",
            "Iteration: 222, loss: 7.6391\n",
            "Iteration: 223, loss: 7.7398\n",
            "Iteration: 224, loss: 7.6678\n",
            "Iteration: 225, loss: 7.5915\n",
            "Iteration: 226, loss: 7.4002\n",
            "Iteration: 227, loss: 7.8044\n",
            "Iteration: 228, loss: 7.6709\n",
            "Iteration: 229, loss: 7.4753\n",
            "Iteration: 230, loss: 7.7495\n",
            "Iteration: 231, loss: 7.6805\n",
            "Iteration: 232, loss: 7.4557\n",
            "Iteration: 233, loss: 7.5072\n",
            "Iteration: 234, loss: 7.5791\n",
            "Iteration: 235, loss: 7.6022\n",
            "Iteration: 236, loss: 7.6946\n",
            "Iteration: 237, loss: 7.5264\n",
            "Iteration: 238, loss: 7.6409\n",
            "Iteration: 239, loss: 7.7181\n",
            "Iteration: 240, loss: 7.4846\n",
            "Iteration: 241, loss: 7.6185\n",
            "Iteration: 242, loss: 7.7153\n",
            "Iteration: 243, loss: 7.5687\n",
            "Iteration: 244, loss: 7.4796\n",
            "Iteration: 245, loss: 7.5777\n",
            "Iteration: 246, loss: 7.4490\n",
            "Iteration: 247, loss: 7.3557\n",
            "Iteration: 248, loss: 7.5565\n",
            "Iteration: 249, loss: 7.6691\n",
            "Iteration: 250, loss: 7.8011\n",
            "Iteration: 251, loss: 7.6382\n",
            "Iteration: 252, loss: 7.6224\n",
            "Iteration: 253, loss: 7.6317\n",
            "Iteration: 254, loss: 7.5441\n",
            "Iteration: 255, loss: 7.8322\n",
            "Iteration: 256, loss: 7.5965\n",
            "Iteration: 257, loss: 7.6030\n",
            "Iteration: 258, loss: 7.6080\n",
            "Iteration: 259, loss: 7.8052\n",
            "Iteration: 260, loss: 7.6226\n",
            "Iteration: 261, loss: 7.3356\n",
            "Iteration: 262, loss: 7.6353\n",
            "Iteration: 263, loss: 7.7118\n",
            "Iteration: 264, loss: 7.4630\n",
            "Iteration: 265, loss: 7.6725\n",
            "Iteration: 266, loss: 7.5365\n",
            "Iteration: 267, loss: 7.8321\n",
            "Iteration: 268, loss: 7.4904\n",
            "Iteration: 269, loss: 7.6857\n",
            "Iteration: 270, loss: 7.5401\n",
            "Iteration: 271, loss: 7.3534\n",
            "Iteration: 272, loss: 7.4610\n",
            "Iteration: 273, loss: 7.6255\n",
            "Iteration: 274, loss: 7.6439\n",
            "Iteration: 275, loss: 7.5885\n",
            "Iteration: 276, loss: 7.5399\n",
            "Iteration: 277, loss: 7.5118\n",
            "Iteration: 278, loss: 7.7881\n",
            "Iteration: 279, loss: 7.4217\n",
            "Iteration: 280, loss: 7.7652\n",
            "Iteration: 281, loss: 7.5965\n",
            "Iteration: 282, loss: 7.5064\n",
            "Iteration: 283, loss: 7.4741\n",
            "Iteration: 284, loss: 7.5755\n",
            "Iteration: 285, loss: 7.5368\n",
            "Iteration: 286, loss: 7.7513\n",
            "Iteration: 287, loss: 7.6235\n",
            "Iteration: 288, loss: 7.7347\n",
            "Iteration: 289, loss: 7.7033\n",
            "Iteration: 290, loss: 7.4981\n",
            "Iteration: 291, loss: 7.4821\n",
            "Iteration: 292, loss: 7.6439\n",
            "Iteration: 293, loss: 7.7500\n",
            "Iteration: 294, loss: 7.8771\n",
            "Iteration: 295, loss: 7.5882\n",
            "Iteration: 296, loss: 7.6602\n",
            "Iteration: 297, loss: 7.5685\n",
            "Iteration: 298, loss: 7.4674\n",
            "Iteration: 299, loss: 7.7317\n",
            "Iteration: 300, loss: 7.4673\n",
            "Iteration: 301, loss: 7.4470\n",
            "Iteration: 302, loss: 7.6751\n",
            "Iteration: 303, loss: 7.4247\n",
            "Iteration: 304, loss: 7.5395\n",
            "Iteration: 305, loss: 7.6374\n",
            "Iteration: 306, loss: 7.5144\n",
            "Iteration: 307, loss: 7.5211\n",
            "Iteration: 308, loss: 7.8362\n",
            "Iteration: 309, loss: 7.6676\n",
            "Iteration: 310, loss: 7.5476\n",
            "Iteration: 311, loss: 7.6264\n",
            "Iteration: 312, loss: 7.4703\n",
            "Iteration: 313, loss: 7.6740\n",
            "Iteration: 314, loss: 7.5478\n",
            "Iteration: 315, loss: 7.6139\n",
            "Iteration: 316, loss: 7.4865\n",
            "Iteration: 317, loss: 7.5763\n",
            "Iteration: 318, loss: 7.5441\n",
            "Iteration: 319, loss: 7.4299\n",
            "Iteration: 320, loss: 7.4122\n",
            "Iteration: 321, loss: 7.6964\n",
            "Iteration: 322, loss: 7.6433\n",
            "Iteration: 323, loss: 7.4962\n",
            "Iteration: 324, loss: 7.4838\n",
            "Iteration: 325, loss: 7.5773\n",
            "Iteration: 326, loss: 7.7215\n",
            "Iteration: 327, loss: 7.5592\n",
            "Iteration: 328, loss: 7.6198\n",
            "Iteration: 329, loss: 7.5498\n",
            "Iteration: 330, loss: 7.4875\n",
            "Iteration: 331, loss: 7.7741\n",
            "Iteration: 332, loss: 7.6895\n",
            "Iteration: 333, loss: 7.6540\n",
            "Iteration: 334, loss: 7.6788\n",
            "Iteration: 335, loss: 7.5370\n",
            "Iteration: 336, loss: 7.4975\n",
            "Iteration: 337, loss: 7.7018\n",
            "Iteration: 338, loss: 7.6392\n",
            "Iteration: 339, loss: 7.4063\n",
            "Iteration: 340, loss: 7.5991\n",
            "Iteration: 341, loss: 7.6328\n",
            "Iteration: 342, loss: 7.6109\n",
            "Iteration: 343, loss: 7.6839\n",
            "Iteration: 344, loss: 7.6469\n",
            "Iteration: 345, loss: 7.5013\n",
            "Iteration: 346, loss: 7.8751\n",
            "Iteration: 347, loss: 7.6252\n",
            "Iteration: 348, loss: 7.5715\n",
            "Iteration: 349, loss: 7.5597\n",
            "Iteration: 350, loss: 7.4366\n",
            "Iteration: 351, loss: 7.4882\n",
            "Iteration: 352, loss: 7.5500\n",
            "Iteration: 353, loss: 7.4807\n",
            "Iteration: 354, loss: 7.4762\n",
            "Iteration: 355, loss: 7.5799\n",
            "Iteration: 356, loss: 7.5228\n",
            "Iteration: 357, loss: 7.6571\n",
            "Iteration: 358, loss: 7.3895\n",
            "Iteration: 359, loss: 7.6725\n",
            "Iteration: 360, loss: 7.6575\n",
            "Iteration: 361, loss: 7.4463\n",
            "Iteration: 362, loss: 7.7245\n",
            "Iteration: 363, loss: 7.3481\n",
            "Iteration: 364, loss: 7.4974\n",
            "Iteration: 365, loss: 7.6028\n",
            "Iteration: 366, loss: 7.5772\n",
            "Iteration: 367, loss: 7.6682\n",
            "Iteration: 368, loss: 7.5506\n",
            "Iteration: 369, loss: 7.4870\n",
            "Iteration: 370, loss: 7.3918\n",
            "Iteration: 371, loss: 7.6831\n",
            "Iteration: 372, loss: 7.4312\n",
            "Iteration: 373, loss: 7.5627\n",
            "Iteration: 374, loss: 7.6112\n",
            "Iteration: 375, loss: 7.5542\n",
            "Iteration: 376, loss: 7.6069\n",
            "Iteration: 377, loss: 7.5058\n",
            "Iteration: 378, loss: 7.3888\n",
            "Iteration: 379, loss: 7.5107\n",
            "Iteration: 380, loss: 7.6432\n",
            "Iteration: 381, loss: 7.5126\n",
            "Iteration: 382, loss: 7.5121\n",
            "Iteration: 383, loss: 7.5464\n",
            "Iteration: 384, loss: 7.4426\n",
            "Iteration: 385, loss: 7.6370\n",
            "Iteration: 386, loss: 7.4772\n",
            "Iteration: 387, loss: 7.8104\n",
            "Iteration: 388, loss: 7.4795\n",
            "Iteration: 389, loss: 7.5623\n",
            "Iteration: 390, loss: 7.6465\n",
            "Iteration: 391, loss: 7.4944\n",
            "Iteration: 392, loss: 7.6954\n",
            "Iteration: 393, loss: 7.4700\n",
            "Iteration: 394, loss: 7.5992\n",
            "Iteration: 395, loss: 7.7408\n",
            "Iteration: 396, loss: 7.7706\n",
            "Iteration: 397, loss: 7.5141\n",
            "Iteration: 398, loss: 7.3647\n",
            "Iteration: 399, loss: 7.5261\n",
            "Iteration: 400, loss: 7.6796\n",
            "Iteration: 401, loss: 7.5684\n",
            "Iteration: 402, loss: 7.5496\n",
            "Iteration: 403, loss: 7.5944\n",
            "Iteration: 404, loss: 7.3557\n",
            "Iteration: 405, loss: 7.4857\n",
            "Iteration: 406, loss: 7.5576\n",
            "Iteration: 407, loss: 7.4659\n",
            "Iteration: 408, loss: 7.5223\n",
            "Iteration: 409, loss: 7.7463\n",
            "Iteration: 410, loss: 7.5424\n",
            "Iteration: 411, loss: 7.4016\n",
            "Iteration: 412, loss: 7.5756\n",
            "Iteration: 413, loss: 7.6893\n",
            "Iteration: 414, loss: 7.5181\n",
            "Iteration: 415, loss: 7.4773\n",
            "Iteration: 416, loss: 7.4112\n",
            "Iteration: 417, loss: 7.6498\n",
            "Iteration: 418, loss: 7.5140\n",
            "Iteration: 419, loss: 7.3418\n",
            "Iteration: 420, loss: 7.6549\n",
            "Iteration: 421, loss: 7.1968\n",
            "Iteration: 422, loss: 7.6205\n",
            "Iteration: 423, loss: 7.6758\n",
            "Iteration: 424, loss: 7.5725\n",
            "Iteration: 425, loss: 7.4068\n",
            "Iteration: 426, loss: 7.2781\n",
            "Iteration: 427, loss: 7.7011\n",
            "Iteration: 428, loss: 7.5924\n",
            "Iteration: 429, loss: 7.7761\n",
            "Iteration: 430, loss: 7.5905\n",
            "Iteration: 431, loss: 7.5533\n",
            "Iteration: 432, loss: 7.5459\n",
            "Iteration: 433, loss: 7.5430\n",
            "Iteration: 434, loss: 7.5078\n",
            "Iteration: 435, loss: 7.5329\n",
            "Iteration: 436, loss: 7.5543\n",
            "Iteration: 437, loss: 7.6446\n",
            "Iteration: 438, loss: 7.4165\n",
            "Iteration: 439, loss: 7.4425\n",
            "Iteration: 440, loss: 7.3806\n",
            "Iteration: 441, loss: 7.4883\n",
            "Iteration: 442, loss: 7.4258\n",
            "Iteration: 443, loss: 7.7545\n",
            "Iteration: 444, loss: 7.4569\n",
            "Iteration: 445, loss: 7.7065\n",
            "Iteration: 446, loss: 7.5514\n",
            "Iteration: 447, loss: 7.7270\n",
            "Iteration: 448, loss: 7.7918\n",
            "Iteration: 449, loss: 7.5492\n",
            "Iteration: 450, loss: 7.5419\n",
            "Iteration: 451, loss: 7.5604\n",
            "Iteration: 452, loss: 7.6711\n",
            "Iteration: 453, loss: 7.7821\n",
            "Iteration: 454, loss: 7.4782\n",
            "Iteration: 455, loss: 7.7179\n",
            "Iteration: 456, loss: 7.6656\n",
            "Iteration: 457, loss: 7.7039\n",
            "Iteration: 458, loss: 7.6834\n",
            "Iteration: 459, loss: 7.4621\n",
            "Iteration: 460, loss: 7.4778\n",
            "Iteration: 461, loss: 7.4640\n",
            "Iteration: 462, loss: 7.4231\n",
            "Iteration: 463, loss: 7.7141\n",
            "Iteration: 464, loss: 7.6424\n",
            "Iteration: 465, loss: 7.4903\n",
            "Iteration: 466, loss: 7.6487\n",
            "Iteration: 467, loss: 7.7228\n",
            "Iteration: 468, loss: 7.5334\n",
            "Iteration: 469, loss: 7.5054\n",
            "Iteration: 470, loss: 7.5446\n",
            "Iteration: 471, loss: 7.4991\n",
            "Iteration: 472, loss: 7.3405\n",
            "Iteration: 473, loss: 7.5564\n",
            "Iteration: 474, loss: 7.4882\n",
            "Iteration: 475, loss: 7.4791\n",
            "Iteration: 476, loss: 7.2409\n",
            "Iteration: 477, loss: 7.7591\n",
            "Iteration: 478, loss: 7.5804\n",
            "Iteration: 479, loss: 7.4719\n",
            "Iteration: 480, loss: 7.7024\n",
            "Iteration: 481, loss: 7.8164\n",
            "Iteration: 482, loss: 7.4948\n",
            "Iteration: 483, loss: 7.5400\n",
            "Iteration: 484, loss: 7.6253\n",
            "Iteration: 485, loss: 7.5505\n",
            "Iteration: 486, loss: 7.4716\n",
            "Iteration: 487, loss: 7.3173\n",
            "Iteration: 488, loss: 7.3753\n",
            "Iteration: 489, loss: 7.6868\n",
            "Iteration: 490, loss: 7.5395\n",
            "Iteration: 491, loss: 7.7428\n",
            "Iteration: 492, loss: 7.5912\n",
            "Iteration: 493, loss: 7.6195\n",
            "Iteration: 494, loss: 7.3507\n",
            "Iteration: 495, loss: 7.5584\n",
            "Iteration: 496, loss: 7.4863\n",
            "Iteration: 497, loss: 7.4035\n",
            "Iteration: 498, loss: 7.4185\n",
            "Iteration: 499, loss: 7.5446\n",
            "Iteration: 500, loss: 7.3879\n",
            "Iteration: 501, loss: 7.4934\n",
            "Iteration: 502, loss: 7.5592\n",
            "Iteration: 503, loss: 7.5145\n",
            "Iteration: 504, loss: 7.5810\n",
            "Iteration: 505, loss: 7.5846\n",
            "Iteration: 506, loss: 7.4272\n",
            "Iteration: 507, loss: 7.4865\n",
            "Iteration: 508, loss: 7.4254\n",
            "Iteration: 509, loss: 7.5467\n",
            "Iteration: 510, loss: 7.6080\n",
            "Iteration: 511, loss: 7.6567\n",
            "Iteration: 512, loss: 7.4512\n",
            "Iteration: 513, loss: 7.5953\n",
            "Iteration: 514, loss: 7.5276\n",
            "Iteration: 515, loss: 7.5212\n",
            "Iteration: 516, loss: 7.4225\n",
            "Iteration: 517, loss: 7.7382\n",
            "Iteration: 518, loss: 7.5707\n",
            "Iteration: 519, loss: 7.4870\n",
            "Iteration: 520, loss: 7.7481\n",
            "Iteration: 521, loss: 7.5127\n",
            "Iteration: 522, loss: 7.5869\n",
            "Iteration: 523, loss: 7.5469\n",
            "Iteration: 524, loss: 7.5879\n",
            "Iteration: 525, loss: 7.5032\n",
            "Iteration: 526, loss: 7.4216\n",
            "Iteration: 527, loss: 7.5188\n",
            "Iteration: 528, loss: 7.5519\n",
            "Iteration: 529, loss: 7.6475\n",
            "Iteration: 530, loss: 7.6277\n",
            "Iteration: 531, loss: 7.4587\n",
            "Iteration: 532, loss: 7.4796\n",
            "Iteration: 533, loss: 7.5866\n",
            "Iteration: 534, loss: 7.6854\n",
            "Iteration: 535, loss: 7.5305\n",
            "Iteration: 536, loss: 7.3969\n",
            "Iteration: 537, loss: 7.5446\n",
            "Iteration: 538, loss: 7.4766\n",
            "Iteration: 539, loss: 7.6022\n",
            "Iteration: 540, loss: 7.5604\n",
            "Iteration: 541, loss: 7.5301\n",
            "Iteration: 542, loss: 7.5497\n",
            "Iteration: 543, loss: 7.5051\n",
            "Iteration: 544, loss: 7.5382\n",
            "Iteration: 545, loss: 7.4061\n",
            "Iteration: 546, loss: 7.4791\n",
            "Iteration: 547, loss: 7.5884\n",
            "Iteration: 548, loss: 7.6198\n",
            "Iteration: 549, loss: 7.6528\n",
            "Iteration: 550, loss: 7.4663\n",
            "Iteration: 551, loss: 7.6193\n",
            "Iteration: 552, loss: 7.3277\n",
            "Iteration: 553, loss: 7.4248\n",
            "Iteration: 554, loss: 7.5745\n",
            "Iteration: 555, loss: 7.6334\n",
            "Iteration: 556, loss: 7.5963\n",
            "Iteration: 557, loss: 7.5793\n",
            "Iteration: 558, loss: 7.6458\n",
            "Iteration: 559, loss: 7.6031\n",
            "Iteration: 560, loss: 7.5848\n",
            "Iteration: 561, loss: 7.5248\n",
            "Iteration: 562, loss: 7.5064\n",
            "Iteration: 563, loss: 7.6428\n",
            "Iteration: 564, loss: 7.5337\n",
            "Iteration: 565, loss: 7.7983\n",
            "Iteration: 566, loss: 7.5884\n",
            "Iteration: 567, loss: 7.5163\n",
            "Iteration: 568, loss: 7.4870\n",
            "Iteration: 569, loss: 7.6210\n",
            "Iteration: 570, loss: 7.2927\n",
            "Iteration: 571, loss: 7.4206\n",
            "Iteration: 572, loss: 7.6528\n",
            "Iteration: 573, loss: 7.5057\n",
            "Iteration: 574, loss: 7.5331\n",
            "Iteration: 575, loss: 7.4855\n",
            "Iteration: 576, loss: 7.5377\n",
            "Iteration: 577, loss: 7.4736\n",
            "Iteration: 578, loss: 7.6119\n",
            "Iteration: 579, loss: 7.4906\n",
            "Iteration: 580, loss: 7.5509\n",
            "Iteration: 581, loss: 7.6827\n",
            "Iteration: 582, loss: 7.5544\n",
            "Iteration: 583, loss: 7.5014\n",
            "Iteration: 584, loss: 7.4100\n",
            "Iteration: 585, loss: 7.3020\n",
            "Iteration: 586, loss: 7.6410\n",
            "Iteration: 587, loss: 7.7082\n",
            "Iteration: 588, loss: 7.4661\n",
            "Iteration: 589, loss: 7.5819\n",
            "Iteration: 590, loss: 7.4577\n",
            "Iteration: 591, loss: 7.6250\n",
            "Iteration: 592, loss: 7.5842\n",
            "Iteration: 593, loss: 7.4505\n",
            "Iteration: 594, loss: 7.4764\n",
            "Iteration: 595, loss: 7.5066\n",
            "Iteration: 596, loss: 7.3388\n",
            "Iteration: 597, loss: 7.4708\n",
            "Iteration: 598, loss: 7.6863\n",
            "Iteration: 599, loss: 7.7026\n",
            "Iteration: 600, loss: 7.6398\n",
            "Iteration: 601, loss: 7.5385\n",
            "Iteration: 602, loss: 7.6299\n",
            "Iteration: 603, loss: 7.3105\n",
            "Iteration: 604, loss: 7.4516\n",
            "Iteration: 605, loss: 7.7854\n",
            "Iteration: 606, loss: 7.5792\n",
            "Iteration: 607, loss: 7.3970\n",
            "Iteration: 608, loss: 7.5268\n",
            "Iteration: 609, loss: 7.4405\n",
            "Iteration: 610, loss: 7.6347\n",
            "Iteration: 611, loss: 7.3930\n",
            "Iteration: 612, loss: 7.4413\n",
            "Iteration: 613, loss: 7.4504\n",
            "Iteration: 614, loss: 7.4355\n",
            "Iteration: 615, loss: 7.6282\n",
            "Iteration: 616, loss: 7.7412\n",
            "Iteration: 617, loss: 7.7195\n",
            "Iteration: 618, loss: 7.3937\n",
            "Iteration: 619, loss: 7.6016\n",
            "Iteration: 620, loss: 7.3309\n",
            "Iteration: 621, loss: 7.3683\n",
            "Iteration: 622, loss: 7.3872\n",
            "Iteration: 623, loss: 7.5331\n",
            "Iteration: 624, loss: 7.4297\n",
            "Iteration: 625, loss: 7.3248\n",
            "Iteration: 626, loss: 7.6423\n",
            "Iteration: 627, loss: 7.5001\n",
            "Iteration: 628, loss: 7.4766\n",
            "Iteration: 629, loss: 7.3590\n",
            "Iteration: 630, loss: 7.5673\n",
            "Iteration: 631, loss: 7.2793\n",
            "Iteration: 632, loss: 7.6673\n",
            "Iteration: 633, loss: 7.5211\n",
            "Iteration: 634, loss: 7.5398\n",
            "Iteration: 635, loss: 7.2808\n",
            "Iteration: 636, loss: 7.5691\n",
            "Iteration: 637, loss: 7.4507\n",
            "Iteration: 638, loss: 7.5088\n",
            "Iteration: 639, loss: 7.3870\n",
            "Iteration: 640, loss: 7.4571\n",
            "Iteration: 641, loss: 7.5915\n",
            "Iteration: 642, loss: 7.4972\n",
            "Iteration: 643, loss: 7.4677\n",
            "Iteration: 644, loss: 7.5345\n",
            "Iteration: 645, loss: 7.4579\n",
            "Iteration: 646, loss: 7.4580\n",
            "Iteration: 647, loss: 7.3220\n",
            "Iteration: 648, loss: 7.7360\n",
            "Iteration: 649, loss: 7.5511\n",
            "Iteration: 650, loss: 7.3653\n",
            "Iteration: 651, loss: 7.3105\n",
            "Iteration: 652, loss: 7.5733\n",
            "Iteration: 653, loss: 7.4022\n",
            "Iteration: 654, loss: 7.6876\n",
            "Iteration: 655, loss: 7.3456\n",
            "Iteration: 656, loss: 7.4677\n",
            "Iteration: 657, loss: 7.6484\n",
            "Iteration: 658, loss: 7.4001\n",
            "Iteration: 659, loss: 7.3747\n",
            "Iteration: 660, loss: 7.6036\n",
            "Iteration: 661, loss: 7.5373\n",
            "Iteration: 662, loss: 7.4536\n",
            "Iteration: 663, loss: 7.6440\n",
            "Iteration: 664, loss: 7.6632\n",
            "Iteration: 665, loss: 7.5434\n",
            "Iteration: 666, loss: 7.6183\n",
            "Iteration: 667, loss: 7.4731\n",
            "Iteration: 668, loss: 7.5006\n",
            "Iteration: 669, loss: 7.2468\n",
            "Iteration: 670, loss: 7.4758\n",
            "Iteration: 671, loss: 7.4031\n",
            "Iteration: 672, loss: 7.4411\n",
            "Iteration: 673, loss: 7.5231\n",
            "Iteration: 674, loss: 7.5486\n",
            "Iteration: 675, loss: 7.4572\n",
            "Iteration: 676, loss: 7.4380\n",
            "Iteration: 677, loss: 7.5541\n",
            "Iteration: 678, loss: 7.4774\n",
            "Iteration: 679, loss: 7.4098\n",
            "Iteration: 680, loss: 7.5336\n",
            "Iteration: 681, loss: 7.6098\n",
            "Iteration: 682, loss: 7.5180\n",
            "Iteration: 683, loss: 7.3452\n",
            "Iteration: 684, loss: 7.5017\n",
            "Iteration: 685, loss: 7.6680\n",
            "Iteration: 686, loss: 7.5389\n",
            "Iteration: 687, loss: 7.3987\n",
            "Iteration: 688, loss: 7.6346\n",
            "Iteration: 689, loss: 7.5122\n",
            "Iteration: 690, loss: 7.3214\n",
            "Iteration: 691, loss: 7.4601\n",
            "Iteration: 692, loss: 7.5692\n",
            "Iteration: 693, loss: 7.4560\n",
            "Iteration: 694, loss: 7.3532\n",
            "Iteration: 695, loss: 7.3594\n",
            "Iteration: 696, loss: 7.4627\n",
            "Iteration: 697, loss: 7.5590\n",
            "Iteration: 698, loss: 7.5190\n",
            "Iteration: 699, loss: 7.6163\n",
            "Iteration: 700, loss: 7.3908\n",
            "Iteration: 701, loss: 7.5129\n",
            "Iteration: 702, loss: 7.6012\n",
            "Iteration: 703, loss: 7.5005\n",
            "Iteration: 704, loss: 7.4970\n",
            "Iteration: 705, loss: 7.4647\n",
            "Iteration: 706, loss: 7.6971\n",
            "Iteration: 707, loss: 7.6663\n",
            "Iteration: 708, loss: 7.4845\n",
            "Iteration: 709, loss: 7.7009\n",
            "Iteration: 710, loss: 7.5807\n",
            "Iteration: 711, loss: 7.8096\n",
            "Iteration: 712, loss: 7.4669\n",
            "Iteration: 713, loss: 7.6047\n",
            "Iteration: 714, loss: 7.4073\n",
            "Iteration: 715, loss: 7.4750\n",
            "Iteration: 716, loss: 7.6376\n",
            "Iteration: 717, loss: 7.3830\n",
            "Iteration: 718, loss: 7.5186\n",
            "Iteration: 719, loss: 7.2716\n",
            "Iteration: 720, loss: 7.5319\n",
            "Iteration: 721, loss: 7.6309\n",
            "Iteration: 722, loss: 7.5536\n",
            "Iteration: 723, loss: 7.3996\n",
            "Iteration: 724, loss: 7.4993\n",
            "Iteration: 725, loss: 7.3814\n",
            "Iteration: 726, loss: 7.5870\n",
            "Iteration: 727, loss: 7.6285\n",
            "Iteration: 728, loss: 7.7050\n",
            "Iteration: 729, loss: 7.6135\n",
            "Iteration: 730, loss: 7.6018\n",
            "Iteration: 731, loss: 7.4010\n",
            "Iteration: 732, loss: 7.5872\n",
            "Iteration: 733, loss: 7.3683\n",
            "Iteration: 734, loss: 7.4304\n",
            "Iteration: 735, loss: 7.5227\n",
            "Iteration: 736, loss: 7.4210\n",
            "Iteration: 737, loss: 7.6318\n",
            "Iteration: 738, loss: 7.4514\n",
            "Iteration: 739, loss: 7.5703\n",
            "Iteration: 740, loss: 7.4670\n",
            "Iteration: 741, loss: 7.6774\n",
            "Iteration: 742, loss: 7.5848\n",
            "Iteration: 743, loss: 7.5832\n",
            "Iteration: 744, loss: 7.4198\n",
            "Iteration: 745, loss: 7.4143\n",
            "Iteration: 746, loss: 7.4640\n",
            "Iteration: 747, loss: 7.3847\n",
            "Iteration: 748, loss: 7.6811\n",
            "Iteration: 749, loss: 7.5166\n",
            "Iteration: 750, loss: 7.4023\n",
            "Iteration: 751, loss: 7.4737\n",
            "Iteration: 752, loss: 7.6057\n",
            "Iteration: 753, loss: 7.5703\n",
            "Iteration: 754, loss: 7.3846\n",
            "Iteration: 755, loss: 7.5117\n",
            "Iteration: 756, loss: 7.6233\n",
            "Iteration: 757, loss: 7.3785\n",
            "Iteration: 758, loss: 7.5701\n",
            "Iteration: 759, loss: 7.6002\n",
            "Iteration: 760, loss: 7.5313\n",
            "Iteration: 761, loss: 7.2045\n",
            "Iteration: 762, loss: 7.3079\n",
            "Iteration: 763, loss: 7.2532\n",
            "Iteration: 764, loss: 7.4812\n",
            "Iteration: 765, loss: 7.4368\n",
            "Iteration: 766, loss: 7.4922\n",
            "Iteration: 767, loss: 7.4531\n",
            "Iteration: 768, loss: 7.5513\n",
            "Iteration: 769, loss: 7.6109\n",
            "Iteration: 770, loss: 7.3091\n",
            "Iteration: 771, loss: 7.4961\n",
            "Iteration: 772, loss: 7.5140\n",
            "Iteration: 773, loss: 7.4425\n",
            "Iteration: 774, loss: 7.4700\n",
            "Iteration: 775, loss: 7.4971\n",
            "Iteration: 776, loss: 7.3226\n",
            "Iteration: 777, loss: 7.4495\n",
            "Iteration: 778, loss: 7.5009\n",
            "Iteration: 779, loss: 7.4844\n",
            "Iteration: 780, loss: 7.5892\n",
            "Iteration: 781, loss: 7.4659\n",
            "Iteration: 782, loss: 7.4844\n",
            "Iteration: 783, loss: 7.4672\n",
            "Iteration: 784, loss: 7.5683\n",
            "Iteration: 785, loss: 7.4741\n",
            "Iteration: 786, loss: 7.5334\n",
            "Iteration: 787, loss: 7.4204\n",
            "Iteration: 788, loss: 7.6052\n",
            "Iteration: 789, loss: 7.4651\n",
            "Iteration: 790, loss: 7.6253\n",
            "Iteration: 791, loss: 7.4516\n",
            "Iteration: 792, loss: 7.4426\n",
            "Iteration: 793, loss: 7.7418\n",
            "Iteration: 794, loss: 7.4428\n",
            "Iteration: 795, loss: 7.3297\n",
            "Iteration: 796, loss: 7.6179\n",
            "Iteration: 797, loss: 7.4253\n",
            "Iteration: 798, loss: 7.4947\n",
            "Iteration: 799, loss: 7.4271\n",
            "Iteration: 800, loss: 7.5661\n",
            "Iteration: 801, loss: 7.5625\n",
            "Iteration: 802, loss: 7.3921\n",
            "Iteration: 803, loss: 7.3840\n",
            "Iteration: 804, loss: 7.5055\n",
            "Iteration: 805, loss: 7.7308\n",
            "Iteration: 806, loss: 7.3749\n",
            "Iteration: 807, loss: 7.3684\n",
            "Iteration: 808, loss: 7.4678\n",
            "Iteration: 809, loss: 7.6458\n",
            "Iteration: 810, loss: 7.7050\n",
            "Iteration: 811, loss: 7.5781\n",
            "Iteration: 812, loss: 7.3879\n",
            "Iteration: 813, loss: 7.6341\n",
            "Iteration: 814, loss: 7.7001\n",
            "Iteration: 815, loss: 7.7863\n",
            "Iteration: 816, loss: 7.7223\n",
            "Iteration: 817, loss: 7.5122\n",
            "Iteration: 818, loss: 7.4404\n",
            "Iteration: 819, loss: 7.4188\n",
            "Iteration: 820, loss: 7.4091\n",
            "Iteration: 821, loss: 7.4204\n",
            "Iteration: 822, loss: 7.5995\n",
            "Iteration: 823, loss: 7.5542\n",
            "Iteration: 824, loss: 7.7955\n",
            "Iteration: 825, loss: 7.4302\n",
            "Iteration: 826, loss: 7.5550\n",
            "Iteration: 827, loss: 7.5855\n",
            "Iteration: 828, loss: 7.5436\n",
            "Iteration: 829, loss: 7.4082\n",
            "Iteration: 830, loss: 7.4958\n",
            "Iteration: 831, loss: 7.4177\n",
            "Iteration: 832, loss: 7.4061\n",
            "Iteration: 833, loss: 7.5401\n",
            "Iteration: 834, loss: 7.3693\n",
            "Iteration: 835, loss: 7.4022\n",
            "Iteration: 836, loss: 7.7184\n",
            "Iteration: 837, loss: 7.5415\n",
            "Iteration: 838, loss: 7.5238\n",
            "Iteration: 839, loss: 7.5490\n",
            "Iteration: 840, loss: 7.6289\n",
            "Iteration: 841, loss: 7.3911\n",
            "Iteration: 842, loss: 7.3906\n",
            "Iteration: 843, loss: 7.6680\n",
            "Iteration: 844, loss: 7.2166\n",
            "Iteration: 845, loss: 7.2699\n",
            "Iteration: 846, loss: 7.6468\n",
            "Iteration: 847, loss: 7.6334\n",
            "Iteration: 848, loss: 7.4719\n",
            "Iteration: 849, loss: 7.3877\n",
            "Iteration: 850, loss: 7.5523\n",
            "Iteration: 851, loss: 7.5189\n",
            "Iteration: 852, loss: 7.5936\n",
            "Iteration: 853, loss: 7.3938\n",
            "Iteration: 854, loss: 7.4554\n",
            "Iteration: 855, loss: 7.4736\n",
            "Iteration: 856, loss: 7.3834\n",
            "Iteration: 857, loss: 7.6232\n",
            "Iteration: 858, loss: 7.4416\n",
            "Iteration: 859, loss: 7.6198\n",
            "Iteration: 860, loss: 7.5114\n",
            "Iteration: 861, loss: 7.3550\n",
            "Iteration: 862, loss: 7.3695\n",
            "Iteration: 863, loss: 7.5989\n",
            "Iteration: 864, loss: 7.3540\n",
            "Iteration: 865, loss: 7.5286\n",
            "Iteration: 866, loss: 7.5351\n",
            "Iteration: 867, loss: 7.3866\n",
            "Iteration: 868, loss: 7.5930\n",
            "Iteration: 869, loss: 7.3252\n",
            "Iteration: 870, loss: 7.5538\n",
            "Iteration: 871, loss: 7.5858\n",
            "Iteration: 872, loss: 7.6925\n",
            "Iteration: 873, loss: 7.3704\n",
            "Iteration: 874, loss: 7.6720\n",
            "Iteration: 875, loss: 7.4438\n",
            "Iteration: 876, loss: 7.5367\n",
            "Iteration: 877, loss: 7.4203\n",
            "Iteration: 878, loss: 7.4530\n",
            "Iteration: 879, loss: 7.3950\n",
            "Iteration: 880, loss: 7.4350\n",
            "Iteration: 881, loss: 7.7470\n",
            "Iteration: 882, loss: 7.4915\n",
            "Iteration: 883, loss: 7.4720\n",
            "Iteration: 884, loss: 7.4720\n",
            "Iteration: 885, loss: 7.4181\n",
            "Iteration: 886, loss: 7.3680\n",
            "Iteration: 887, loss: 7.5435\n",
            "Iteration: 888, loss: 7.4441\n",
            "Iteration: 889, loss: 7.4814\n",
            "Iteration: 890, loss: 7.3675\n",
            "Iteration: 891, loss: 7.7599\n",
            "Iteration: 892, loss: 7.5175\n",
            "Iteration: 893, loss: 7.4017\n",
            "Iteration: 894, loss: 7.4961\n",
            "Iteration: 895, loss: 7.5076\n",
            "Iteration: 896, loss: 7.2552\n",
            "Iteration: 897, loss: 7.3344\n",
            "Iteration: 898, loss: 7.1754\n",
            "Iteration: 899, loss: 7.5506\n",
            "Iteration: 900, loss: 7.4824\n",
            "Iteration: 901, loss: 7.4934\n",
            "Iteration: 902, loss: 7.5420\n",
            "Iteration: 903, loss: 7.4628\n",
            "Iteration: 904, loss: 7.2762\n",
            "Iteration: 905, loss: 7.4704\n",
            "Iteration: 906, loss: 7.4184\n",
            "Iteration: 907, loss: 7.5389\n",
            "Iteration: 908, loss: 7.3167\n",
            "Iteration: 909, loss: 7.5440\n",
            "Iteration: 910, loss: 7.4461\n",
            "Iteration: 911, loss: 7.5593\n",
            "Iteration: 912, loss: 7.4212\n",
            "Iteration: 913, loss: 7.4594\n",
            "Iteration: 914, loss: 7.2474\n",
            "Iteration: 915, loss: 7.4590\n",
            "Iteration: 916, loss: 7.7680\n",
            "Iteration: 917, loss: 7.7681\n",
            "Iteration: 918, loss: 7.5855\n",
            "Iteration: 919, loss: 7.6129\n",
            "Iteration: 920, loss: 7.8297\n",
            "Iteration: 921, loss: 7.3987\n",
            "Iteration: 922, loss: 7.4942\n",
            "Iteration: 923, loss: 7.3506\n",
            "Iteration: 924, loss: 7.5813\n",
            "Iteration: 925, loss: 7.6782\n",
            "Iteration: 926, loss: 7.4050\n",
            "Iteration: 927, loss: 7.4796\n",
            "Iteration: 928, loss: 7.3276\n",
            "Iteration: 929, loss: 7.5206\n",
            "Iteration: 930, loss: 7.5047\n",
            "Iteration: 931, loss: 7.6761\n",
            "Iteration: 932, loss: 7.4163\n",
            "Iteration: 933, loss: 7.4706\n",
            "Iteration: 934, loss: 7.4993\n",
            "Iteration: 935, loss: 7.5803\n",
            "Iteration: 936, loss: 7.5013\n",
            "Iteration: 937, loss: 7.4327\n",
            "Iteration: 938, loss: 7.5323\n",
            "Iteration: 939, loss: 7.4913\n",
            "Iteration: 940, loss: 7.3191\n",
            "Iteration: 941, loss: 7.4854\n",
            "Iteration: 942, loss: 7.4525\n",
            "Iteration: 943, loss: 7.5219\n",
            "Iteration: 944, loss: 7.4010\n",
            "Iteration: 945, loss: 7.4626\n",
            "Iteration: 946, loss: 7.4361\n",
            "Iteration: 947, loss: 7.4277\n",
            "Iteration: 948, loss: 7.4210\n",
            "Iteration: 949, loss: 7.5627\n",
            "Iteration: 950, loss: 7.4860\n",
            "Iteration: 951, loss: 7.6506\n",
            "Iteration: 952, loss: 7.3854\n",
            "Iteration: 953, loss: 7.2151\n",
            "Iteration: 954, loss: 7.5020\n",
            "Iteration: 955, loss: 7.5462\n",
            "Iteration: 956, loss: 7.5243\n",
            "Iteration: 957, loss: 7.6383\n",
            "Iteration: 958, loss: 7.3388\n",
            "Iteration: 959, loss: 7.3305\n",
            "Iteration: 960, loss: 7.4742\n",
            "Iteration: 961, loss: 7.4272\n",
            "Iteration: 962, loss: 7.5309\n",
            "Iteration: 963, loss: 7.4105\n",
            "Iteration: 964, loss: 7.4420\n",
            "Iteration: 965, loss: 7.4947\n",
            "Iteration: 966, loss: 7.4358\n",
            "Iteration: 967, loss: 7.2446\n",
            "Iteration: 968, loss: 7.5057\n",
            "Iteration: 969, loss: 7.6042\n",
            "Iteration: 970, loss: 7.3805\n",
            "Iteration: 971, loss: 7.6800\n",
            "Iteration: 972, loss: 7.7003\n",
            "Iteration: 973, loss: 7.4478\n",
            "Iteration: 974, loss: 7.4390\n",
            "Iteration: 975, loss: 7.5660\n",
            "Iteration: 976, loss: 7.4065\n",
            "Iteration: 977, loss: 7.5032\n",
            "Iteration: 978, loss: 7.3778\n",
            "Iteration: 979, loss: 7.5147\n",
            "Iteration: 980, loss: 7.6030\n",
            "Iteration: 981, loss: 7.4467\n",
            "Iteration: 982, loss: 7.4056\n",
            "Iteration: 983, loss: 7.4164\n",
            "Iteration: 984, loss: 7.6022\n",
            "Iteration: 985, loss: 7.5133\n",
            "Iteration: 986, loss: 7.7397\n",
            "Iteration: 987, loss: 7.5769\n",
            "Iteration: 988, loss: 7.5077\n",
            "Iteration: 989, loss: 7.5818\n",
            "Iteration: 990, loss: 7.3802\n",
            "Iteration: 991, loss: 7.4466\n",
            "Iteration: 992, loss: 7.4665\n",
            "Iteration: 993, loss: 7.2032\n",
            "Iteration: 994, loss: 7.3837\n",
            "Iteration: 995, loss: 7.3301\n",
            "Iteration: 996, loss: 7.5056\n",
            "Iteration: 997, loss: 7.5203\n",
            "Iteration: 998, loss: 7.4732\n",
            "Iteration: 999, loss: 7.4636\n",
            "Iteration: 1000, loss: 7.3207\n",
            "Iteration: 1001, loss: 7.5245\n",
            "Iteration: 1002, loss: 7.6771\n",
            "Iteration: 1003, loss: 7.2912\n",
            "Iteration: 1004, loss: 7.3247\n",
            "Iteration: 1005, loss: 7.4014\n",
            "Iteration: 1006, loss: 7.5396\n",
            "Iteration: 1007, loss: 7.5258\n",
            "Iteration: 1008, loss: 7.3343\n",
            "Iteration: 1009, loss: 7.3782\n",
            "Iteration: 1010, loss: 7.4752\n",
            "Iteration: 1011, loss: 7.4941\n",
            "Iteration: 1012, loss: 7.4259\n",
            "Iteration: 1013, loss: 7.4628\n",
            "Iteration: 1014, loss: 7.4744\n",
            "Iteration: 1015, loss: 7.5743\n",
            "Iteration: 1016, loss: 7.4623\n",
            "Iteration: 1017, loss: 7.6104\n",
            "Iteration: 1018, loss: 7.3710\n",
            "Iteration: 1019, loss: 7.4626\n",
            "Iteration: 1020, loss: 7.4809\n",
            "Iteration: 1021, loss: 7.4809\n",
            "Iteration: 1022, loss: 7.3958\n",
            "Iteration: 1023, loss: 7.5222\n",
            "Iteration: 1024, loss: 7.6529\n",
            "Iteration: 1025, loss: 7.5043\n",
            "Iteration: 1026, loss: 7.4508\n",
            "Iteration: 1027, loss: 7.4476\n",
            "Iteration: 1028, loss: 7.5123\n",
            "Iteration: 1029, loss: 7.4497\n",
            "Iteration: 1030, loss: 7.4796\n",
            "Iteration: 1031, loss: 7.5561\n",
            "Iteration: 1032, loss: 7.5775\n",
            "Iteration: 1033, loss: 7.6431\n",
            "Iteration: 1034, loss: 7.5092\n",
            "Iteration: 1035, loss: 7.3528\n",
            "Iteration: 1036, loss: 7.6873\n",
            "Iteration: 1037, loss: 7.5202\n",
            "Iteration: 1038, loss: 7.4797\n",
            "Iteration: 1039, loss: 7.3137\n",
            "Iteration: 1040, loss: 7.5138\n",
            "Iteration: 1041, loss: 7.2414\n",
            "Iteration: 1042, loss: 7.6170\n",
            "Iteration: 1043, loss: 7.6061\n",
            "Iteration: 1044, loss: 7.5709\n",
            "Iteration: 1045, loss: 7.5034\n",
            "Iteration: 1046, loss: 7.2917\n",
            "Iteration: 1047, loss: 7.5005\n",
            "Iteration: 1048, loss: 7.6445\n",
            "Iteration: 1049, loss: 7.4137\n",
            "Iteration: 1050, loss: 7.5442\n",
            "Iteration: 1051, loss: 7.4914\n",
            "Iteration: 1052, loss: 7.3312\n",
            "Iteration: 1053, loss: 7.5411\n",
            "Iteration: 1054, loss: 7.4152\n",
            "Iteration: 1055, loss: 7.3937\n",
            "Iteration: 1056, loss: 7.3660\n",
            "Iteration: 1057, loss: 7.5504\n",
            "Iteration: 1058, loss: 7.5181\n",
            "Iteration: 1059, loss: 7.3718\n",
            "Iteration: 1060, loss: 7.5839\n",
            "Iteration: 1061, loss: 7.5013\n",
            "Iteration: 1062, loss: 7.4990\n",
            "Iteration: 1063, loss: 7.6817\n",
            "Iteration: 1064, loss: 7.5955\n",
            "Iteration: 1065, loss: 7.2974\n",
            "Iteration: 1066, loss: 7.3031\n",
            "Iteration: 1067, loss: 7.4781\n",
            "Iteration: 1068, loss: 7.3538\n",
            "Iteration: 1069, loss: 7.3384\n",
            "Iteration: 1070, loss: 7.5270\n",
            "Iteration: 1071, loss: 7.5165\n",
            "Iteration: 1072, loss: 7.3299\n",
            "Iteration: 1073, loss: 7.4973\n",
            "Iteration: 1074, loss: 7.3327\n",
            "Iteration: 1075, loss: 7.4884\n",
            "Iteration: 1076, loss: 7.3173\n",
            "Iteration: 1077, loss: 7.6537\n",
            "Iteration: 1078, loss: 7.5640\n",
            "Iteration: 1079, loss: 7.3680\n",
            "Iteration: 1080, loss: 7.5093\n",
            "Iteration: 1081, loss: 7.6479\n",
            "Iteration: 1082, loss: 7.3938\n",
            "Iteration: 1083, loss: 7.4489\n",
            "Iteration: 1084, loss: 7.3915\n",
            "Iteration: 1085, loss: 7.3499\n",
            "Iteration: 1086, loss: 7.7142\n",
            "Iteration: 1087, loss: 7.3874\n",
            "Iteration: 1088, loss: 7.4643\n",
            "Iteration: 1089, loss: 7.6007\n",
            "Iteration: 1090, loss: 7.3598\n",
            "Iteration: 1091, loss: 7.4628\n",
            "Iteration: 1092, loss: 7.4772\n",
            "Iteration: 1093, loss: 7.5124\n",
            "Iteration: 1094, loss: 7.5033\n",
            "Iteration: 1095, loss: 7.6117\n",
            "Iteration: 1096, loss: 7.3127\n",
            "Iteration: 1097, loss: 7.4227\n",
            "Iteration: 1098, loss: 7.4008\n",
            "Iteration: 1099, loss: 7.1990\n",
            "Iteration: 1100, loss: 7.4913\n",
            "Iteration: 1101, loss: 7.5173\n",
            "Iteration: 1102, loss: 7.5040\n",
            "Iteration: 1103, loss: 7.4948\n",
            "Iteration: 1104, loss: 7.5090\n",
            "Iteration: 1105, loss: 7.4827\n",
            "Iteration: 1106, loss: 7.3019\n",
            "Iteration: 1107, loss: 7.5575\n",
            "Iteration: 1108, loss: 7.4318\n",
            "Iteration: 1109, loss: 7.3071\n",
            "Iteration: 1110, loss: 7.5850\n",
            "Iteration: 1111, loss: 7.4181\n",
            "Iteration: 1112, loss: 7.4714\n",
            "Iteration: 1113, loss: 7.5850\n",
            "Iteration: 1114, loss: 7.2926\n",
            "Iteration: 1115, loss: 7.4297\n",
            "Iteration: 1116, loss: 7.3236\n",
            "Iteration: 1117, loss: 7.5194\n",
            "Iteration: 1118, loss: 7.5398\n",
            "Iteration: 1119, loss: 7.5950\n",
            "Iteration: 1120, loss: 7.3688\n",
            "Iteration: 1121, loss: 7.2676\n",
            "Iteration: 1122, loss: 7.3316\n",
            "Iteration: 1123, loss: 7.2456\n",
            "Iteration: 1124, loss: 7.3576\n",
            "Iteration: 1125, loss: 7.4658\n",
            "Iteration: 1126, loss: 7.3910\n",
            "Iteration: 1127, loss: 7.3180\n",
            "Iteration: 1128, loss: 7.0525\n",
            "Iteration: 1129, loss: 7.5054\n",
            "Iteration: 1130, loss: 7.6791\n",
            "Iteration: 1131, loss: 7.5519\n",
            "Iteration: 1132, loss: 7.3771\n",
            "Iteration: 1133, loss: 7.2914\n",
            "Iteration: 1134, loss: 7.4471\n",
            "Iteration: 1135, loss: 7.6188\n",
            "Iteration: 1136, loss: 7.4011\n",
            "Iteration: 1137, loss: 7.4540\n",
            "Iteration: 1138, loss: 7.5291\n",
            "Iteration: 1139, loss: 7.4560\n",
            "Iteration: 1140, loss: 7.3921\n",
            "Iteration: 1141, loss: 7.4640\n",
            "Iteration: 1142, loss: 7.1880\n",
            "Iteration: 1143, loss: 7.5994\n",
            "Iteration: 1144, loss: 7.3085\n",
            "Iteration: 1145, loss: 7.5596\n",
            "Iteration: 1146, loss: 7.5120\n",
            "Iteration: 1147, loss: 7.2674\n",
            "Iteration: 1148, loss: 7.3217\n",
            "Iteration: 1149, loss: 7.2099\n",
            "Iteration: 1150, loss: 7.4710\n",
            "Iteration: 1151, loss: 7.4837\n",
            "Iteration: 1152, loss: 7.3439\n",
            "Iteration: 1153, loss: 7.5946\n",
            "Iteration: 1154, loss: 7.4507\n",
            "Iteration: 1155, loss: 7.5453\n",
            "Iteration: 1156, loss: 7.2793\n",
            "Iteration: 1157, loss: 7.7393\n",
            "Iteration: 1158, loss: 7.3391\n",
            "Iteration: 1159, loss: 7.4501\n",
            "Iteration: 1160, loss: 7.4185\n",
            "Iteration: 1161, loss: 7.5814\n",
            "Iteration: 1162, loss: 7.5037\n",
            "Iteration: 1163, loss: 7.4341\n",
            "Iteration: 1164, loss: 7.4356\n",
            "Iteration: 1165, loss: 7.3724\n",
            "Iteration: 1166, loss: 7.4045\n",
            "Iteration: 1167, loss: 7.5520\n",
            "Iteration: 1168, loss: 7.4871\n",
            "Iteration: 1169, loss: 7.3478\n",
            "Iteration: 1170, loss: 7.5411\n",
            "Iteration: 1171, loss: 7.3821\n",
            "Iteration: 1172, loss: 7.5250\n",
            "Iteration: 1173, loss: 7.5579\n",
            "Iteration: 1174, loss: 7.5587\n",
            "Iteration: 1175, loss: 7.4287\n",
            "Iteration: 1176, loss: 7.3684\n",
            "Iteration: 1177, loss: 7.3695\n",
            "Iteration: 1178, loss: 7.3612\n",
            "Iteration: 1179, loss: 7.2508\n",
            "Iteration: 1180, loss: 7.5601\n",
            "Iteration: 1181, loss: 7.2978\n",
            "Iteration: 1182, loss: 7.5723\n",
            "Iteration: 1183, loss: 7.3796\n",
            "Iteration: 1184, loss: 7.5148\n",
            "Iteration: 1185, loss: 7.4150\n",
            "Iteration: 1186, loss: 7.3420\n",
            "Iteration: 1187, loss: 7.5519\n",
            "Iteration: 1188, loss: 7.3730\n",
            "Iteration: 1189, loss: 7.4375\n",
            "Iteration: 1190, loss: 7.3115\n",
            "Iteration: 1191, loss: 7.3369\n",
            "Iteration: 1192, loss: 7.4447\n",
            "Iteration: 1193, loss: 7.4109\n",
            "Iteration: 1194, loss: 7.3365\n",
            "Iteration: 1195, loss: 7.3602\n",
            "Iteration: 1196, loss: 7.3469\n",
            "Iteration: 1197, loss: 7.5052\n",
            "Iteration: 1198, loss: 7.3824\n",
            "Iteration: 1199, loss: 7.4344\n",
            "Iteration: 1200, loss: 7.3848\n",
            "Iteration: 1201, loss: 7.5903\n",
            "Iteration: 1202, loss: 7.4107\n",
            "Iteration: 1203, loss: 7.4822\n",
            "Iteration: 1204, loss: 7.5910\n",
            "Iteration: 1205, loss: 7.5183\n",
            "Iteration: 1206, loss: 7.4949\n",
            "Iteration: 1207, loss: 7.4824\n",
            "Iteration: 1208, loss: 7.2824\n",
            "Iteration: 1209, loss: 7.3672\n",
            "Iteration: 1210, loss: 7.4879\n",
            "Iteration: 1211, loss: 7.3560\n",
            "Iteration: 1212, loss: 7.3066\n",
            "Iteration: 1213, loss: 7.5207\n",
            "Iteration: 1214, loss: 7.5896\n",
            "Iteration: 1215, loss: 7.5506\n",
            "Iteration: 1216, loss: 7.2694\n",
            "Iteration: 1217, loss: 7.5431\n",
            "Iteration: 1218, loss: 7.3032\n",
            "Iteration: 1219, loss: 7.5043\n",
            "Iteration: 1220, loss: 7.5832\n",
            "Iteration: 1221, loss: 7.5453\n",
            "Iteration: 1222, loss: 7.4788\n",
            "Iteration: 1223, loss: 7.3400\n",
            "Iteration: 1224, loss: 7.4117\n",
            "Iteration: 1225, loss: 7.6084\n",
            "Iteration: 1226, loss: 7.4644\n",
            "Iteration: 1227, loss: 7.2130\n",
            "Iteration: 1228, loss: 7.4858\n",
            "Iteration: 1229, loss: 7.5033\n",
            "Iteration: 1230, loss: 7.5582\n",
            "Iteration: 1231, loss: 7.4986\n",
            "Iteration: 1232, loss: 7.5492\n",
            "Iteration: 1233, loss: 7.4093\n",
            "Iteration: 1234, loss: 7.6152\n",
            "Iteration: 1235, loss: 7.3621\n",
            "Iteration: 1236, loss: 7.4535\n",
            "Iteration: 1237, loss: 7.4420\n",
            "Iteration: 1238, loss: 7.4641\n",
            "Iteration: 1239, loss: 7.2846\n",
            "Iteration: 1240, loss: 7.5263\n",
            "Iteration: 1241, loss: 7.2507\n",
            "Iteration: 1242, loss: 7.4260\n",
            "Iteration: 1243, loss: 7.4829\n",
            "Iteration: 1244, loss: 7.4264\n",
            "Iteration: 1245, loss: 7.4276\n",
            "Iteration: 1246, loss: 7.5873\n",
            "Iteration: 1247, loss: 7.4587\n",
            "Iteration: 1248, loss: 7.4798\n",
            "Iteration: 1249, loss: 7.4891\n",
            "Iteration: 1250, loss: 7.6507\n",
            "Iteration: 1251, loss: 7.4611\n",
            "Iteration: 1252, loss: 7.4333\n",
            "Iteration: 1253, loss: 7.5223\n",
            "Iteration: 1254, loss: 7.5793\n",
            "Iteration: 1255, loss: 7.5314\n",
            "Iteration: 1256, loss: 7.5145\n",
            "Iteration: 1257, loss: 7.4649\n",
            "Iteration: 1258, loss: 7.5557\n",
            "Iteration: 1259, loss: 7.5307\n",
            "Iteration: 1260, loss: 7.6157\n",
            "Iteration: 1261, loss: 7.4129\n",
            "Iteration: 1262, loss: 7.5261\n",
            "Iteration: 1263, loss: 7.4454\n",
            "Iteration: 1264, loss: 7.4896\n",
            "Iteration: 1265, loss: 7.4524\n",
            "Iteration: 1266, loss: 7.5247\n",
            "Iteration: 1267, loss: 7.4698\n",
            "Iteration: 1268, loss: 7.4961\n",
            "Iteration: 1269, loss: 7.5023\n",
            "Iteration: 1270, loss: 7.3494\n",
            "Iteration: 1271, loss: 7.3755\n",
            "Iteration: 1272, loss: 7.2396\n",
            "Iteration: 1273, loss: 7.4199\n",
            "Iteration: 1274, loss: 7.1752\n",
            "Iteration: 1275, loss: 7.4751\n",
            "Iteration: 1276, loss: 7.3859\n",
            "Iteration: 1277, loss: 7.4635\n",
            "Iteration: 1278, loss: 7.2670\n",
            "Iteration: 1279, loss: 7.5913\n",
            "Iteration: 1280, loss: 7.2184\n",
            "Iteration: 1281, loss: 7.4369\n",
            "Iteration: 1282, loss: 7.4660\n",
            "Iteration: 1283, loss: 7.6038\n",
            "Iteration: 1284, loss: 7.5102\n",
            "Iteration: 1285, loss: 7.6536\n",
            "Iteration: 1286, loss: 7.4942\n",
            "Iteration: 1287, loss: 7.5404\n",
            "Iteration: 1288, loss: 7.1639\n",
            "Iteration: 1289, loss: 7.1055\n",
            "Iteration: 1290, loss: 7.4045\n",
            "Iteration: 1291, loss: 7.3701\n",
            "Iteration: 1292, loss: 7.2185\n",
            "Iteration: 1293, loss: 7.5610\n",
            "Iteration: 1294, loss: 7.5551\n",
            "Iteration: 1295, loss: 7.3940\n",
            "Iteration: 1296, loss: 7.5329\n",
            "Iteration: 1297, loss: 7.4035\n",
            "Iteration: 1298, loss: 7.3729\n",
            "Iteration: 1299, loss: 7.4349\n",
            "Iteration: 1300, loss: 7.4801\n",
            "Iteration: 1301, loss: 7.3592\n",
            "Iteration: 1302, loss: 7.4715\n",
            "Iteration: 1303, loss: 7.4729\n",
            "Iteration: 1304, loss: 7.2777\n",
            "Iteration: 1305, loss: 7.4242\n",
            "Iteration: 1306, loss: 7.6444\n",
            "Iteration: 1307, loss: 7.5246\n",
            "Iteration: 1308, loss: 7.4414\n",
            "Iteration: 1309, loss: 7.3594\n",
            "Iteration: 1310, loss: 7.5361\n",
            "Iteration: 1311, loss: 7.6209\n",
            "Iteration: 1312, loss: 7.4476\n",
            "Iteration: 1313, loss: 7.3652\n",
            "Iteration: 1314, loss: 7.4243\n",
            "Iteration: 1315, loss: 7.3835\n",
            "Iteration: 1316, loss: 7.3855\n",
            "Iteration: 1317, loss: 7.2696\n",
            "Iteration: 1318, loss: 7.4779\n",
            "Iteration: 1319, loss: 7.4723\n",
            "Iteration: 1320, loss: 7.3142\n",
            "Iteration: 1321, loss: 7.4549\n",
            "Iteration: 1322, loss: 7.4429\n",
            "Iteration: 1323, loss: 7.3381\n",
            "Iteration: 1324, loss: 7.4675\n",
            "Iteration: 1325, loss: 7.4637\n",
            "Iteration: 1326, loss: 7.4582\n",
            "Iteration: 1327, loss: 7.5070\n",
            "Iteration: 1328, loss: 7.3360\n",
            "Iteration: 1329, loss: 7.5346\n",
            "Iteration: 1330, loss: 7.3362\n",
            "Iteration: 1331, loss: 7.3796\n",
            "Iteration: 1332, loss: 7.1186\n",
            "Iteration: 1333, loss: 7.5504\n",
            "Iteration: 1334, loss: 7.4264\n",
            "Iteration: 1335, loss: 7.4972\n",
            "Iteration: 1336, loss: 7.3725\n",
            "Iteration: 1337, loss: 7.2445\n",
            "Iteration: 1338, loss: 7.4562\n",
            "Iteration: 1339, loss: 7.4408\n",
            "Iteration: 1340, loss: 7.4483\n",
            "Iteration: 1341, loss: 7.5563\n",
            "Iteration: 1342, loss: 7.4567\n",
            "Iteration: 1343, loss: 7.5579\n",
            "Iteration: 1344, loss: 7.3767\n",
            "Iteration: 1345, loss: 7.3722\n",
            "Iteration: 1346, loss: 7.6478\n",
            "Iteration: 1347, loss: 7.5093\n",
            "Iteration: 1348, loss: 7.3035\n",
            "Iteration: 1349, loss: 7.5085\n",
            "Iteration: 1350, loss: 7.4640\n",
            "Iteration: 1351, loss: 7.4682\n",
            "Iteration: 1352, loss: 7.4083\n",
            "Iteration: 1353, loss: 7.4706\n",
            "Iteration: 1354, loss: 7.4450\n",
            "Iteration: 1355, loss: 7.5534\n",
            "Iteration: 1356, loss: 7.5788\n",
            "Iteration: 1357, loss: 7.4222\n",
            "Iteration: 1358, loss: 7.3805\n",
            "Iteration: 1359, loss: 7.6171\n",
            "Iteration: 1360, loss: 7.4215\n",
            "Iteration: 1361, loss: 7.5065\n",
            "Iteration: 1362, loss: 7.4477\n",
            "Iteration: 1363, loss: 7.5685\n",
            "Iteration: 1364, loss: 7.4729\n",
            "Iteration: 1365, loss: 7.4034\n",
            "Iteration: 1366, loss: 7.4029\n",
            "Iteration: 1367, loss: 7.3821\n",
            "Iteration: 1368, loss: 7.2623\n",
            "Iteration: 1369, loss: 7.6247\n",
            "Iteration: 1370, loss: 7.2706\n",
            "Iteration: 1371, loss: 7.6437\n",
            "Iteration: 1372, loss: 7.4210\n",
            "Iteration: 1373, loss: 7.4774\n",
            "Iteration: 1374, loss: 7.4830\n",
            "Iteration: 1375, loss: 7.5914\n",
            "Iteration: 1376, loss: 7.3691\n",
            "Iteration: 1377, loss: 7.3673\n",
            "Iteration: 1378, loss: 7.7566\n",
            "Iteration: 1379, loss: 7.1566\n",
            "Iteration: 1380, loss: 7.3875\n",
            "Iteration: 1381, loss: 7.5053\n",
            "Iteration: 1382, loss: 7.4666\n",
            "Iteration: 1383, loss: 7.4425\n",
            "Iteration: 1384, loss: 7.5069\n",
            "Iteration: 1385, loss: 7.5075\n",
            "Iteration: 1386, loss: 7.5382\n",
            "Iteration: 1387, loss: 7.5817\n",
            "Iteration: 1388, loss: 7.4669\n",
            "Iteration: 1389, loss: 7.4886\n",
            "Iteration: 1390, loss: 7.3374\n",
            "Iteration: 1391, loss: 7.3513\n",
            "Iteration: 1392, loss: 7.3141\n",
            "Iteration: 1393, loss: 7.2960\n",
            "Iteration: 1394, loss: 7.4969\n",
            "Iteration: 1395, loss: 7.3894\n",
            "Iteration: 1396, loss: 7.5029\n",
            "Iteration: 1397, loss: 7.5215\n",
            "Iteration: 1398, loss: 7.5377\n",
            "Iteration: 1399, loss: 7.4645\n",
            "Iteration: 1400, loss: 7.5172\n",
            "Iteration: 1401, loss: 7.4554\n",
            "Iteration: 1402, loss: 7.3218\n",
            "Iteration: 1403, loss: 7.2708\n",
            "Iteration: 1404, loss: 7.3826\n",
            "Iteration: 1405, loss: 7.4674\n",
            "Iteration: 1406, loss: 7.3597\n",
            "Iteration: 1407, loss: 7.6615\n",
            "Iteration: 1408, loss: 7.3863\n",
            "Iteration: 1409, loss: 7.3988\n",
            "Iteration: 1410, loss: 7.3462\n",
            "Iteration: 1411, loss: 7.5296\n",
            "Iteration: 1412, loss: 7.4215\n",
            "Iteration: 1413, loss: 7.2903\n",
            "Iteration: 1414, loss: 7.3685\n",
            "Iteration: 1415, loss: 7.4797\n",
            "Iteration: 1416, loss: 7.3631\n",
            "Iteration: 1417, loss: 7.4976\n",
            "Iteration: 1418, loss: 7.3731\n",
            "Iteration: 1419, loss: 7.4691\n",
            "Iteration: 1420, loss: 7.5199\n",
            "Iteration: 1421, loss: 7.4762\n",
            "Iteration: 1422, loss: 7.5781\n",
            "Iteration: 1423, loss: 7.4373\n",
            "Iteration: 1424, loss: 7.5427\n",
            "Iteration: 1425, loss: 7.3501\n",
            "Iteration: 1426, loss: 7.3984\n",
            "Iteration: 1427, loss: 7.4576\n",
            "Iteration: 1428, loss: 7.4668\n",
            "Iteration: 1429, loss: 7.3743\n",
            "Iteration: 1430, loss: 7.3345\n",
            "Iteration: 1431, loss: 7.2565\n",
            "Iteration: 1432, loss: 7.3347\n",
            "Iteration: 1433, loss: 7.3810\n",
            "Iteration: 1434, loss: 7.4266\n",
            "Iteration: 1435, loss: 7.1781\n",
            "Iteration: 1436, loss: 7.5172\n",
            "Iteration: 1437, loss: 7.2723\n",
            "Iteration: 1438, loss: 7.3172\n",
            "Iteration: 1439, loss: 7.3345\n",
            "Iteration: 1440, loss: 7.3802\n",
            "Iteration: 1441, loss: 7.2248\n",
            "Iteration: 1442, loss: 7.4329\n",
            "Iteration: 1443, loss: 7.4029\n",
            "Iteration: 1444, loss: 7.4350\n",
            "Iteration: 1445, loss: 7.5578\n",
            "Iteration: 1446, loss: 7.3884\n",
            "Iteration: 1447, loss: 7.4206\n",
            "Iteration: 1448, loss: 7.4793\n",
            "Iteration: 1449, loss: 7.5563\n",
            "Iteration: 1450, loss: 7.5003\n",
            "Iteration: 1451, loss: 7.3370\n",
            "Iteration: 1452, loss: 7.4641\n",
            "Iteration: 1453, loss: 7.6395\n",
            "Iteration: 1454, loss: 7.4423\n",
            "Iteration: 1455, loss: 7.3641\n",
            "Iteration: 1456, loss: 7.5855\n",
            "Iteration: 1457, loss: 7.4833\n",
            "Iteration: 1458, loss: 7.4803\n",
            "Iteration: 1459, loss: 7.4628\n",
            "Iteration: 1460, loss: 7.2012\n",
            "Iteration: 1461, loss: 7.4889\n",
            "Iteration: 1462, loss: 7.5445\n",
            "Iteration: 1463, loss: 7.3917\n",
            "Iteration: 1464, loss: 7.5003\n",
            "Iteration: 1465, loss: 7.5632\n",
            "Iteration: 1466, loss: 7.4046\n",
            "Iteration: 1467, loss: 7.4030\n",
            "Iteration: 1468, loss: 7.4561\n",
            "Iteration: 1469, loss: 7.3139\n",
            "Iteration: 1470, loss: 7.3701\n",
            "Iteration: 1471, loss: 7.4910\n",
            "Iteration: 1472, loss: 7.4104\n",
            "Iteration: 1473, loss: 7.4160\n",
            "Iteration: 1474, loss: 7.2950\n",
            "Iteration: 1475, loss: 7.4707\n",
            "Iteration: 1476, loss: 7.4764\n",
            "Iteration: 1477, loss: 7.3318\n",
            "Iteration: 1478, loss: 7.3916\n",
            "Iteration: 1479, loss: 7.5043\n",
            "Iteration: 1480, loss: 7.4416\n",
            "Iteration: 1481, loss: 7.3817\n",
            "Iteration: 1482, loss: 7.4209\n",
            "Iteration: 1483, loss: 7.5176\n",
            "Iteration: 1484, loss: 7.4982\n",
            "Iteration: 1485, loss: 7.3929\n",
            "Iteration: 1486, loss: 7.5822\n",
            "Iteration: 1487, loss: 7.3541\n",
            "Iteration: 1488, loss: 7.4799\n",
            "Iteration: 1489, loss: 7.4699\n",
            "Iteration: 1490, loss: 7.5226\n",
            "Iteration: 1491, loss: 7.3340\n",
            "Iteration: 1492, loss: 7.4073\n",
            "Iteration: 1493, loss: 7.5116\n",
            "Iteration: 1494, loss: 7.4213\n",
            "Iteration: 1495, loss: 7.5370\n",
            "Iteration: 1496, loss: 7.3663\n",
            "Iteration: 1497, loss: 7.2997\n",
            "Iteration: 1498, loss: 7.2602\n",
            "Iteration: 1499, loss: 7.5402\n",
            "Iteration: 1500, loss: 7.3906\n",
            "Iteration: 1501, loss: 7.5043\n",
            "Iteration: 1502, loss: 7.3222\n",
            "Iteration: 1503, loss: 7.4206\n",
            "Iteration: 1504, loss: 7.4082\n",
            "Iteration: 1505, loss: 7.3981\n",
            "Iteration: 1506, loss: 7.3577\n",
            "Iteration: 1507, loss: 7.3481\n",
            "Iteration: 1508, loss: 7.4642\n",
            "Iteration: 1509, loss: 7.4262\n",
            "Iteration: 1510, loss: 7.4345\n",
            "Iteration: 1511, loss: 7.4908\n",
            "Iteration: 1512, loss: 7.4105\n",
            "Iteration: 1513, loss: 7.4267\n",
            "Iteration: 1514, loss: 7.1946\n",
            "Iteration: 1515, loss: 7.5364\n",
            "Iteration: 1516, loss: 7.5249\n",
            "Iteration: 1517, loss: 7.5380\n",
            "Iteration: 1518, loss: 7.2587\n",
            "Iteration: 1519, loss: 7.3116\n",
            "Iteration: 1520, loss: 7.4870\n",
            "Iteration: 1521, loss: 7.4390\n",
            "Iteration: 1522, loss: 7.4584\n",
            "Iteration: 1523, loss: 7.4373\n",
            "Iteration: 1524, loss: 7.4560\n",
            "Iteration: 1525, loss: 7.4967\n",
            "Iteration: 1526, loss: 7.2586\n",
            "Iteration: 1527, loss: 7.2401\n",
            "Iteration: 1528, loss: 7.3157\n",
            "Iteration: 1529, loss: 7.3836\n",
            "Iteration: 1530, loss: 7.3809\n",
            "Iteration: 1531, loss: 7.4207\n",
            "Iteration: 1532, loss: 7.4415\n",
            "Iteration: 1533, loss: 7.4749\n",
            "Iteration: 1534, loss: 7.4398\n",
            "Iteration: 1535, loss: 7.5442\n",
            "Iteration: 1536, loss: 7.2824\n",
            "Iteration: 1537, loss: 7.5263\n",
            "Iteration: 1538, loss: 7.6416\n",
            "Iteration: 1539, loss: 7.5360\n",
            "Iteration: 1540, loss: 7.4315\n",
            "Iteration: 1541, loss: 7.3329\n",
            "Iteration: 1542, loss: 7.4715\n",
            "Iteration: 1543, loss: 7.2707\n",
            "Iteration: 1544, loss: 7.5074\n",
            "Iteration: 1545, loss: 7.2518\n",
            "Iteration: 1546, loss: 7.5850\n",
            "Iteration: 1547, loss: 7.3249\n",
            "Iteration: 1548, loss: 7.4706\n",
            "Iteration: 1549, loss: 7.1853\n",
            "Iteration: 1550, loss: 7.5814\n",
            "Iteration: 1551, loss: 7.6663\n",
            "Iteration: 1552, loss: 7.5157\n",
            "Iteration: 1553, loss: 7.4049\n",
            "Iteration: 1554, loss: 7.4711\n",
            "Iteration: 1555, loss: 7.4788\n",
            "Iteration: 1556, loss: 7.5546\n",
            "Iteration: 1557, loss: 7.2546\n",
            "Iteration: 1558, loss: 7.3773\n",
            "Iteration: 1559, loss: 7.2248\n",
            "Iteration: 1560, loss: 7.2691\n",
            "Iteration: 1561, loss: 7.3593\n",
            "Iteration: 1562, loss: 7.3880\n",
            "Iteration: 1563, loss: 7.4733\n",
            "Iteration: 1564, loss: 7.3756\n",
            "Iteration: 1565, loss: 7.2750\n",
            "Iteration: 1566, loss: 7.5068\n",
            "Iteration: 1567, loss: 7.3775\n",
            "Iteration: 1568, loss: 7.1497\n",
            "Iteration: 1569, loss: 7.2558\n",
            "Iteration: 1570, loss: 7.3892\n",
            "Iteration: 1571, loss: 7.4549\n",
            "Iteration: 1572, loss: 7.5452\n",
            "Iteration: 1573, loss: 7.4382\n",
            "Iteration: 1574, loss: 7.3731\n",
            "Iteration: 1575, loss: 7.2834\n",
            "Iteration: 1576, loss: 7.6776\n",
            "Iteration: 1577, loss: 7.1684\n",
            "Iteration: 1578, loss: 7.4252\n",
            "Iteration: 1579, loss: 7.2370\n",
            "Iteration: 1580, loss: 7.4738\n",
            "Iteration: 1581, loss: 7.4497\n",
            "Iteration: 1582, loss: 7.4182\n",
            "Iteration: 1583, loss: 7.4007\n",
            "Iteration: 1584, loss: 7.3955\n",
            "Iteration: 1585, loss: 7.4294\n",
            "Iteration: 1586, loss: 7.4841\n",
            "Iteration: 1587, loss: 7.5135\n",
            "Iteration: 1588, loss: 7.4520\n",
            "Iteration: 1589, loss: 7.3973\n",
            "Iteration: 1590, loss: 7.4393\n",
            "Iteration: 1591, loss: 7.3635\n",
            "Iteration: 1592, loss: 7.3975\n",
            "Iteration: 1593, loss: 7.4248\n",
            "Iteration: 1594, loss: 7.5386\n",
            "Iteration: 1595, loss: 7.6664\n",
            "Iteration: 1596, loss: 7.4692\n",
            "Iteration: 1597, loss: 7.5995\n",
            "Iteration: 1598, loss: 7.5372\n",
            "Iteration: 1599, loss: 7.4431\n",
            "Iteration: 1600, loss: 7.4392\n",
            "Iteration: 1601, loss: 7.3709\n",
            "Iteration: 1602, loss: 7.4049\n",
            "Iteration: 1603, loss: 7.2706\n",
            "Iteration: 1604, loss: 7.4312\n",
            "Iteration: 1605, loss: 7.5409\n",
            "Iteration: 1606, loss: 7.3392\n",
            "Iteration: 1607, loss: 7.3508\n",
            "Iteration: 1608, loss: 7.4236\n",
            "Iteration: 1609, loss: 7.4315\n",
            "Iteration: 1610, loss: 7.4229\n",
            "Iteration: 1611, loss: 7.2637\n",
            "Iteration: 1612, loss: 7.3481\n",
            "Iteration: 1613, loss: 7.3165\n",
            "Iteration: 1614, loss: 7.2546\n",
            "Iteration: 1615, loss: 7.6518\n",
            "Iteration: 1616, loss: 7.3123\n",
            "Iteration: 1617, loss: 7.3355\n",
            "Iteration: 1618, loss: 7.4059\n",
            "Iteration: 1619, loss: 7.5231\n",
            "Iteration: 1620, loss: 7.3603\n",
            "Iteration: 1621, loss: 7.7262\n",
            "Iteration: 1622, loss: 7.5173\n",
            "Iteration: 1623, loss: 7.2891\n",
            "Iteration: 1624, loss: 7.3428\n",
            "Iteration: 1625, loss: 7.3804\n",
            "Iteration: 1626, loss: 7.6141\n",
            "Iteration: 1627, loss: 7.3956\n",
            "Iteration: 1628, loss: 7.2198\n",
            "Iteration: 1629, loss: 7.5088\n",
            "Iteration: 1630, loss: 7.4578\n",
            "Iteration: 1631, loss: 7.4681\n",
            "Iteration: 1632, loss: 7.3989\n",
            "Iteration: 1633, loss: 7.4981\n",
            "Iteration: 1634, loss: 7.4147\n",
            "Iteration: 1635, loss: 7.6463\n",
            "Iteration: 1636, loss: 7.6229\n",
            "Iteration: 1637, loss: 7.3775\n",
            "Iteration: 1638, loss: 7.3139\n",
            "Iteration: 1639, loss: 7.3991\n",
            "Iteration: 1640, loss: 7.5170\n",
            "Iteration: 1641, loss: 7.5702\n",
            "Iteration: 1642, loss: 7.2546\n",
            "Iteration: 1643, loss: 7.3877\n",
            "Iteration: 1644, loss: 7.5365\n",
            "Iteration: 1645, loss: 7.4207\n",
            "Iteration: 1646, loss: 7.5132\n",
            "Iteration: 1647, loss: 7.5819\n",
            "Iteration: 1648, loss: 7.5334\n",
            "Iteration: 1649, loss: 7.4477\n",
            "Iteration: 1650, loss: 7.3442\n",
            "Iteration: 1651, loss: 7.5553\n",
            "Iteration: 1652, loss: 7.4672\n",
            "Iteration: 1653, loss: 7.3729\n",
            "Iteration: 1654, loss: 7.4730\n",
            "Iteration: 1655, loss: 7.1869\n",
            "Iteration: 1656, loss: 7.4276\n",
            "Iteration: 1657, loss: 7.2202\n",
            "Iteration: 1658, loss: 7.4857\n",
            "Iteration: 1659, loss: 7.4114\n",
            "Iteration: 1660, loss: 7.2693\n",
            "Iteration: 1661, loss: 7.5165\n",
            "Iteration: 1662, loss: 7.4950\n",
            "Iteration: 1663, loss: 7.3992\n",
            "Iteration: 1664, loss: 7.1220\n",
            "Iteration: 1665, loss: 7.2836\n",
            "Iteration: 1666, loss: 7.5374\n",
            "Iteration: 1667, loss: 7.1809\n",
            "Iteration: 1668, loss: 7.3894\n",
            "Iteration: 1669, loss: 7.5308\n",
            "Iteration: 1670, loss: 7.5110\n",
            "Iteration: 1671, loss: 7.2079\n",
            "Iteration: 1672, loss: 7.3032\n",
            "Iteration: 1673, loss: 7.3365\n",
            "Iteration: 1674, loss: 7.2216\n",
            "Iteration: 1675, loss: 7.4424\n",
            "Iteration: 1676, loss: 7.4464\n",
            "Iteration: 1677, loss: 7.3845\n",
            "Iteration: 1678, loss: 7.4215\n",
            "Iteration: 1679, loss: 7.3401\n",
            "Iteration: 1680, loss: 7.3779\n",
            "Iteration: 1681, loss: 7.5478\n",
            "Iteration: 1682, loss: 7.5147\n",
            "Iteration: 1683, loss: 7.4623\n",
            "Iteration: 1684, loss: 7.2443\n",
            "Iteration: 1685, loss: 7.6095\n",
            "Iteration: 1686, loss: 7.3481\n",
            "Iteration: 1687, loss: 7.1890\n",
            "Iteration: 1688, loss: 7.4790\n",
            "Iteration: 1689, loss: 7.3441\n",
            "Iteration: 1690, loss: 7.2520\n",
            "Iteration: 1691, loss: 7.5336\n",
            "Iteration: 1692, loss: 7.3372\n",
            "Iteration: 1693, loss: 7.3821\n",
            "Iteration: 1694, loss: 7.4931\n",
            "Iteration: 1695, loss: 7.4973\n",
            "Iteration: 1696, loss: 7.2664\n",
            "Iteration: 1697, loss: 7.2176\n",
            "Iteration: 1698, loss: 7.6589\n",
            "Iteration: 1699, loss: 7.6677\n",
            "Iteration: 1700, loss: 7.3165\n",
            "Iteration: 1701, loss: 7.3314\n",
            "Iteration: 1702, loss: 7.2355\n",
            "Iteration: 1703, loss: 7.4148\n",
            "Iteration: 1704, loss: 7.1955\n",
            "Iteration: 1705, loss: 7.4272\n",
            "Iteration: 1706, loss: 7.4692\n",
            "Iteration: 1707, loss: 7.5001\n",
            "Iteration: 1708, loss: 7.1903\n",
            "Iteration: 1709, loss: 7.4584\n",
            "Iteration: 1710, loss: 7.5318\n",
            "Iteration: 1711, loss: 7.4227\n",
            "Iteration: 1712, loss: 7.5647\n",
            "Iteration: 1713, loss: 7.2451\n",
            "Iteration: 1714, loss: 7.5413\n",
            "Iteration: 1715, loss: 7.5909\n",
            "Iteration: 1716, loss: 7.2136\n",
            "Iteration: 1717, loss: 7.3267\n",
            "Iteration: 1718, loss: 7.5975\n",
            "Iteration: 1719, loss: 7.2897\n",
            "Iteration: 1720, loss: 7.4203\n",
            "Iteration: 1721, loss: 7.3953\n",
            "Iteration: 1722, loss: 7.4451\n",
            "Iteration: 1723, loss: 7.6493\n",
            "Iteration: 1724, loss: 7.3788\n",
            "Iteration: 1725, loss: 7.3909\n",
            "Iteration: 1726, loss: 7.4387\n",
            "Iteration: 1727, loss: 7.4107\n",
            "Iteration: 1728, loss: 7.3291\n",
            "Iteration: 1729, loss: 7.5536\n",
            "Iteration: 1730, loss: 7.4533\n",
            "Iteration: 1731, loss: 7.4630\n",
            "Iteration: 1732, loss: 7.2881\n",
            "Iteration: 1733, loss: 7.3838\n",
            "Iteration: 1734, loss: 7.3605\n",
            "Iteration: 1735, loss: 7.3555\n",
            "Iteration: 1736, loss: 7.3874\n",
            "Iteration: 1737, loss: 7.3791\n",
            "Iteration: 1738, loss: 7.4456\n",
            "Iteration: 1739, loss: 7.3519\n",
            "Iteration: 1740, loss: 7.3482\n",
            "Iteration: 1741, loss: 7.4716\n",
            "Iteration: 1742, loss: 7.4911\n",
            "Iteration: 1743, loss: 7.3885\n",
            "Iteration: 1744, loss: 7.2834\n",
            "Iteration: 1745, loss: 7.3402\n",
            "Iteration: 1746, loss: 7.4032\n",
            "Iteration: 1747, loss: 7.6546\n",
            "Iteration: 1748, loss: 7.4763\n",
            "Iteration: 1749, loss: 7.3074\n",
            "Iteration: 1750, loss: 7.6923\n",
            "Iteration: 1751, loss: 7.5320\n",
            "Iteration: 1752, loss: 7.4915\n",
            "Iteration: 1753, loss: 7.5257\n",
            "Iteration: 1754, loss: 7.3104\n",
            "Iteration: 1755, loss: 7.3130\n",
            "Iteration: 1756, loss: 7.3741\n",
            "Iteration: 1757, loss: 7.5417\n",
            "Iteration: 1758, loss: 7.1637\n",
            "Iteration: 1759, loss: 7.3721\n",
            "Iteration: 1760, loss: 7.1325\n",
            "Iteration: 1761, loss: 7.4887\n",
            "Iteration: 1762, loss: 7.4719\n",
            "Iteration: 1763, loss: 7.4957\n",
            "Iteration: 1764, loss: 7.3468\n",
            "Iteration: 1765, loss: 7.2000\n",
            "Iteration: 1766, loss: 7.5744\n",
            "Iteration: 1767, loss: 7.2370\n",
            "Iteration: 1768, loss: 7.3531\n",
            "Iteration: 1769, loss: 7.3430\n",
            "Iteration: 1770, loss: 7.5112\n",
            "Iteration: 1771, loss: 7.3641\n",
            "Iteration: 1772, loss: 7.2994\n",
            "Iteration: 1773, loss: 7.5292\n",
            "Iteration: 1774, loss: 7.3551\n",
            "Iteration: 1775, loss: 7.4295\n",
            "Iteration: 1776, loss: 7.4143\n",
            "Iteration: 1777, loss: 7.3061\n",
            "Iteration: 1778, loss: 7.2022\n",
            "Iteration: 1779, loss: 7.3404\n",
            "Iteration: 1780, loss: 7.2054\n",
            "Iteration: 1781, loss: 7.4233\n",
            "Iteration: 1782, loss: 7.6193\n",
            "Iteration: 1783, loss: 7.2217\n",
            "Iteration: 1784, loss: 7.3527\n",
            "Iteration: 1785, loss: 7.2252\n",
            "Iteration: 1786, loss: 7.2618\n",
            "Iteration: 1787, loss: 7.3705\n",
            "Iteration: 1788, loss: 7.3186\n",
            "Iteration: 1789, loss: 7.4638\n",
            "Iteration: 1790, loss: 7.3818\n",
            "Iteration: 1791, loss: 7.3629\n",
            "Iteration: 1792, loss: 7.4986\n",
            "Iteration: 1793, loss: 7.2212\n",
            "Iteration: 1794, loss: 7.4374\n",
            "Iteration: 1795, loss: 7.1195\n",
            "Iteration: 1796, loss: 7.3874\n",
            "Iteration: 1797, loss: 7.4119\n",
            "Iteration: 1798, loss: 7.6264\n",
            "Iteration: 1799, loss: 7.1936\n",
            "Iteration: 1800, loss: 7.4931\n",
            "Iteration: 1801, loss: 7.5601\n",
            "Iteration: 1802, loss: 7.2480\n",
            "Iteration: 1803, loss: 7.2503\n",
            "Iteration: 1804, loss: 7.4260\n",
            "Iteration: 1805, loss: 7.4709\n",
            "Iteration: 1806, loss: 7.5660\n",
            "Iteration: 1807, loss: 7.4436\n",
            "Iteration: 1808, loss: 7.4108\n",
            "Iteration: 1809, loss: 7.3261\n",
            "Iteration: 1810, loss: 7.4096\n",
            "Iteration: 1811, loss: 7.2173\n",
            "Iteration: 1812, loss: 7.4812\n",
            "Iteration: 1813, loss: 7.6353\n",
            "Iteration: 1814, loss: 7.4079\n",
            "Iteration: 1815, loss: 7.2997\n",
            "Iteration: 1816, loss: 7.3459\n",
            "Iteration: 1817, loss: 7.5633\n",
            "Iteration: 1818, loss: 7.3561\n",
            "Iteration: 1819, loss: 7.3079\n",
            "Iteration: 1820, loss: 7.3585\n",
            "Iteration: 1821, loss: 7.4468\n",
            "Iteration: 1822, loss: 7.4710\n",
            "Iteration: 1823, loss: 7.3759\n",
            "Iteration: 1824, loss: 7.2079\n",
            "Iteration: 1825, loss: 7.3073\n",
            "Iteration: 1826, loss: 7.4134\n",
            "Iteration: 1827, loss: 7.2365\n",
            "Iteration: 1828, loss: 7.5492\n",
            "Iteration: 1829, loss: 7.5732\n",
            "Iteration: 1830, loss: 7.3509\n",
            "Iteration: 1831, loss: 7.2569\n",
            "Iteration: 1832, loss: 7.5553\n",
            "Iteration: 1833, loss: 7.4345\n",
            "Iteration: 1834, loss: 7.3470\n",
            "Iteration: 1835, loss: 7.4758\n",
            "Iteration: 1836, loss: 7.3605\n",
            "Iteration: 1837, loss: 7.4923\n",
            "Iteration: 1838, loss: 7.5389\n",
            "Iteration: 1839, loss: 7.4336\n",
            "Iteration: 1840, loss: 7.3652\n",
            "Iteration: 1841, loss: 7.5991\n",
            "Iteration: 1842, loss: 7.4751\n",
            "Iteration: 1843, loss: 7.3349\n",
            "Iteration: 1844, loss: 7.4894\n",
            "Iteration: 1845, loss: 7.3999\n",
            "Iteration: 1846, loss: 7.2438\n",
            "Iteration: 1847, loss: 7.3638\n",
            "Iteration: 1848, loss: 7.2163\n",
            "Iteration: 1849, loss: 7.3871\n",
            "Iteration: 1850, loss: 7.1853\n",
            "Iteration: 1851, loss: 7.3388\n",
            "Iteration: 1852, loss: 7.4253\n",
            "Iteration: 1853, loss: 7.3858\n",
            "Iteration: 1854, loss: 7.1721\n",
            "Iteration: 1855, loss: 7.4868\n",
            "Iteration: 1856, loss: 7.2983\n",
            "Iteration: 1857, loss: 7.2903\n",
            "Iteration: 1858, loss: 7.3745\n",
            "Iteration: 1859, loss: 7.2470\n",
            "Iteration: 1860, loss: 7.2665\n",
            "Iteration: 1861, loss: 7.3533\n",
            "Iteration: 1862, loss: 7.3693\n",
            "Iteration: 1863, loss: 7.5384\n",
            "Iteration: 1864, loss: 7.3222\n",
            "Iteration: 1865, loss: 7.3408\n",
            "Iteration: 1866, loss: 7.3683\n",
            "Iteration: 1867, loss: 7.4508\n",
            "Iteration: 1868, loss: 7.2609\n",
            "Iteration: 1869, loss: 7.2346\n",
            "Iteration: 1870, loss: 7.3352\n",
            "Iteration: 1871, loss: 7.5115\n",
            "Iteration: 1872, loss: 7.5765\n",
            "Iteration: 1873, loss: 7.4727\n",
            "Iteration: 1874, loss: 7.3478\n",
            "Iteration: 1875, loss: 7.1160\n",
            "Iteration: 1876, loss: 7.4081\n",
            "Iteration: 1877, loss: 7.5966\n",
            "Iteration: 1878, loss: 7.2600\n",
            "Iteration: 1879, loss: 7.4913\n",
            "Iteration: 1880, loss: 7.3016\n",
            "Iteration: 1881, loss: 7.3459\n",
            "Iteration: 1882, loss: 7.2685\n",
            "Iteration: 1883, loss: 7.5391\n",
            "Iteration: 1884, loss: 7.3539\n",
            "Iteration: 1885, loss: 7.5141\n",
            "Iteration: 1886, loss: 7.6088\n",
            "Iteration: 1887, loss: 7.3558\n",
            "Iteration: 1888, loss: 7.3985\n",
            "Iteration: 1889, loss: 7.2271\n",
            "Iteration: 1890, loss: 7.2728\n",
            "Iteration: 1891, loss: 7.6364\n",
            "Iteration: 1892, loss: 7.4369\n",
            "Iteration: 1893, loss: 7.3909\n",
            "Iteration: 1894, loss: 7.6271\n",
            "Iteration: 1895, loss: 7.3945\n",
            "Iteration: 1896, loss: 7.4415\n",
            "Iteration: 1897, loss: 7.3418\n",
            "Iteration: 1898, loss: 7.4478\n",
            "Iteration: 1899, loss: 7.3801\n",
            "Iteration: 1900, loss: 7.2042\n",
            "Iteration: 1901, loss: 7.4117\n",
            "Iteration: 1902, loss: 7.4246\n",
            "Iteration: 1903, loss: 7.4188\n",
            "Iteration: 1904, loss: 7.3390\n",
            "Iteration: 1905, loss: 7.3012\n",
            "Iteration: 1906, loss: 7.3391\n",
            "Iteration: 1907, loss: 7.5710\n",
            "Iteration: 1908, loss: 7.2384\n",
            "Iteration: 1909, loss: 7.4799\n",
            "Iteration: 1910, loss: 7.4665\n",
            "Iteration: 1911, loss: 7.4428\n",
            "Iteration: 1912, loss: 7.5154\n",
            "Iteration: 1913, loss: 7.2287\n",
            "Iteration: 1914, loss: 7.5187\n",
            "Iteration: 1915, loss: 7.3524\n",
            "Iteration: 1916, loss: 7.4567\n",
            "Iteration: 1917, loss: 7.3833\n",
            "Iteration: 1918, loss: 7.3727\n",
            "Iteration: 1919, loss: 7.6289\n",
            "Iteration: 1920, loss: 7.5056\n",
            "Iteration: 1921, loss: 7.5101\n",
            "Iteration: 1922, loss: 7.3606\n",
            "Iteration: 1923, loss: 7.5840\n",
            "Iteration: 1924, loss: 7.3223\n",
            "Iteration: 1925, loss: 7.1774\n",
            "Iteration: 1926, loss: 7.4630\n",
            "Iteration: 1927, loss: 7.5961\n",
            "Iteration: 1928, loss: 7.5530\n",
            "Iteration: 1929, loss: 7.4770\n",
            "Iteration: 1930, loss: 7.5783\n",
            "Iteration: 1931, loss: 7.4848\n",
            "Iteration: 1932, loss: 7.3635\n",
            "Iteration: 1933, loss: 7.3973\n",
            "Iteration: 1934, loss: 7.3016\n",
            "Iteration: 1935, loss: 7.6135\n",
            "Iteration: 1936, loss: 7.1443\n",
            "Iteration: 1937, loss: 7.3963\n",
            "Iteration: 1938, loss: 7.3157\n",
            "Iteration: 1939, loss: 7.5462\n",
            "Iteration: 1940, loss: 7.3212\n",
            "Iteration: 1941, loss: 7.3700\n",
            "Iteration: 1942, loss: 7.4060\n",
            "Iteration: 1943, loss: 7.1094\n",
            "Iteration: 1944, loss: 7.3576\n",
            "Iteration: 1945, loss: 7.4444\n",
            "Iteration: 1946, loss: 7.5658\n",
            "Iteration: 1947, loss: 7.3409\n",
            "Iteration: 1948, loss: 7.3908\n",
            "Iteration: 1949, loss: 7.2729\n",
            "Iteration: 1950, loss: 7.3857\n",
            "Iteration: 1951, loss: 7.5086\n",
            "Iteration: 1952, loss: 7.2291\n",
            "Iteration: 1953, loss: 7.4171\n",
            "Iteration: 1954, loss: 7.4594\n",
            "Iteration: 1955, loss: 7.3476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cHKNLccJQr82"
      },
      "source": [
        "### 2.1.4. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwicNPkIqd1",
        "colab_type": "code",
        "outputId": "a10d7ad5-06ea-4037-bfd9-8b8bb7f04681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "#Hint: you can refer lab1 to know how to get the weight from a Model Linear layer\n",
        "weight1 = skip_gram_model.linear1.weight.data\n",
        "trained_embeddings = weight1.detach().T.numpy()\n",
        "np.save('word_embedding.npy',trained_embeddings)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7dfd07e8fbdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mweight1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskip_gram_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrained_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word_embedding.npy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrained_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'skip_gram_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T0ap96aeGlIk"
      },
      "source": [
        "## 2.2. Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d16v3oKaGlI0"
      },
      "source": [
        "### 2.2.1. Data Preprocessing for Character Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AKbLnN-3GlI1"
      },
      "source": [
        "*You are required to describe which preprocessing techniques were used with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i2CUCL1cGlI2",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zgiOPcsTGlI6"
      },
      "source": [
        "### 2.2.2. Build Character Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4NtqFFcjGlI7"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jj3YZ3PWGlI8",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "46W0zFfWGlI_"
      },
      "source": [
        "### 2.1.4. Train Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UWQn-VyNGlJA",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R5Bym9bBGlJE"
      },
      "source": [
        "### 2.1.5. Save Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ggTsYIm7GlJF",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JwOI-wIKGlJI"
      },
      "source": [
        "### 2.1.6. Load Character Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-jyj-lOHWWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "source": [
        "## 2.3. Sequence model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1. Apply/Import Word Embedding and Character Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAMJrxx-iOVn",
        "colab_type": "text"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7PKX1gIePA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2. Build Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R204UIyDKhZ4",
        "colab_type": "text"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.3. Train Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQnUSX1LZ6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.4. Save Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.5. Load Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtNxLzDGMCan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Evaluation\n",
        "\n",
        "(*Please show your empirical evidence*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Performance Evaluation\n",
        "\n",
        "\n",
        "You are required to provide the table with precision, recall, f1 of test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Hyperparameter Testing\n",
        "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb",
        "colab_type": "text"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX",
        "colab_type": "text"
      },
      "source": [
        "*You can use multiple code snippets. Just add more if needed* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1hVmx4E52dXS",
        "colab": {}
      },
      "source": [
        "# If you used OOP style, use this section"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}