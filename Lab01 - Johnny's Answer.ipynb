{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/Lab01%20-%20Johnny's%20Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-tNXEbFOf-g",
        "colab_type": "text"
      },
      "source": [
        "# Lab 01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vmb3ZK5SOhz",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jJgT_lITGdG",
        "colab_type": "text"
      },
      "source": [
        "[PyTorch](https://pytorch.org/) is an open source machine learning library used for applications such as natural language processing and computer vision. It is based on the [Torch](http://torch.ch/) library.\n",
        "\n",
        "Before we use Pytorch it is (obviously) neccessary to understand what Pytorch is. Let's start from the two core concepts: **Tensor**, **(Computational) Graph** and **Automatic Differentiation**\n",
        "\n",
        "\n",
        "## Tensor\n",
        "A tensor is a generalization of vectors and matrices to potentially higher dimensions. It is the primary data structure used by neural networks. Normally, we can use **nd**-tensor to call any of its instances where **nd** stands for **n** **dimensional**.\n",
        "\n",
        "There are three basic attributes we need to know about tensors:\n",
        "*   *Rank*: The number of dimensions present within the tensor. e.g. rank-2 tensor means 2d-tensor.\n",
        "*   *Axes*: Used to refer to a specific dimensions. The number of axes equals to the number of dimensions. The length of an axis represents the number of elements running along this axis. \n",
        "*   *Shape*: Formed by the length of each axis. e.g. shape(1,2) means a 2d-tensor with the first axis of length 1 and the second axis of length 2. \n",
        "\n",
        "\n",
        "![Tensor_Rank](https://drive.google.com/uc?id=1o5wulLHGxUuPxH3t3xfV8U7d2CrsL1oG)\n",
        "\n",
        "\n",
        "A [torch.Tensor](https://pytorch.org/docs/stable/tensors.html) (tensor in PyTorch) has the following key properties : \n",
        "*   *torch.dtype*: an object representing the data type of a torch.Tensor. e.g. torch.float32\n",
        "*   *torch.device*: an object representing the device on which a torch.Tensor is or will be allocated. e.g. CPU or CUDA (GPU)\n",
        "*   *torch.layout*: an object representing the memory layout of a torch.Tensor.\n",
        "\n",
        "More details with illustrative examples can be found [here](https://pytorch.org/docs/stable/tensor_attributes.html#tensor-attributes-doc) \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lONWx3FtSwWJ",
        "colab_type": "text"
      },
      "source": [
        "## Computational Graph and Automatic Differentiation\n",
        "PyTorch uses (directed acyclic) computational graphs  to graph the functional operations that are applied to tensors inside neural networks so as to computationally calculate derivatives for the network optimization. In graphs, the nodes are Tensors while the eages are functions that produce output Tensors from input Tensors (e.g. summation, mutiplication). Those graphs enable PyTorch to do the automatic differentiation for us, i.e. it can automatically calculate the derivatives that are needed for network optimization. We will learn more about it through pratical examples in following sections.\n",
        "\n",
        "Specifically, PyTorch generates the computational graph on the fly as when operations are created during forward passes in neural networks, which is refered as dynamic computational graph. This is the one of the main differences between PyTorch and TensorFlow which uses static computational graphs.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWQxFb_W8flk",
        "colab_type": "text"
      },
      "source": [
        "## Importing PyTorch library\n",
        "[Google Colab](https://colab.research.google.com/notebooks/welcome.ipynb) has torch library installed as default so you just need to import it as below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "horLjqk6TNZX",
        "colab_type": "code",
        "outputId": "8dcd1c53-779f-4cf1-bce3-0f0d09ee1513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "print(torch.__version__) #check version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPp2MGCe1apL",
        "colab_type": "text"
      },
      "source": [
        "## Tensor creation\n",
        "With PyTorch, we will be implementing lots of models. To get started, let's  have a look at how to create a tensor.\n",
        "\n",
        "We can creating tensors **with numerical data**, typically numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSymfXYxUT-8",
        "colab_type": "code",
        "outputId": "cc5597a5-81af-42a2-c17b-1aeee9ee356c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Scalar (0 Rank)\n",
        "data = torch.tensor(1)\n",
        "print(data.shape)\n",
        "\n",
        "\n",
        "# Vector (1 Rank)\n",
        "data = np.array([1,2]) \n",
        "data = torch.Tensor(data)\n",
        "print(data.shape)\n",
        "\n",
        "\n",
        "# Matrix (2 Rank)\n",
        "data = np.ones((2,2,)) \n",
        "data = torch.Tensor(data)\n",
        "print(data.shape)\n",
        "\n",
        "\n",
        "# Cube (3 Rank)\n",
        "data = np.ones((2,2,2)) \n",
        "data = torch.Tensor(data)\n",
        "print(data.shape)\n",
        "\n",
        "\n",
        "# Vector of cubes (4 Rank)\n",
        "data = np.ones((2,2,2,2)) \n",
        "data = torch.Tensor(data)\n",
        "print(data.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([])\n",
            "torch.Size([2])\n",
            "torch.Size([2, 2])\n",
            "torch.Size([2, 2, 2])\n",
            "torch.Size([2, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LEAyEhrn03F",
        "colab_type": "text"
      },
      "source": [
        "Notice that although both torch.Tensor() and torch.tensor() can be used to are used to generate tensors, there are some differences:\n",
        "\n",
        "*   uppercase t VS. lowercase T (obviously)\n",
        "*   torch.Tensor() is the constructor of the torch.Tensor class while the torch.tensor() is a factory function that constructs torch.Tensor objects and return them to the caller \n",
        "*   torch.Tensor() can return an empty tensor without specifying incoming data while torch.tensor() with no input data will prouduce a TypeError (you can try)\n",
        "*   torch.Tensor uses the default dtype \"float32\" while the torch.tensor() choose the same dtype based on the incoming data (type inference), this can be easily illustrated thorugh following example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxsULaaptyF7",
        "colab_type": "code",
        "outputId": "afc5a42c-116e-43b2-b3c7-f7bad4161384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "data = np.array([1,2]) \n",
        "print(data.dtype)\n",
        "\n",
        "data_T = torch.Tensor(data)\n",
        "print(data_T)\n",
        "print(data_T.dtype)\n",
        "print(data_T.dtype==torch.get_default_dtype()) #get the torch default data type, which can also be changed through 'torch.set_default_dtype(dtype)'\n",
        "print()\n",
        "\n",
        "data_t = torch.tensor(data)\n",
        "print(data_t)\n",
        "print(data_t.dtype)\n",
        "print()\n",
        "\n",
        "#we can also specify a datatype with torch.tensor()\n",
        "data_t = torch.tensor(data, dtype=torch.float64)\n",
        "print(data_t)\n",
        "print(data_t.dtype)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "int64\n",
            "tensor([1., 2.])\n",
            "torch.float32\n",
            "True\n",
            "\n",
            "tensor([1, 2])\n",
            "torch.int64\n",
            "\n",
            "tensor([1., 2.], dtype=torch.float64)\n",
            "torch.float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE2tWt771BYk",
        "colab_type": "text"
      },
      "source": [
        "Expect for torch.Tensor() and torch.tensor(), We can also use torch.as_tensor and torch.from_numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE5zySUlbYPK",
        "colab_type": "code",
        "outputId": "7474b436-e911-4a14-f5dd-6a202c46cb4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Create tensor using torch.as_tensor \n",
        "data = np.ones((2,2,2)) \n",
        "data = torch.as_tensor(data)\n",
        "print(data.shape)\n",
        "\n",
        "# Create tensor using torch.from_numpy\n",
        "data = np.ones((2,2,2,2))\n",
        "data = torch.from_numpy(data)\n",
        "print(data.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 2, 2])\n",
            "torch.Size([2, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-hrVh5Eb50F",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, we can also create tensors **without data** using factory functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGnl32afcBnv",
        "colab_type": "code",
        "outputId": "5e9ea5aa-2634-4993-8167-5ed23e0b7482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# torch.eye: Returns an identity matrix \n",
        "torch.eye(2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0.],\n",
              "        [0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8QlyKrPcxs_",
        "colab_type": "code",
        "outputId": "a2b15f8e-ab5c-450a-df36-3fd670cbb558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# torch.zeros: Returns a tensor of given shape filled with all zeros\n",
        "torch.zeros(2,2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rO_IkTuSc66X",
        "colab_type": "code",
        "outputId": "db3c8181-7c6f-40f0-ed8b-132b578b311a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# torch.ones: Returns a tensor of given shape filled with all ones\n",
        "torch.ones(2,2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZqJpoIXc8wL",
        "colab_type": "code",
        "outputId": "2df0b530-33cf-45e7-cbd0-88c6eb79adf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# torch.rand: Returns a tensor of given shape filled with values drawn from a uniform distribution on [0, 1).\n",
        "torch.rand(2,2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4698, 0.8208],\n",
              "        [0.9462, 0.4228]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTOQtikExO7M",
        "colab_type": "text"
      },
      "source": [
        "More factory functions for tensor creation can be found [here](https://pytorch.org/cppdocs/notes/tensor_creation.html#factory-functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bff_DPKHbSvY",
        "colab_type": "text"
      },
      "source": [
        "## Basic tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKz1UOFEHYVr",
        "colab_type": "text"
      },
      "source": [
        "The list of operations with examples can be found [here](https://pytorch.org/docs/stable/torch.html#math-operations). Please go through and try to practice yourself with examples before you move on. You don't need to remember all of them, you can easily refer back when needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE91VTvN_YVB",
        "colab_type": "text"
      },
      "source": [
        "## Simple Linear Regression\n",
        "\n",
        "![Linear_Regression](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/320px-Linear_regression.svg.png)\n",
        "\n",
        "The following code implements a simple linear regression algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fXyR-6lGlie",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALnKOtUpGZyk",
        "colab_type": "text"
      },
      "source": [
        "Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK900tGJBcbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# training data\n",
        "x_training = numpy.asarray([1,2,5,8,9,12,14,16,18,20])\n",
        "y_training = numpy.asarray([1500,3500,7200,11000,12500,18500,22000,24500,28000,30500])\n",
        "\n",
        "x_test = numpy.asarray([3,7,13,15,19])\n",
        "y_test = numpy.asarray([4400,10000,19500,23500,29000])\n",
        "\n",
        "# creating tensor for trainig from training data\n",
        "x_data = torch.from_numpy(x_training)\n",
        "y_data = torch.from_numpy(y_training)\n",
        "x_test_data = torch.from_numpy(x_test)\n",
        "y_test_data = torch.from_numpy(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4DA9NoJG10G",
        "colab_type": "text"
      },
      "source": [
        " Once the dataset is prepared, we can start defining our model architecture\n",
        " \n",
        " Let's first build it from scratch so as to gain a clear understanding about how the automatic differentiation works\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzlkNOOpGk8r",
        "colab_type": "code",
        "outputId": "7f0f92e4-aa4f-476b-d9f1-099d2d68346c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Define weights and biases\n",
        "weight = torch.tensor(numpy.random.randn(), requires_grad=True)\n",
        "bias = torch.tensor(numpy.random.randn(), requires_grad=True)\n",
        "print(weight)\n",
        "print(bias)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-0.3148, requires_grad=True)\n",
            "tensor(-2.8430, requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p3lWE7DL_c2",
        "colab_type": "text"
      },
      "source": [
        "Note that we set 'requires_grad=True' above, which turns on the automatic gradient computation for weight and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnGNiyccJEU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the model\n",
        "# Hypothesis = W * X + b (Linear Model)\n",
        "def linearRegression(x):\n",
        "  return x * weight + bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYVxYYEaJw_9",
        "colab_type": "code",
        "outputId": "0d67459e-0655-40a5-a2da-bfa9d21499bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Generate predictions and compare with ground truth labels\n",
        "# As we can see, we randomly initialise the weight and bias, the model does not predict properly at the moment\n",
        "predictions = linearRegression(x_data)\n",
        "print(predictions)\n",
        "print(y_data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-3.1578, -3.4726, -4.4171, -5.3615, -5.6763, -6.6208, -7.2504, -7.8801,\n",
            "        -8.5097, -9.1393], grad_fn=<AddBackward0>)\n",
            "tensor([ 1500,  3500,  7200, 11000, 12500, 18500, 22000, 24500, 28000, 30500])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rjCmeI3KxEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define loss function \n",
        "# here we use mean squared error (MSE)\n",
        "def mse(x1, x2):\n",
        "  diff = x1 - x2\n",
        "  return torch.sum(diff*diff)/diff.numel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnnZ_W8yLPLw",
        "colab_type": "code",
        "outputId": "04228bb3-23dc-4611-c039-2b3c5e1fb544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Compute loss\n",
        "# As we all know, the lower, the better\n",
        "loss = mse(predictions, y_data)\n",
        "print(loss)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3.4867e+08, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua8D9hd2MmSb",
        "colab_type": "text"
      },
      "source": [
        "As is mentioned, PyTorch automatically compute the gradient/derivative of the loss (with regard to the weight and bias here). This was enabled when we set 'requires_grad=True' to them.\n",
        "\n",
        "All we need to do now is to call the backward() over our loss, which will trigger the automatical computation of gradients based on the chain rule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ5PL5rbNJ09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute gradients\n",
        "loss.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwuD_tYMNnZP",
        "colab_type": "text"
      },
      "source": [
        "After the backward passing, the gradients are stored in the .grad property of the involved tensors. Let's have a look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5glqZhkN1Fl",
        "colab_type": "code",
        "outputId": "fc5018dc-20de-47d1-8d4a-2b7fb1f11ed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Gradient for weight\n",
        "print(weight)\n",
        "print(weight.grad)\n",
        "print()\n",
        "\n",
        "# Gradient for bias\n",
        "print(bias)\n",
        "print(bias.grad)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(-0.3148, requires_grad=True)\n",
            "tensor(-456353.8750)\n",
            "\n",
            "tensor(-2.8430, requires_grad=True)\n",
            "tensor(-31852.2969)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvHjhErrPiDk",
        "colab_type": "text"
      },
      "source": [
        "Now we can easily adjust the weight and bias using the gradients.\n",
        "\n",
        "We need to reset the gradients after before next forward pass, because PyTorch accumulates gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_HXZkOUP-vB",
        "colab_type": "code",
        "outputId": "3c5fe0bb-6e73-4709-91fc-7ae6d2c2a594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# We do not want gradient for the updation operation\n",
        "# There will not be automatic gradient computation within the torch.no_grad() \n",
        "\n",
        "with torch.no_grad(): \n",
        "  weight -= weight.grad * 1e-5\n",
        "  bias -= bias * 1e-5\n",
        "  # remember to reset the gradients\n",
        "  weight.grad.zero_()\n",
        "  bias.grad.zero_()\n",
        "print(weight)\n",
        "print(bias)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4.2487, requires_grad=True)\n",
            "tensor(-2.8430, requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSj4yiSSSOJj",
        "colab_type": "code",
        "outputId": "0de791bc-1089-43cb-9681-2f81fa30f35b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(weight.grad)\n",
        "print(bias.grad)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.)\n",
            "tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rmo63CASlPp",
        "colab_type": "text"
      },
      "source": [
        "Let's predict and compute loss again. The loss should be lower with new weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRBatrmLSxmW",
        "colab_type": "code",
        "outputId": "fd7cb80f-3091-488c-c4f2-88865c4e4b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "predictions = linearRegression(x_data)\n",
        "loss = mse(predictions, y_data)\n",
        "print(loss)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(3.4659e+08, grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE05OxGETIGd",
        "colab_type": "text"
      },
      "source": [
        "Hope you now have an initial clear understanding of how automatic gradient computing works.\n",
        "\n",
        "Let's start training the model for multiple epochs.\n",
        "\n",
        "We can just simply create python loop to do it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glakWvo4TmmU",
        "colab_type": "code",
        "outputId": "a56eb21a-0f46-4292-8c15-d0e56cf88376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# An epoch is one iteration over the entire input data\n",
        "no_of_epochs = 5000\n",
        "# How often you want to display training info.\n",
        "display_interval = 200\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  predictions = linearRegression(x_data)\n",
        "  loss = mse(predictions, y_data)\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    weight -= weight.grad * 1e-5\n",
        "    bias -= bias.grad * 1e-5\n",
        "    weight.grad.zero_()\n",
        "    bias.grad.zero_()\n",
        "  if epoch % display_interval == 0 :\n",
        "      # calculate the cost of the current model\n",
        "      predictions = linearRegression(x_data)\n",
        "      loss = mse(predictions, y_data)           \n",
        "      print(\"Epoch:\", '%04d' %(epoch), \"loss=\", \"{:.8f}\".format(loss), \"W=\", \"{:.4f}\".format(weight), \"b=\",  \"{:.4f}\".format(bias))\n",
        "\n",
        "print(\"=========================================================\")\n",
        "training_loss = mse(linearRegression(x_data), y_data)   \n",
        "print(\"Optimised:\", \"lost=\", \"{:.9f}\".format(training_loss.data), \\\n",
        "              \"W=\", \"{:.9f}\".format(weight.data), \"b=\", \"{:.9f}\".format(bias.data))\n",
        "    \n",
        "# Plot training data on the graph\n",
        "plt.plot(x_training, y_training, 'ro', label='Training data')\n",
        "plt.plot(x_training, weight.data * x_training + bias.data, label='Linear')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate testing loss\n",
        "testing_loss = mse(linearRegression(x_test_data), y_test_data) \n",
        "print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "      training_loss.data - testing_loss.data)))\n",
        "  \n",
        "# Plot testing data on the graph\n",
        "plt.plot(x_test, y_test, 'bo', label='Testing data')\n",
        "plt.plot(x_test, weight.data * x_test + bias.data, label='Linear')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0000 loss= 344511584.00000000 W= 8.7986 b= -2.5254\n",
            "Epoch: 0200 loss= 103682920.00000000 W= 691.4930 b= 44.9967\n",
            "Epoch: 0400 loss= 31415994.00000000 W= 1065.4833 b= 70.7999\n",
            "Epoch: 0600 loss= 9730302.00000000 W= 1270.3684 b= 84.7059\n",
            "Epoch: 0800 loss= 3222821.75000000 W= 1382.6195 b= 92.0949\n",
            "Epoch: 1000 loss= 1270010.75000000 W= 1444.1256 b= 95.9142\n",
            "Epoch: 1200 loss= 683933.43750000 W= 1477.8341 b= 97.7782\n",
            "Epoch: 1400 loss= 507978.34375000 W= 1496.3149 b= 98.5714\n",
            "Epoch: 1600 loss= 455091.43750000 W= 1506.4542 b= 98.7783\n",
            "Epoch: 1800 loss= 439128.75000000 W= 1512.0253 b= 98.6643\n",
            "Epoch: 2000 loss= 434250.18750000 W= 1515.0929 b= 98.3746\n",
            "Epoch: 2200 loss= 432698.00000000 W= 1516.7892 b= 97.9891\n",
            "Epoch: 2400 loss= 432143.09375000 W= 1517.7345 b= 97.5512\n",
            "Epoch: 2600 loss= 431888.93750000 W= 1518.2682 b= 97.0848\n",
            "Epoch: 2800 loss= 431723.93750000 W= 1518.5765 b= 96.6032\n",
            "Epoch: 3000 loss= 431586.68750000 W= 1518.7606 b= 96.1134\n",
            "Epoch: 3200 loss= 431457.56250000 W= 1518.8772 b= 95.6194\n",
            "Epoch: 3400 loss= 431331.40625000 W= 1518.9570 b= 95.1234\n",
            "Epoch: 3600 loss= 431205.68750000 W= 1519.0139 b= 94.6260\n",
            "Epoch: 3800 loss= 431081.15625000 W= 1519.0627 b= 94.1286\n",
            "Epoch: 4000 loss= 430956.34375000 W= 1519.1116 b= 93.6311\n",
            "Epoch: 4200 loss= 430832.50000000 W= 1519.1500 b= 93.1337\n",
            "Epoch: 4400 loss= 430708.59375000 W= 1519.1849 b= 92.6373\n",
            "Epoch: 4600 loss= 430584.93750000 W= 1519.2197 b= 92.1414\n",
            "Epoch: 4800 loss= 430461.50000000 W= 1519.2546 b= 91.6455\n",
            "=========================================================\n",
            "Optimised: lost= 430338.906250000 W= 1519.289184570 b= 91.153175354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngP8PGfSjIAEQU10AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Testing loss= 218367.125000000\n",
            "Absolute mean square loss difference: 211971.781250000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD6CAYAAABDPiuvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsDAwxhiDhYExxhjg/wOwgxh3t9O3cwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54_Tj-BIaDPU",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression using PyTorch built-ins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUrcVErtayVN",
        "colab_type": "text"
      },
      "source": [
        "Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DwoqFKbaiSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# training data: this time we use 2d array\n",
        "# Assuming we have 90 samples of 10 featues about a house condition, such as bedroom number, distance to city center etc.\n",
        "# and will predict the house price \n",
        "x_data = torch.randn(90, 10)\n",
        "y_data = torch.randn(90, 1)\n",
        "\n",
        "# testing data:\n",
        "x_test_data = torch.randn(10, 10)\n",
        "y_test_data = torch.randn(10, 1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79_tCTbKa_Go",
        "colab_type": "text"
      },
      "source": [
        "This time we don't need to initialize the weight and bias manually. Instead, we will define the model using the built-in [torch.nn.Linear](https://pytorch.org/docs/stable/nn.html#linear).\n",
        "\n",
        "[**torch.nn**](https://pytorch.org/docs/stable/nn.html) is a subpackage that contains modules and extensible classes for us to build neural networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA8sqMF_bOSL",
        "colab_type": "code",
        "outputId": "655a951c-96b5-4a4e-daa9-4f1b43bac611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Define model\n",
        "linearRegression =  nn.Linear(10,1)\n",
        "print(linearRegression.weight)\n",
        "print(linearRegression.bias)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.2248,  0.2368,  0.0493, -0.2252,  0.2220, -0.1025,  0.2685, -0.1953,\n",
            "         -0.2361, -0.1089]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0570], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4UxFi0bbeC3",
        "colab_type": "text"
      },
      "source": [
        "Similarly, we don't mannually updating the weight and bias using gradients by ouselves. Instead, we will use the optimizer optim.SGD.\n",
        "\n",
        "[**torhc.optim**](https://pytorch.org/docs/stable/optim.html) is a subpackage that contains the standard optimization operations like Adam and SGD.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k62ayy77bu5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define optimizer\n",
        "# Just pass the model parameters to be updated and specify the learning rate when calling optim.SGD\n",
        "# SGD optimizer in PyTorch actually is Mini-batch Gradient Descent with momentum.\n",
        "# In this case, as the batch size of our model is N, SGD here is actually Batch Gradient Descent.\n",
        "optimizer = torch.optim.SGD(linearRegression.parameters(), lr=1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfKM513UcH-4",
        "colab_type": "text"
      },
      "source": [
        "Again, we use the built-in loss function mse_loss instead of defining it mannually.\n",
        "\n",
        "We will need the [**torch.nn.functional**](https://pytorch.org/docs/stable/nn.functional.html) interface, which contains typical operations used for building neural networks such as convolution operations, activation functions and loss functions we need here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKWCYuiocOUS",
        "colab_type": "code",
        "outputId": "1136b9e0-0cc6-4622-b729-8af772d6725d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Import nn.functional \n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the loss function\n",
        "loss_func = F.mse_loss\n",
        "\n",
        "# Calculate loss\n",
        "loss = loss_func(linearRegression(x_data), y_data)\n",
        "print(loss)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.5314, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1pjqzZCNJgJ",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbYtLruwwRfr",
        "colab_type": "code",
        "outputId": "b7edb777-9a7f-4634-eb84-8dbbfad4d069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# An epoch is one iteration over the entire input data\n",
        "no_of_epochs = 5000\n",
        "# How often you want to display training info.\n",
        "display_interval = 200\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  predictions = linearRegression(x_data)\n",
        "  loss = loss_func(predictions, y_data)\n",
        "  loss.backward()\n",
        "  optimizer.step() #call step() to automatically update the parameters through our defined optimizer, which can be called once after backward()\n",
        "  optimizer.zero_grad() #reset the gradient as what we did before\n",
        "  if epoch % display_interval == 0 :\n",
        "      # calculate the cost of the current model\n",
        "      predictions = linearRegression(x_data)\n",
        "      loss = loss_func(predictions, y_data)          \n",
        "      print(\"Epoch:\", '%04d' % (epoch), \"loss=\", \"{:.8f}\".format(loss))\n",
        "\n",
        "print(\"=========================================================\")\n",
        "training_loss = mse(linearRegression(x_data), y_data)   \n",
        "print(\"Optimised:\", \"lost=\", \"{:.9f}\".format(training_loss.data))\n",
        "\n",
        "\n",
        "# Calculate testing loss\n",
        "testing_loss = loss_func(linearRegression(x_test_data), y_test_data) \n",
        "print(\"Testing loss=\", \"{:.9f}\".format(testing_loss.data))\n",
        "print(\"Absolute mean square loss difference:\", \"{:.9f}\".format(abs(\n",
        "      training_loss.data - testing_loss.data)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0000 loss= 1.53134489\n",
            "Epoch: 0200 loss= 1.52791440\n",
            "Epoch: 0400 loss= 1.52450633\n",
            "Epoch: 0600 loss= 1.52112305\n",
            "Epoch: 0800 loss= 1.51776445\n",
            "Epoch: 1000 loss= 1.51442885\n",
            "Epoch: 1200 loss= 1.51111603\n",
            "Epoch: 1400 loss= 1.50782609\n",
            "Epoch: 1600 loss= 1.50455809\n",
            "Epoch: 1800 loss= 1.50131297\n",
            "Epoch: 2000 loss= 1.49809146\n",
            "Epoch: 2200 loss= 1.49489164\n",
            "Epoch: 2400 loss= 1.49171436\n",
            "Epoch: 2600 loss= 1.48855805\n",
            "Epoch: 2800 loss= 1.48542333\n",
            "Epoch: 3000 loss= 1.48230934\n",
            "Epoch: 3200 loss= 1.47921944\n",
            "Epoch: 3400 loss= 1.47614968\n",
            "Epoch: 3600 loss= 1.47310162\n",
            "Epoch: 3800 loss= 1.47007322\n",
            "Epoch: 4000 loss= 1.46706510\n",
            "Epoch: 4200 loss= 1.46407747\n",
            "Epoch: 4400 loss= 1.46111226\n",
            "Epoch: 4600 loss= 1.45816648\n",
            "Epoch: 4800 loss= 1.45524025\n",
            "=========================================================\n",
            "Optimised: lost= 1.452348113\n",
            "Testing loss= 1.944238067\n",
            "Absolute mean square loss difference: 0.491889954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g3eNemEOklg",
        "colab_type": "text"
      },
      "source": [
        "# NLTK Library and WordNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8btJKEBxSS1",
        "colab_type": "text"
      },
      "source": [
        "WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofWRw7omOeWf",
        "colab_type": "text"
      },
      "source": [
        "In Python, NLTK library includes English WordNet.\n",
        "\n",
        "**To use wordnet, you need to download the wordnet data via NLTK library**\n",
        "\n",
        " * **[NLTK](https://www.nltk.org/)** is a **N**atural **L**anguage **T**ool**k**iit for python. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVqxuaIpOIOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2cusD7IovCi",
        "colab_type": "text"
      },
      "source": [
        "## WordNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuDwzN6OosjB",
        "colab_type": "code",
        "outputId": "b8dbf544-3943-43a0-9925-65ceee89592c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JKvb0XKNzxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import wordnet as wn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucVb1XeAnnFo",
        "colab_type": "text"
      },
      "source": [
        "Let's get a set of synonyms that share a common meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecoLLQPlxlOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dog = wn.synset('dog.n.01')\n",
        "person = wn.synset('person.n.01')\n",
        "cat = wn.synset('cat.n.01')\n",
        "computer = wn.synset('computer.n.01')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4rd7JPrr4sc",
        "colab_type": "text"
      },
      "source": [
        "### path_similarity()\n",
        "path_similarity() returns a score denoting how similar two word senses are, based on the shortest path that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiTFSsHqsREt",
        "colab_type": "code",
        "outputId": "f0bc533c-724d-476f-c2f5-2be84483aff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"dog<->cat : \", wn.path_similarity(dog,cat))\n",
        "print(\"person<->cat : \", wn.path_similarity(person,cat))\n",
        "print(\"person<->dog : \", wn.path_similarity(person,dog))\n",
        "print(\"person<->computer : \", wn.path_similarity(person,computer))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog<->cat :  0.2\n",
            "person<->cat :  0.1\n",
            "person<->dog :  0.2\n",
            "person<->computer :  0.1111111111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJfYbAtPrMNS",
        "colab_type": "text"
      },
      "source": [
        "### Wu-Palmer Similarity (wup_similarity() )\n",
        "wup_similarity() returns a score denoting how similar two word senses are, based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer (most specific ancestor node)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jno6d9Mdrbu0",
        "colab_type": "code",
        "outputId": "6c5072cb-6f41-4c6b-8250-5de634f76817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"dog<->cat : \", wn.wup_similarity(dog,cat))\n",
        "print(\"person<->cat : \", wn.wup_similarity(person,cat))\n",
        "print(\"person<->dog : \", wn.wup_similarity(person,dog))\n",
        "print(\"person<->computer : \", wn.wup_similarity(person,computer))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog<->cat :  0.8571428571428571\n",
            "person<->cat :  0.5714285714285714\n",
            "person<->dog :  0.75\n",
            "person<->computer :  0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVtVC7LnqS9T",
        "colab_type": "text"
      },
      "source": [
        "# TFIDF (Term Frequency Inverse Document Frequency)\n",
        "\n",
        "TFIDF is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
        "\n",
        "\n",
        "**Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcjJPxPIHsVo",
        "colab_type": "code",
        "outputId": "896b277e-8219-4142-abea-cfc95532313d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "corpus = [\n",
        "    'Caren loves the NLP. The NLP hates Caren', #document 1\n",
        "    'Caren hates the NLP' #document 2\n",
        "]\n",
        "\n",
        "#Tokenize sentences - for only doc1\n",
        "tokenized_sentence = sent_tokenize(corpus[0])\n",
        "print(\"\\ntokenized_sentence: \")\n",
        "print(tokenized_sentence)\n",
        "\n",
        "#Remove punctuations - for only doc1\n",
        "clean_doc1 = re.sub(r'[^\\w\\s]','',corpus[0])\n",
        "print(\"\\nclean_sentence: \")\n",
        "print(clean_doc1)\n",
        "\n",
        "#Tokenize words - for only doc1\n",
        "tokenized_doc1 = word_tokenize(clean_doc1)\n",
        "print(\"\\ntokenized_word: \")\n",
        "print(tokenized_doc1)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokenized_doc1]\n",
        "print(\"\\nlower_case: \")\n",
        "print(lower_tokens)\n",
        "\n",
        "#stop word removal\n",
        "sww = sw.words()\n",
        "tokenized_doc1 = [w for w in lower_tokens if not w in sww]\n",
        "print(\"\\ntokensized_word (in lower case, w/o stopwords): \")\n",
        "print(tokenized_doc1)\n",
        "\n",
        "#same process for doc2\n",
        "clean_doc2 = re.sub(r'[^\\w\\s]','',corpus[1])\n",
        "tokenized_doc2 = word_tokenize(clean_doc2)\n",
        "lower_tokens2 = [t.lower() for t in tokenized_doc2]\n",
        "tokenized_doc2 = [w for w in lower_tokens2 if not w in sww]\n",
        "\n",
        "tokensized_docs = [tokenized_doc1, tokenized_doc2]\n",
        "print(\"\\nfinal_docs: \")\n",
        "print(tokensized_docs[0])\n",
        "print(tokensized_docs[1])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "\n",
            "tokenized_sentence: \n",
            "['Caren loves the NLP.', 'The NLP hates Caren']\n",
            "\n",
            "clean_sentence: \n",
            "Caren loves the NLP The NLP hates Caren\n",
            "\n",
            "tokenized_word: \n",
            "['Caren', 'loves', 'the', 'NLP', 'The', 'NLP', 'hates', 'Caren']\n",
            "\n",
            "lower_case: \n",
            "['caren', 'loves', 'the', 'nlp', 'the', 'nlp', 'hates', 'caren']\n",
            "\n",
            "tokensized_word (in lower case, w/o stopwords): \n",
            "['caren', 'loves', 'nlp', 'nlp', 'hates', 'caren']\n",
            "\n",
            "final_docs: \n",
            "['caren', 'loves', 'nlp', 'nlp', 'hates', 'caren']\n",
            "['caren', 'hates', 'nlp']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSeX6VnhTeA5",
        "colab_type": "text"
      },
      "source": [
        "**Document Frequency (DF)**\n",
        "\n",
        "DF is the count of occurrences of term t in the document set N\n",
        "\n",
        "*df(t) = occurrence of t in documents*\n",
        "\n",
        "*idf(t) = log(N/(df + 1))*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEOTCuiZTdl6",
        "colab_type": "code",
        "outputId": "de91174e-677b-4940-94b2-546792f118e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "DF = {}\n",
        "\n",
        "for tokensized_doc in tokensized_docs:\n",
        "    # get each unique word in the doc - we need to know whether the word is appeared in the document\n",
        "    for term in np.unique(tokensized_doc):\n",
        "        try:\n",
        "            DF[term] +=1\n",
        "        except:\n",
        "            DF[term] =1\n",
        "\n",
        "DF"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'caren': 2, 'hates': 2, 'loves': 1, 'nlp': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6s15-FpdhrM",
        "colab_type": "code",
        "outputId": "b9850335-4fe4-470d-f841-f634af866464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "DF['caren']"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEpq2ws8Lsos",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF calculation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNeraCvfIO3E",
        "colab_type": "code",
        "outputId": "293b0212-68d0-4231-b96d-8f2e25524195",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "\n",
        "tf_idf = {}\n",
        "\n",
        "# total number of documents\n",
        "N = len(tokensized_docs)\n",
        "\n",
        "doc_id = 0\n",
        "# get each tokenised doc\n",
        "for tokensized_doc in tokensized_docs:\n",
        "    # initialise counter for the doc\n",
        "    counter = Counter(tokensized_doc)\n",
        "    # calculate total number of words in the doc\n",
        "    total_num_words = len(tokensized_doc)    \n",
        "\n",
        "    # get each unique word in the doc\n",
        "    for term in np.unique(tokensized_doc):\n",
        "\n",
        "        #calculate Term Frequency \n",
        "        tf = counter[term]/total_num_words\n",
        "        \n",
        "        #calculate Document Frequency\n",
        "        df = DF[term]\n",
        "\n",
        "        # calculate Inverse Document Frequency\n",
        "        idf = math.log(N/(df+1))+1\n",
        "\n",
        "        # calculate TF-IDF\n",
        "        tf_idf[doc_id, term] = tf*idf\n",
        "\n",
        "    doc_id += 1\n",
        "\n",
        "tf_idf"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 'caren'): 0.19817829729727854,\n",
              " (0, 'hates'): 0.09908914864863927,\n",
              " (0, 'loves'): 0.16666666666666666,\n",
              " (0, 'nlp'): 0.19817829729727854,\n",
              " (1, 'caren'): 0.19817829729727854,\n",
              " (1, 'hates'): 0.19817829729727854,\n",
              " (1, 'nlp'): 0.19817829729727854}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tETfBpygMC-u",
        "colab_type": "text"
      },
      "source": [
        "**Sort by the importance - Descending Order**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R4BI1-UH35-",
        "colab_type": "code",
        "outputId": "e394e364-4282-4b13-919b-97018fc787e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "import numpy as np\n",
        "#sort the dictionary based on values\n",
        "dict_exmaple = tf_idf\n",
        "sorted_dict = sorted(dict_exmaple.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_dict"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((0, 'caren'), 0.19817829729727854),\n",
              " ((0, 'nlp'), 0.19817829729727854),\n",
              " ((1, 'caren'), 0.19817829729727854),\n",
              " ((1, 'hates'), 0.19817829729727854),\n",
              " ((1, 'nlp'), 0.19817829729727854),\n",
              " ((0, 'loves'), 0.16666666666666666),\n",
              " ((0, 'hates'), 0.09908914864863927)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqD22EW5PZRa",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "**Write a function which returns the top N (e.g. 10 or 20) words with the largest tf value and with the largest tfidf values for a paragraph of corpus (Wikipedia page - Sydney and Melbourne).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAd2bqFR3CfJ",
        "colab_type": "code",
        "outputId": "43036f6d-6d1c-454e-9ad4-3b2b2110e0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "!pip install wikipedia\n",
        "import wikipedia\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=b833502b1baecea010ddc8f8a6ab3131d0565edc57eec8c6cad43441e04e74db\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JSYTirTx6ML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cities = [\"Canberra\",\"Perth\", \"Sydney\", \"Melbourne\", \"Darwin,_Northern_Territory\", \"Brisbane\", \"Hobart\"]\n",
        "corpus = [wikipedia.page(city).content for city in cities]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgdayfSF_RW-",
        "colab_type": "code",
        "outputId": "63cdf179-bb47-481c-84b7-1deae28864c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "# You should submit \"ipynb\" file (You can download it from \"File\" > \"Download .ipynb\") to Canvas\n",
        "# You can write extra functions if you need\n",
        "\n",
        "#pass both the corpus and the number of words to focus into the function\n",
        "#In the function, you may:\n",
        "  #process the corpus (e.g. tokenization) - provided\n",
        "  #calculate the tf/tfidf values for each unique words\n",
        "  #sorting the words based on the tf/tfidf values\n",
        "  #print out the top n tf words tfidf words from the sorted list in parallel for comparison\n",
        "\n",
        "def cleaning(corpus):\n",
        "  tokenized_docs=[]\n",
        "  for doc in corpus:\n",
        "    clean_doc = re.sub(r'[^\\w\\s]','', doc)\n",
        "    tokenized_sentence = sent_tokenize(clean_doc.lower())\n",
        "    lower_case = word_tokenize(clean_doc.lower())\n",
        "    stopword_removal = [w for w in lower_case if not w in sww]\n",
        "    tokenized_docs.append(stopword_removal)\n",
        "  \n",
        "  return tokenized_docs\n",
        "\n",
        "def get_df(tokenized_docs):\n",
        "  DF = {}\n",
        "  for tokenized_docs in tokenized_docs:\n",
        "    for term in np.unique(tokenized_docs):\n",
        "      try:\n",
        "        DF[term] +=1\n",
        "      except:\n",
        "        DF[term] =1  \n",
        "  return DF\n",
        "\n",
        "def get_tf_and_idf(corpus, top_n):\n",
        "  tokenized_docs = cleaning(corpus)\n",
        "  tf_idf = {}\n",
        "  tf_1 = {}\n",
        "  N = len(tokenized_docs)\n",
        "  doc_id = 0\n",
        "\n",
        "  DF = get_df(tokenized_docs)\n",
        "  for tokenized_docs in tokenized_docs:    \n",
        "    counter = Counter(tokenized_docs)\n",
        "    total_num_words = len(tokenized_docs)  \n",
        "    \n",
        "    for term in np.unique(tokenized_docs): \n",
        "      tf = counter[term]/total_num_words\n",
        "      df = DF[term]\n",
        "      idf = math.log(N/(df+1))+1\n",
        "      tf_1[doc_id, term] = tf\n",
        "      tf_idf[doc_id, term] = tf*idf\n",
        "        \n",
        "    doc_id += 1\n",
        "\n",
        "  print(\"Total docs in corpus: \" + str(N))\n",
        "  print(\"\")\n",
        "  print(\"Top 10 of tf values:\")\n",
        "  print(\"(doc id , word: tf)\")\n",
        "  sorted_dict = sorted(tf_1.items(), key=lambda x: x[1], reverse=True)\n",
        "  for i in range(top_n):\n",
        "    print(sorted_dict[i])\n",
        "  print(\"\")\n",
        "  print(\"Top 10 of tfidf values:\")\n",
        "  print(\"(doc id , word: tf*idf)\")\n",
        "  sorted_dict = sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)\n",
        "  for i in range(top_n):\n",
        "    print(sorted_dict[i])\n",
        "\n",
        "get_tf_and_idf(corpus, 10)\n",
        "\n",
        "#Expected Outcomes are as follows:"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total docs in corpus: 7\n",
            "\n",
            "Top 10 of tf values:\n",
            "(doc id , word: tf)\n",
            "((5, 'brisbane'), 0.04632814080517904)\n",
            "((4, 'darwin'), 0.04120879120879121)\n",
            "((3, 'melbourne'), 0.03969006957621758)\n",
            "((1, 'perth'), 0.03878231859883236)\n",
            "((2, 'sydney'), 0.03666121112929623)\n",
            "((6, 'hobart'), 0.03344575604272063)\n",
            "((0, 'canberra'), 0.02618181818181818)\n",
            "((3, 'city'), 0.014705882352941176)\n",
            "((5, 'city'), 0.01436374671252276)\n",
            "((6, 'city'), 0.013771781899943788)\n",
            "\n",
            "Top 10 of tfidf values:\n",
            "(doc id , word: tf*idf)\n",
            "((4, 'darwin'), 0.055074405355269765)\n",
            "((5, 'brisbane'), 0.05346965520543033)\n",
            "((1, 'perth'), 0.044760639376119696)\n",
            "((6, 'hobart'), 0.04469932438390229)\n",
            "((2, 'sydney'), 0.03666121112929623)\n",
            "((0, 'canberra'), 0.03499127310426448)\n",
            "((3, 'melbourne'), 0.03439019931234105)\n",
            "((5, 'queensland'), 0.015176976368353589)\n",
            "((4, 'darwins'), 0.013409303383901)\n",
            "((5, 'brisbanes'), 0.013216695546503272)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}