{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/Lab03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn_kEIjOfZ2I",
        "colab_type": "text"
      },
      "source": [
        "# Lab03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcAvq3CefbuL",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network Example\n",
        "We are going to implement a Neural Network for identifying animal species using features of animals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEoHF5OqElbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6KQ_iFvPHMe",
        "colab_type": "text"
      },
      "source": [
        "Let's say we have two features that represent each: \n",
        "1. Does animal has hair? \n",
        "2. Does animal has feather?\n",
        "\n",
        "![alt text](https://usydnlpgroup.files.wordpress.com/2020/03/lab3_1-e1584520889100.png)\n",
        "![alt text](https://usydnlpgroup.files.wordpress.com/2020/03/lab3_2-e1584520954981.png)\n",
        "![alt text](https://usydnlpgroup.files.wordpress.com/2020/03/lab3_3-e1584520932969.png)\n",
        "\n",
        "We would like to predict whether the animal is bird, mammal, or other. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA528d5gtbW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Hair, Feather]\n",
        "x_data = np.array(\n",
        "    [[0, 0], [1, 0], [1, 1], [0, 0], [0, 0], [0, 1]])\n",
        "x_data_torch = torch.from_numpy(x_data).float()  #You need to transform the data into Torch data type to be used in Pytorch\n",
        "#print(x_data_torch) #try print if you want to see the details\n",
        "\n",
        "y_data = np.array([0,1,2,0,0,2]) # 0-Other  1-Mammal  2-Bird\n",
        "y_data_torch = torch.from_numpy(y_data)\n",
        "#print(y_data_torch) #try print if you want to see the details\n",
        "\n",
        "# number of input (features), 2 - Hair, Feather\n",
        "num_input = 2\n",
        "# number of output (classes), 3 - Other, Mammal, Bird\n",
        "num_classes = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI4k8NogzksQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**But, why One-hot encoding?**\n",
        "There is no ordinal relationship exists!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIMNFiTB27Wv",
        "colab_type": "text"
      },
      "source": [
        "### No hidden layer with Manual Parameter Update\n",
        "\n",
        "Build a classification model without hidden layer.\n",
        "We manually update parameters (weight and bias) in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCUh760AMa37",
        "colab_type": "code",
        "outputId": "d5182db8-f55c-4b2f-f998-8ea60c9bbc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize Weight and Bias manually, and setup the gradient\n",
        "W1 = Variable(torch.randn(num_input, num_classes).float(), requires_grad=True)  \n",
        "B1 = Variable(torch.randn(num_classes).float(), requires_grad=True)\n",
        "\n",
        "#Learning Rate - determines the step size at each iteration while moving toward a minimum of a loss function\n",
        "learning_rate=0.01\n",
        "#Epoch - A measure of the number of times all of the training vectors are used once to update the weights.\n",
        "number_of_epochs = 1000 #try 2000 epochs and see how it goes\n",
        "\n",
        "for epoch in range(number_of_epochs): \n",
        "\n",
        "    # forward propagataion - the calculation and storage of intermediate variables (incl. outputs) from the input layer to the output layer\n",
        "    z = torch.add(torch.matmul(x_data_torch ,W1),B1)\n",
        "    #softmax changes each value to have between 0 and 1, and all values will add up to 1\n",
        "    #i.e.) [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
        "    #log_softmax applies logarithm after softmax.\n",
        "    #softmax: exp(x_i) / exp(x).sum() and log_softmax: log( exp(x_i) / exp(x).sum() )\n",
        "    #log_softmax essential does log(softmax(x)), but the practical implementation is different and more efficient\n",
        "    #https://pytorch.org/docs/master/nn.html?highlight=log_softmax#torch.nn.LogSoftmax\n",
        "    log_softmax = F.log_softmax(z,dim=1)\n",
        "\n",
        "    # calculate the loss\n",
        "    loss = F.nll_loss(log_softmax, y_data_torch)\n",
        "\n",
        "    # back propagation\n",
        "    loss.backward()  #calculate gradient\n",
        "    with torch.no_grad(): # When doing back propagation, do not accumalte gradient\n",
        "        W1.data -= learning_rate*W1.grad.data #Gradient descent\n",
        "        B1.data -= learning_rate*B1.grad.data\n",
        "    W1.grad.data.zero_() # reset the gradient\n",
        "    B1.grad.data.zero_()\n",
        "\n",
        "    if epoch % 200 == 199: \n",
        "        with torch.no_grad(): # prediction section does not require gradient \n",
        "            pred_outputs = torch.add(torch.matmul(x_data_torch ,W1),B1)\n",
        "            predicted = torch.argmax(pred_outputs, 1)\n",
        "            train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "            print('Epoch: %d, loss: %.4f, train_acc: %.3f' %(epoch + 1, loss , train_acc))\n",
        "\n",
        "### Result\n",
        "print('Predicted :', predicted.numpy())\n",
        "print('Answer :', y_data)\n",
        "print('Accuracy : %.2f' %train_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 200, loss: 1.3079, train_acc: 0.500\n",
            "Epoch: 400, loss: 0.9672, train_acc: 0.667\n",
            "Epoch: 600, loss: 0.7559, train_acc: 0.667\n",
            "Epoch: 800, loss: 0.6205, train_acc: 0.667\n",
            "Epoch: 1000, loss: 0.5262, train_acc: 0.833\n",
            "Predicted : [0 0 2 0 0 2]\n",
            "Answer : [0 1 2 0 0 2]\n",
            "Accuracy : 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88ZeQitaiBHN",
        "colab_type": "text"
      },
      "source": [
        "### No hidden layer with the Optimiser\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpnimRmV2Xlm",
        "colab_type": "code",
        "outputId": "5b90bd6c-0683-4870-fa6b-ffcaac04285d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Build a neural network model rather than initializing the parameters manually\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.linear = nn.Linear(num_input,num_classes) # This coresponds to W1,B1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "#get the model\n",
        "net = Net()\n",
        "\n",
        "#learning rate\n",
        "learning_rate=0.01\n",
        "\n",
        "#calculate loss by function. The negative log likelihood loss.\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Define the optimiser\n",
        "# Pass the model parameters to be updated, and setup the learning rate when calling optim.SGD\n",
        "# Please find the detailed information about the SGD optimiser in PyTorch (https://pytorch.org/docs/stable/optim.html).\n",
        "optimiser = optim.SGD(net.parameters(), lr=learning_rate) \n",
        "\n",
        "#epochs\n",
        "no_of_epochs = 1000\n",
        "\n",
        "# Every epoch, the model will be modified by the given learning rate\n",
        "for epoch in range(no_of_epochs):  \n",
        "\n",
        "    # get the input and label\n",
        "    inputs = x_data_torch\n",
        "    labels = y_data_torch\n",
        "\n",
        "    # training the model\n",
        "    net.train()\n",
        "\n",
        "    # set the gradients to zero\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(F.log_softmax(outputs,dim=1), labels)\n",
        "    loss.backward()\n",
        "    optimiser.step() # back propagation\n",
        "\n",
        "    if epoch % 200 == 199:    # print every 200 epochs\n",
        "        net.eval() # we are using the model to predict here\n",
        "        pred_outputs = net(inputs)\n",
        "        predicted = torch.argmax(outputs, 1)\n",
        "        train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "        print('%d, loss: %.3f, train_acc: %.3f' %(epoch + 1, loss.item(), train_acc))\n",
        "\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "### Result\n",
        "net.eval()\n",
        "pred_outputs = net(inputs)\n",
        "predicted = torch.argmax(outputs, 1)\n",
        "print('Predicted :', predicted.numpy())\n",
        "print('Answer :', y_data)\n",
        "\n",
        "train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "print('Accuracy : %.2f' %train_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200, loss: 0.968, train_acc: 0.333\n",
            "400, loss: 0.787, train_acc: 1.000\n",
            "600, loss: 0.657, train_acc: 1.000\n",
            "800, loss: 0.561, train_acc: 1.000\n",
            "1000, loss: 0.487, train_acc: 1.000\n",
            "Finished Training\n",
            "Predicted : [0 1 2 0 0 2]\n",
            "Answer : [0 1 2 0 0 2]\n",
            "Accuracy : 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RF_stSG5Nc6",
        "colab_type": "text"
      },
      "source": [
        "### Hidden Layer with Manual Parameter Update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFK1QHXQiFft",
        "colab_type": "code",
        "outputId": "99bb41d1-115c-4c10-b618-e9a580bb52c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#you can find the comment and explanation in the section - no hidden layer, manual parameter update section \n",
        "\n",
        "# number of neurons in hidden layer, \n",
        "n_hidden_1 = 5\n",
        "\n",
        "# [Input(Features), Output(number of neurons in hidden layer)]\n",
        "W1 = Variable(torch.randn(2, n_hidden_1).float(), requires_grad=True)\n",
        "# Let Hidden Layer Bias the number of output candidates, which is 5 (number of neurons in hidden layer).\n",
        "B1 = Variable(torch.randn(n_hidden_1).float(), requires_grad=True)\n",
        " # [Input(number of neurons in hidden layer), Output(Classes)]\n",
        "Wout = Variable(torch.randn(n_hidden_1, num_classes).float(), requires_grad=True)\n",
        "# Let Bias the number of output candidates, which is 3 (number of classes).\n",
        "Bout = Variable(torch.randn(num_classes).float(), requires_grad=True)\n",
        "\n",
        "learning_rate=0.01\n",
        "no_of_epochs = 1000\n",
        "\n",
        "for epoch in range(1000):    \n",
        "    z1 = torch.add(torch.matmul(x_data_torch ,W1),B1)\n",
        "    Zout = torch.add(torch.matmul(F.relu(z1) ,Wout),Bout)\n",
        "\n",
        "    log_softmax = F.log_softmax(Zout,dim=1)\n",
        "    loss = F.nll_loss(log_softmax, y_data_torch)\n",
        "\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        W1.data -= learning_rate*W1.grad.data\n",
        "        B1.data -= learning_rate*B1.grad.data\n",
        "        Wout.data -= learning_rate*Wout.grad.data\n",
        "        Bout.data -= learning_rate*Bout.grad.data\n",
        "\n",
        "    W1.grad.data.zero_()\n",
        "    B1.grad.data.zero_()\n",
        "    Wout.grad.data.zero_()\n",
        "    Bout.grad.data.zero_()\n",
        "\n",
        "    if epoch % 200 == 199: \n",
        "        with torch.no_grad():\n",
        "            z1 = torch.add(torch.matmul(x_data_torch ,W1),B1)\n",
        "            Zout = torch.add(torch.matmul(F.relu(z1) ,Wout),Bout)\n",
        "            predicted = torch.argmax(Zout, 1)\n",
        "            train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "            print('Epoch: %d, loss: %.4f, train_acc: %.3f' %(epoch + 1, loss.item() , train_acc))\n",
        "print(\"Finished\")\n",
        "### Result\n",
        "print('Predicted :', predicted.numpy())\n",
        "print('Answer :', y_data)\n",
        "print('Accuracy : %.2f' %train_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 200, loss: 0.3950, train_acc: 1.000\n",
            "Epoch: 400, loss: 0.2147, train_acc: 1.000\n",
            "Epoch: 600, loss: 0.1311, train_acc: 1.000\n",
            "Epoch: 800, loss: 0.0900, train_acc: 1.000\n",
            "Epoch: 1000, loss: 0.0663, train_acc: 1.000\n",
            "Finished\n",
            "Predicted : [0 1 2 0 0 2]\n",
            "Answer : [0 1 2 0 0 2]\n",
            "Accuracy : 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZr6f7sVlmcn",
        "colab_type": "text"
      },
      "source": [
        "### Hidden Layer with the Optimiser\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AZS2dx1_qpv",
        "colab_type": "code",
        "outputId": "24266586-874f-4679-80b6-02e5ec99bc1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#you can find the comment and explanation in the section - no hidden layer with the optimiser section \n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.linear1 = nn.Linear(num_input, n_hidden_1)\n",
        "        self.linear2 = nn.Linear(n_hidden_1, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z1 = self.linear1(x)\n",
        "        Zout = self.linear2(F.relu(z1))\n",
        "        return Zout\n",
        "\n",
        "net = Net()\n",
        "\n",
        "learning_rate=0.01\n",
        "# If you apply Pytorch’s CrossEntropyLoss to your output layer,\n",
        "# you get the same result as applying Pytorch’s NLLLoss to a LogSoftmax layer added after your original output layer.\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimiser = optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "for epoch in range(2000):  # loop over the dataset multiple times\n",
        "\n",
        "    # get the inputs\n",
        "    inputs = x_data_torch\n",
        "    labels = y_data_torch\n",
        "\n",
        "    net.train()\n",
        "    # zero the parameter gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels) # We don't need to calcualte logsoftmax here\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    # print statistics\n",
        "    if epoch % 200 == 199:    # print every 200 epochs\n",
        "        net.eval()\n",
        "        pred_outputs = net(inputs)\n",
        "        predicted = torch.argmax(outputs, 1)\n",
        "        train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "        print('%d, loss: %.4f, train_acc: %.4f' %(epoch + 1, loss.item(), train_acc))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "### Result\n",
        "pred_outputs = net(inputs)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "print('Predicted :', predicted.numpy())\n",
        "print('Answer :', y_data)\n",
        "\n",
        "train_acc = accuracy_score(predicted.numpy(),y_data)\n",
        "print('Accuracy : %.2f' %train_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200, loss: 0.7656, train_acc: 0.8333\n",
            "400, loss: 0.5847, train_acc: 0.8333\n",
            "600, loss: 0.4474, train_acc: 0.8333\n",
            "800, loss: 0.3444, train_acc: 0.8333\n",
            "1000, loss: 0.2635, train_acc: 1.0000\n",
            "1200, loss: 0.1996, train_acc: 1.0000\n",
            "1400, loss: 0.1517, train_acc: 1.0000\n",
            "1600, loss: 0.1171, train_acc: 1.0000\n",
            "1800, loss: 0.0924, train_acc: 1.0000\n",
            "2000, loss: 0.0747, train_acc: 1.0000\n",
            "Finished Training\n",
            "Predicted : [0 1 2 0 0 2]\n",
            "Answer : [0 1 2 0 0 2]\n",
            "Accuracy : 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLsGUsJQTDmP",
        "colab_type": "text"
      },
      "source": [
        "## Word2Vec on Pytorch\n",
        "Let's try to implement Word2Vec with Neural Network with Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h73M742bwUGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# raw data - setences \n",
        "sentences = [\"he likes cat\",\n",
        "             \"he likes dog\",\n",
        "             \"he likes animal\",\n",
        "             \"dog cat animal\",\n",
        "             \"she likes cat\",\n",
        "             \"she dislikes dog\",\n",
        "             \"cat likes fish\",\n",
        "             \"cat likes milk\",\n",
        "             \"dog likes bone\",\n",
        "             \"dog dislikes fish\",\n",
        "             \"dog likes milk\",\n",
        "             \"she likes movie\",\n",
        "             \"she likes music\",\n",
        "             \"he likes game\",\n",
        "             \"he likes movie\",\n",
        "             \"cat dislikes dog\"]\n",
        "\n",
        "# convert all sentences to unique word list\n",
        "word_sequence = \" \".join(sentences).split()\n",
        "word_list = \" \".join(sentences).split()\n",
        "word_list = list(set(word_list))\n",
        "\n",
        "# make dictionary so that we can be reference each index of unique word\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "\n",
        "# Making window size 1 skip-gram\n",
        "# i.e.) he likes cat\n",
        "#   -> (he, [likes]), (likes,[he, cat]), (cat,[likes])\n",
        "#   -> (he, likes), (likes, he), (likes, cat), (cat, likes)\n",
        "skip_grams = []\n",
        "\n",
        "for i in range(1, len(word_sequence) - 1):\n",
        "    # (context, target) : ([target index - 1, target index + 1], target)\n",
        "    target = word_dict[word_sequence[i]]\n",
        "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "\n",
        "    # skipgrams - (target, context[0]), (target, context[1])..\n",
        "    for w in context:\n",
        "        skip_grams.append([target, w])\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T-_SHxhHQeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc_size = len(word_list)\n",
        "\n",
        "# prepare random batch from skip-gram - we do not have enought data so we randomly select data\n",
        "def prepare_batch(data, size):\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "    for i in random_index:\n",
        "        input_temp = [0]*voc_size\n",
        "        input_temp[data[i][0]] = 1\n",
        "        random_inputs.append(input_temp)  # target\n",
        "        random_labels.append(data[i][1])  # context word\n",
        "\n",
        "    return np.array(random_inputs), np.array(random_labels)\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "batch_size = 10\n",
        "embedding_size = 2\n",
        "no_of_epochs = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp3ARUXGG0SD",
        "colab_type": "code",
        "outputId": "19581054-cec5-4eb2-bda3-b4f31d062fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "W1 = Variable(torch.randn(voc_size, embedding_size).float(), requires_grad=True)\n",
        "Wout = Variable(torch.randn(embedding_size, voc_size).float(), requires_grad=True)\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "\n",
        "    inputs,labels = prepare_batch(skip_grams, batch_size) # make batch every epoch\n",
        "    inputs_torch = torch.from_numpy(inputs).float()\n",
        "    labels_torch = torch.from_numpy(labels)\n",
        "\n",
        "    hidden = torch.matmul(inputs_torch ,W1)\n",
        "    out = torch.matmul(hidden,Wout)\n",
        "\n",
        "    log_softmax = F.log_softmax(out,dim=1)\n",
        "    loss = F.nll_loss(log_softmax, labels_torch)\n",
        "\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        W1.data -= learning_rate*W1.grad.data\n",
        "        Wout.data -= learning_rate*Wout.grad.data\n",
        "    W1.grad.data.zero_()\n",
        "    Wout.grad.data.zero_()\n",
        "\n",
        "    if epoch % 500 == 499: \n",
        "        print('Epoch: %d, loss: %.4f' %(epoch + 1, loss))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 500, loss: 2.5384\n",
            "Epoch: 1000, loss: 2.0181\n",
            "Epoch: 1500, loss: 2.2543\n",
            "Epoch: 2000, loss: 1.9188\n",
            "Epoch: 2500, loss: 2.0225\n",
            "Epoch: 3000, loss: 1.7935\n",
            "Epoch: 3500, loss: 1.9717\n",
            "Epoch: 4000, loss: 1.8768\n",
            "Epoch: 4500, loss: 2.0389\n",
            "Epoch: 5000, loss: 2.5527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z2uWigF0g0d",
        "colab_type": "code",
        "outputId": "50e4fa98-88ad-4fc4-c62e-48df63c514bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "trained_embeddings = W1.data.numpy()\n",
        "### Visualise result\n",
        "for i, label in enumerate(word_list):\n",
        "    x, y = trained_embeddings[i]\n",
        "    # print (label, \" : \", x, \" \" , y) #please see the detailed vector info\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
        "                 textcoords='offset points', ha='right', va='bottom')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJPx2i5C0Bfc",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "In the tutorial, we learned how to train a word2vec skip-gram model in pytorch, and we manually updates the parameters (weights) in it. \n",
        "\n",
        "In this Lab 03 exercise, please use the \"NN Model\" and the \"Optimiser\" (that we learned in the above sections) to train a word2vec skip-gram model.\n",
        "\n",
        "Note: The embedding size should 2. The code for the preprocessing and the hyperparameter setup are provided. Have fun!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExuvDSozHRb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess and hyperparameters\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# raw data - setences \n",
        "sentences = [\"he likes cat\",\n",
        "             \"he likes dog\",\n",
        "             \"he likes animal\",\n",
        "             \"dog cat animal\",\n",
        "             \"she likes cat\",\n",
        "             \"she dislikes dog\",\n",
        "             \"cat likes fish\",\n",
        "             \"cat likes milk\",\n",
        "             \"dog likes bone\",\n",
        "             \"dog dislikes fish\",\n",
        "             \"dog likes milk\",\n",
        "             \"she likes movie\",\n",
        "             \"she likes music\",\n",
        "             \"he likes game\",\n",
        "             \"he likes movie\",\n",
        "             \"cat dislikes dog\"]\n",
        "\n",
        "# convert all sentences to unique word list\n",
        "word_sequence = \" \".join(sentences).split()\n",
        "word_list = \" \".join(sentences).split()\n",
        "word_list = list(set(word_list))\n",
        "\n",
        "# make dictionary so that we can be reference each index of unique word\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "\n",
        "# Making window size 1 skip-gram\n",
        "# i.e.) he likes cat\n",
        "#   -> (he, [likes]), (likes,[he, cat]), (cat,[likes])\n",
        "#   -> (he, likes), (likes, he), (likes, cat), (cat, likes)\n",
        "skip_grams = []\n",
        "\n",
        "for i in range(1, len(word_sequence) - 1):\n",
        "    # (context, target) : ([target index - 1, target index + 1], target)\n",
        "    target = word_dict[word_sequence[i]]\n",
        "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "\n",
        "    # skipgrams - (target, context[0]), (target, context[1])..\n",
        "    for w in context:\n",
        "        skip_grams.append([target, w])\n",
        "\n",
        "\n",
        "# prepare random batch from skip-gram - we do not have enought data so we randomly select data\n",
        "def prepare_batch(data, size):\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "    for i in random_index:\n",
        "        input_temp = [0]*voc_size\n",
        "        input_temp[data[i][0]] = 1\n",
        "        random_inputs.append(input_temp)  # target\n",
        "        random_labels.append(data[i][1])  # context word\n",
        "\n",
        "    return np.array(random_inputs), np.array(random_labels)\n",
        "\n",
        "#hyperparameters\n",
        "voc_size = len(word_list)\n",
        "learning_rate = 0.1\n",
        "batch_size = 10\n",
        "embedding_size = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5-33jOlBf2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should submit \"ipynb\" file (You can download it from \"File\" > \"Download .ipynb\") to Canvas\n",
        "# Please include the required libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SkipGram, self).__init__()\n",
        "        #You need to use \"bias=False\" when you define Linear functions\n",
        "        #***************put your code here***************\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden = self.linear1(x)\n",
        "        out = self.linear2(hidden)\n",
        "        return out\n",
        "\n",
        "skip_gram_model = SkipGram()\n",
        "criterion = nn.CrossEntropyLoss() #please note we are using \"CrossEntropyLoss\" here\n",
        "optimiser = optim.SGD(skip_gram_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(5000):\n",
        "\n",
        "    inputs,labels = prepare_batch(skip_grams, batch_size)\n",
        "    inputs_torch = torch.from_numpy(inputs).float()\n",
        "    labels_torch = torch.from_numpy(labels)\n",
        "\n",
        "    #***************put your code here***************\n",
        "    # 1. zero grad\n",
        "    # 2. forword propagation\n",
        "    # 3. calculate loss\n",
        "    # 4. back propagation\n",
        "\n",
        "    if epoch % 500 == 499: \n",
        "        print('Epoch: %d, loss: %.4f' %(epoch + 1, loss))\n",
        "\n",
        "#Hint: you can refer lab1 to know how to get the weight from a Model Linear layer\n",
        "weight1 = #***************put your code here***************\n",
        "trained_embeddings = weight1.detach().T.numpy()\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZRyAdjGaufC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Visualise result\n",
        "for i, label in enumerate(word_list):\n",
        "    x, y = trained_embeddings[i]\n",
        "    # print (label, \" : \", x, \" \" , y)\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(label, xy=(x, y), xytext=(5, 2),\n",
        "                 textcoords='offset points', ha='right', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}