{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/Lab04%20-%20Johnny's%20Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgAANQLNyE3p",
        "colab_type": "text"
      },
      "source": [
        "# Lab04\n",
        "\n",
        "In this lab 4, we learn the Recurrent Neural Networks and Sequence Modelling\n",
        "\n",
        "\n",
        "*   Recurrent Neural Networks\n",
        "*   Sequence Modelling (Seq2Seq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-316qjIt-zS",
        "colab_type": "text"
      },
      "source": [
        "# Exercise (Text classification using LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwj7GtQPP6M7",
        "colab_type": "text"
      },
      "source": [
        "In this exercise, you are going to implement a LSTM model to do the text classification problem. Please notice that we have already done the preprocessing and embedding part of the dataset. You can only focus on the Model part.\n",
        "\n",
        "**Sequence Modelling**\n",
        "\n",
        "![alt text](https://usydnlpgroup.files.wordpress.com/2020/03/lstm_textclassification-e1584855309361.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6E9-9EqTNns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "#If you enable GPU here, device will be cuda, otherwise it will be cpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-hlDQBqsV1L",
        "colab_type": "text"
      },
      "source": [
        "### Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvwyvmfDegwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1ORrHW9moXLcWwg8WY9o-Ulq8X9BAiD1P'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('train.pkl')  \n",
        "\n",
        "id = '1eb4gtE8XlN3TcZqzwS18Ik-H7MFAeW4z'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('label.pkl')  \n",
        "\n",
        "import pickle\n",
        "input_embeddings = pickle.load(open(\"train.pkl\",\"rb\"))\n",
        "label = pickle.load(open(\"label.pkl\",\"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4iYaoduQvA9",
        "colab_type": "text"
      },
      "source": [
        "### Split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmkbXPApluQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into training and testing dataset using scikit-learn\n",
        "# For more details, you can refer to: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_embeddings, test_embeddings, train_label, test_label = train_test_split(input_embeddings,label,test_size = 0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrAL53HKufcG",
        "colab_type": "text"
      },
      "source": [
        "## Generate batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4vtcmaZufk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch(input_embeddings,label, batch_size):\n",
        "    idx = np.random.randint(input_embeddings.shape[0], size=batch_size)\n",
        "    return input_embeddings[idx,:,:],label[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_W08vIPv7L7",
        "colab_type": "text"
      },
      "source": [
        "## Model (please complete the following sections)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3rPdMUDQFOU",
        "colab_type": "text"
      },
      "source": [
        "**NOTE**: By updating hyperparameters, you should achieve **at least 0.4** for testset \"weighted avg\" f1. (There will be randomness in the training process, so tutors would run your code several times and there should be at least one of the output reaching 0.4)\n",
        "\n",
        "***What is F1?***\n",
        "\n",
        "![alt text](https://1.bp.blogspot.com/-nkFFqViboVM/XWwaQ5x1YpI/AAAAAAAAAP8/XzTH9hfJSfswcRjxSeQFEU6-yKQCwc0EQCLcBGAs/s640/main-qimg-447d6cdb02d2cc097ff1e6083a6bdc37.png)\n",
        "![alt text](https://i.stack.imgur.com/U0hjG.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4o1hzIzuDvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, batch_first =True)\n",
        "        self.linear = nn.Linear(n_hidden,n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x,_ = self.lstm(x)\n",
        "        x = self.linear(x[:,-1,:])\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mklFWbwhrsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "# Please assign values to these variables by using other variables (instead of hard code)\n",
        "n_input = train_embeddings.shape[2]\n",
        "n_class = len(set(train_label))\n",
        "\n",
        "#Please decide the hyperparameters here by yourself\n",
        "n_hidden = 128\n",
        "batch_size = 32\n",
        "total_epoch = 1200\n",
        "learning_rate = 0.01\n",
        "shown_interval = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNKfUFdRnQJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2da390ed-9e21-459d-d24c-0db61d0922a0"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "net = Net().to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Please find which optimizer provide higher f1\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate) \n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "\n",
        "    input_batch, target_batch = generate_batch(train_embeddings,train_label, batch_size)\n",
        "    input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "    target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "    net.train()\n",
        "    outputs = net(input_batch_torch) \n",
        "    loss = criterion(outputs, target_batch_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % shown_interval == shown_interval-1:\n",
        "        net.eval()\n",
        "        outputs = net(input_batch_torch) \n",
        "        train_loss = criterion(outputs, target_batch_torch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_acc= accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "\n",
        "        print('Epoch: %d, train loss: %.5f, train_acc: %.4f'%(epoch + 1, train_loss.item(), train_acc))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "net.eval()\n",
        "outputs = net(torch.from_numpy(test_embeddings).float().to(device)) \n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_label, predicted.cpu().numpy(),digits=4))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, train loss: 1.40432, train_acc: 0.2500\n",
            "Epoch: 20, train loss: 1.32862, train_acc: 0.4375\n",
            "Epoch: 30, train loss: 1.39763, train_acc: 0.3438\n",
            "Epoch: 40, train loss: 1.35672, train_acc: 0.3125\n",
            "Epoch: 50, train loss: 3.93274, train_acc: 0.1875\n",
            "Epoch: 60, train loss: 1.35486, train_acc: 0.1562\n",
            "Epoch: 70, train loss: 1.34577, train_acc: 0.3125\n",
            "Epoch: 80, train loss: 1.35674, train_acc: 0.1562\n",
            "Epoch: 90, train loss: 1.36166, train_acc: 0.2812\n",
            "Epoch: 100, train loss: 1.33405, train_acc: 0.3438\n",
            "Epoch: 110, train loss: 1.27161, train_acc: 0.3125\n",
            "Epoch: 120, train loss: 1.37252, train_acc: 0.3125\n",
            "Epoch: 130, train loss: 1.29655, train_acc: 0.4062\n",
            "Epoch: 140, train loss: 1.45031, train_acc: 0.1250\n",
            "Epoch: 150, train loss: 1.38354, train_acc: 0.2812\n",
            "Epoch: 160, train loss: 1.30397, train_acc: 0.2812\n",
            "Epoch: 170, train loss: 1.32033, train_acc: 0.2500\n",
            "Epoch: 180, train loss: 1.20329, train_acc: 0.4062\n",
            "Epoch: 190, train loss: 1.22654, train_acc: 0.4375\n",
            "Epoch: 200, train loss: 1.29728, train_acc: 0.3750\n",
            "Epoch: 210, train loss: 1.23308, train_acc: 0.3750\n",
            "Epoch: 220, train loss: 1.26894, train_acc: 0.3125\n",
            "Epoch: 230, train loss: 1.26356, train_acc: 0.4375\n",
            "Epoch: 240, train loss: 1.34528, train_acc: 0.5312\n",
            "Epoch: 250, train loss: 1.31442, train_acc: 0.2500\n",
            "Epoch: 260, train loss: 1.29399, train_acc: 0.4375\n",
            "Epoch: 270, train loss: 1.25085, train_acc: 0.3125\n",
            "Epoch: 280, train loss: 1.33722, train_acc: 0.2188\n",
            "Epoch: 290, train loss: 1.24957, train_acc: 0.4688\n",
            "Epoch: 300, train loss: 1.23561, train_acc: 0.5625\n",
            "Epoch: 310, train loss: 1.37904, train_acc: 0.3438\n",
            "Epoch: 320, train loss: 1.31339, train_acc: 0.3438\n",
            "Epoch: 330, train loss: 1.29256, train_acc: 0.3438\n",
            "Epoch: 340, train loss: 1.22561, train_acc: 0.3750\n",
            "Epoch: 350, train loss: 1.13545, train_acc: 0.4375\n",
            "Epoch: 360, train loss: 1.29421, train_acc: 0.3750\n",
            "Epoch: 370, train loss: 1.24793, train_acc: 0.4375\n",
            "Epoch: 380, train loss: 1.23919, train_acc: 0.3438\n",
            "Epoch: 390, train loss: 1.28625, train_acc: 0.2812\n",
            "Epoch: 400, train loss: 1.26831, train_acc: 0.2500\n",
            "Epoch: 410, train loss: 1.25431, train_acc: 0.3125\n",
            "Epoch: 420, train loss: 1.22000, train_acc: 0.2500\n",
            "Epoch: 430, train loss: 1.10666, train_acc: 0.4062\n",
            "Epoch: 440, train loss: 1.15506, train_acc: 0.3750\n",
            "Epoch: 450, train loss: 1.24559, train_acc: 0.4062\n",
            "Epoch: 460, train loss: 1.06556, train_acc: 0.4062\n",
            "Epoch: 470, train loss: 1.16540, train_acc: 0.4375\n",
            "Epoch: 480, train loss: 1.11511, train_acc: 0.3438\n",
            "Epoch: 490, train loss: 1.27944, train_acc: 0.2812\n",
            "Epoch: 500, train loss: 1.24092, train_acc: 0.2500\n",
            "Epoch: 510, train loss: 1.17137, train_acc: 0.4062\n",
            "Epoch: 520, train loss: 1.08095, train_acc: 0.4062\n",
            "Epoch: 530, train loss: 1.21630, train_acc: 0.3125\n",
            "Epoch: 540, train loss: 1.19566, train_acc: 0.3750\n",
            "Epoch: 550, train loss: 1.14567, train_acc: 0.3125\n",
            "Epoch: 560, train loss: 1.15834, train_acc: 0.3750\n",
            "Epoch: 570, train loss: 1.37541, train_acc: 0.2812\n",
            "Epoch: 580, train loss: 0.85645, train_acc: 0.6250\n",
            "Epoch: 590, train loss: 0.85799, train_acc: 0.6562\n",
            "Epoch: 600, train loss: 0.65192, train_acc: 0.7188\n",
            "Epoch: 610, train loss: 0.51495, train_acc: 0.8125\n",
            "Epoch: 620, train loss: 0.48529, train_acc: 0.8438\n",
            "Epoch: 630, train loss: 0.81432, train_acc: 0.6562\n",
            "Epoch: 640, train loss: 0.83827, train_acc: 0.6875\n",
            "Epoch: 650, train loss: 0.61960, train_acc: 0.8125\n",
            "Epoch: 660, train loss: 0.60394, train_acc: 0.7188\n",
            "Epoch: 670, train loss: 0.74360, train_acc: 0.6875\n",
            "Epoch: 680, train loss: 0.85174, train_acc: 0.6875\n",
            "Epoch: 690, train loss: 0.59672, train_acc: 0.8438\n",
            "Epoch: 700, train loss: 0.80952, train_acc: 0.6875\n",
            "Epoch: 710, train loss: 0.42425, train_acc: 0.9062\n",
            "Epoch: 720, train loss: 0.21407, train_acc: 0.9688\n",
            "Epoch: 730, train loss: 0.55289, train_acc: 0.8438\n",
            "Epoch: 740, train loss: 0.37021, train_acc: 0.9062\n",
            "Epoch: 750, train loss: 0.34797, train_acc: 0.8750\n",
            "Epoch: 760, train loss: 0.25272, train_acc: 0.9375\n",
            "Epoch: 770, train loss: 0.37013, train_acc: 0.8438\n",
            "Epoch: 780, train loss: 0.49732, train_acc: 0.7812\n",
            "Epoch: 790, train loss: 0.41172, train_acc: 0.8750\n",
            "Epoch: 800, train loss: 0.22902, train_acc: 0.9062\n",
            "Epoch: 810, train loss: 0.10214, train_acc: 1.0000\n",
            "Epoch: 820, train loss: 0.21036, train_acc: 0.9375\n",
            "Epoch: 830, train loss: 0.25174, train_acc: 0.9062\n",
            "Epoch: 840, train loss: 0.44525, train_acc: 0.7812\n",
            "Epoch: 850, train loss: 0.28945, train_acc: 0.9375\n",
            "Epoch: 860, train loss: 0.32923, train_acc: 0.8750\n",
            "Epoch: 870, train loss: 0.13919, train_acc: 0.9375\n",
            "Epoch: 880, train loss: 0.14367, train_acc: 0.9688\n",
            "Epoch: 890, train loss: 0.11245, train_acc: 0.9062\n",
            "Epoch: 900, train loss: 0.27500, train_acc: 0.9062\n",
            "Epoch: 910, train loss: 0.22910, train_acc: 0.9062\n",
            "Epoch: 920, train loss: 0.12545, train_acc: 0.9688\n",
            "Epoch: 930, train loss: 0.12048, train_acc: 0.9688\n",
            "Epoch: 940, train loss: 0.22678, train_acc: 0.9375\n",
            "Epoch: 950, train loss: 0.24551, train_acc: 0.9062\n",
            "Epoch: 960, train loss: 0.28112, train_acc: 0.9375\n",
            "Epoch: 970, train loss: 0.12345, train_acc: 0.9688\n",
            "Epoch: 980, train loss: 0.11710, train_acc: 0.9375\n",
            "Epoch: 990, train loss: 0.19162, train_acc: 0.8750\n",
            "Epoch: 1000, train loss: 0.12577, train_acc: 0.9375\n",
            "Epoch: 1010, train loss: 0.17080, train_acc: 0.9375\n",
            "Epoch: 1020, train loss: 0.19398, train_acc: 0.9375\n",
            "Epoch: 1030, train loss: 0.03462, train_acc: 1.0000\n",
            "Epoch: 1040, train loss: 0.12361, train_acc: 0.9688\n",
            "Epoch: 1050, train loss: 0.01710, train_acc: 1.0000\n",
            "Epoch: 1060, train loss: 0.18017, train_acc: 0.8750\n",
            "Epoch: 1070, train loss: 0.05783, train_acc: 1.0000\n",
            "Epoch: 1080, train loss: 0.08295, train_acc: 0.9688\n",
            "Epoch: 1090, train loss: 0.05846, train_acc: 1.0000\n",
            "Epoch: 1100, train loss: 0.26077, train_acc: 0.9062\n",
            "Epoch: 1110, train loss: 0.15186, train_acc: 0.9688\n",
            "Epoch: 1120, train loss: 0.04516, train_acc: 1.0000\n",
            "Epoch: 1130, train loss: 0.16632, train_acc: 0.9375\n",
            "Epoch: 1140, train loss: 0.05592, train_acc: 0.9688\n",
            "Epoch: 1150, train loss: 0.05994, train_acc: 1.0000\n",
            "Epoch: 1160, train loss: 0.18050, train_acc: 0.9688\n",
            "Epoch: 1170, train loss: 0.02956, train_acc: 1.0000\n",
            "Epoch: 1180, train loss: 0.02978, train_acc: 1.0000\n",
            "Epoch: 1190, train loss: 0.13732, train_acc: 0.9375\n",
            "Epoch: 1200, train loss: 0.02744, train_acc: 1.0000\n",
            "Finished Training\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7431    0.8710    0.8020        93\n",
            "           1     0.9286    0.9590    0.9435       122\n",
            "           2     0.9429    0.9083    0.9252       109\n",
            "           3     0.8750    0.7656    0.8167       128\n",
            "\n",
            "    accuracy                         0.8739       452\n",
            "   macro avg     0.8724    0.8760    0.8719       452\n",
            "weighted avg     0.8787    0.8739    0.8741       452\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5b2418d6-2a5f-40b3-e0ed-36da2493f8ae",
        "id": "LJJmS7udQc4q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#The following is the sample output \n",
        "#As mentioned in the previous labs, it is impossible to get the same result (randomness in the training process)."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100, train loss: 1.21694, train_acc: 0.4850\n",
            "Epoch: 200, train loss: 1.24536, train_acc: 0.3950\n",
            "Epoch: 300, train loss: 1.35449, train_acc: 0.3450\n",
            "Epoch: 400, train loss: 1.34606, train_acc: 0.3350\n",
            "Epoch: 500, train loss: 1.35706, train_acc: 0.2800\n",
            "Epoch: 600, train loss: 1.32822, train_acc: 0.3450\n",
            "Epoch: 700, train loss: 1.15759, train_acc: 0.5050\n",
            "Epoch: 800, train loss: 1.03310, train_acc: 0.5100\n",
            "Epoch: 900, train loss: 1.37713, train_acc: 0.2700\n",
            "Epoch: 1000, train loss: 1.36268, train_acc: 0.3200\n",
            "Epoch: 1100, train loss: 1.07713, train_acc: 0.4650\n",
            "Epoch: 1200, train loss: 0.81771, train_acc: 0.5200\n",
            "Epoch: 1300, train loss: 0.76015, train_acc: 0.5100\n",
            "Epoch: 1400, train loss: 0.76653, train_acc: 0.5650\n",
            "Epoch: 1500, train loss: 0.76491, train_acc: 0.6650\n",
            "Epoch: 1600, train loss: 0.59196, train_acc: 0.6950\n",
            "Epoch: 1700, train loss: 1.06773, train_acc: 0.6250\n",
            "Epoch: 1800, train loss: 0.54543, train_acc: 0.6950\n",
            "Epoch: 1900, train loss: 0.50840, train_acc: 0.7450\n",
            "Epoch: 2000, train loss: 0.43135, train_acc: 0.7650\n",
            "Epoch: 2100, train loss: 0.47718, train_acc: 0.7500\n",
            "Epoch: 2200, train loss: 0.45051, train_acc: 0.7350\n",
            "Epoch: 2300, train loss: 0.45664, train_acc: 0.7200\n",
            "Epoch: 2400, train loss: 0.40105, train_acc: 0.7650\n",
            "Epoch: 2500, train loss: 0.49771, train_acc: 0.7700\n",
            "Epoch: 2600, train loss: 0.43443, train_acc: 0.7550\n",
            "Epoch: 2700, train loss: 0.42603, train_acc: 0.7850\n",
            "Epoch: 2800, train loss: 0.43243, train_acc: 0.7650\n",
            "Epoch: 2900, train loss: 0.47906, train_acc: 0.7300\n",
            "Epoch: 3000, train loss: 0.40183, train_acc: 0.7700\n",
            "Epoch: 3100, train loss: 0.36424, train_acc: 0.7950\n",
            "Epoch: 3200, train loss: 0.43411, train_acc: 0.7750\n",
            "Epoch: 3300, train loss: 0.31626, train_acc: 0.7650\n",
            "Epoch: 3400, train loss: 0.58134, train_acc: 0.7300\n",
            "Epoch: 3500, train loss: 0.38333, train_acc: 0.7750\n",
            "Epoch: 3600, train loss: 0.30802, train_acc: 0.8150\n",
            "Epoch: 3700, train loss: 0.28024, train_acc: 0.8450\n",
            "Epoch: 3800, train loss: 0.40205, train_acc: 0.8050\n",
            "Epoch: 3900, train loss: 0.35988, train_acc: 0.8050\n",
            "Epoch: 4000, train loss: 0.26996, train_acc: 0.7900\n",
            "Epoch: 4100, train loss: 0.31308, train_acc: 0.8500\n",
            "Epoch: 4200, train loss: 0.32267, train_acc: 0.8000\n",
            "Epoch: 4300, train loss: 0.31892, train_acc: 0.7850\n",
            "Epoch: 4400, train loss: 0.31536, train_acc: 0.8450\n",
            "Epoch: 4500, train loss: 0.32055, train_acc: 0.8200\n",
            "Epoch: 4600, train loss: 0.27143, train_acc: 0.7850\n",
            "Epoch: 4700, train loss: 0.70663, train_acc: 0.7400\n",
            "Epoch: 4800, train loss: 0.32027, train_acc: 0.8100\n",
            "Epoch: 4900, train loss: 0.31337, train_acc: 0.7850\n",
            "Epoch: 5000, train loss: 0.23999, train_acc: 0.8250\n",
            "Finished Training\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4536    0.8925    0.6014        93\n",
            "           1     0.9120    0.9344    0.9231       122\n",
            "           2     0.9709    0.9174    0.9434       109\n",
            "           3     0.6341    0.2031    0.3077       128\n",
            "\n",
            "    accuracy                         0.7146       452\n",
            "   macro avg     0.7426    0.7369    0.6939       452\n",
            "weighted avg     0.7532    0.7146    0.6875       452\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}