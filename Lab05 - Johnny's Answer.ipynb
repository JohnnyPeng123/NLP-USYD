{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/Lab05%20-%20Johnny's%20Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JX8CSelGdJf",
        "colab_type": "text"
      },
      "source": [
        "# Lab 05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9dPCF1BMNpv",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "In this exercise, you are to preprocess the train and test data, and apply different pre-trained embeddings.\n",
        "\n",
        "**Note**: We won't mark your exercise based on the test set performance, we will only check whether the preprocessing part and embedding part are correct.\n",
        "\n",
        "**Important**: This exercise is very important to your assignment1 since you can use most of the codes here in your assignment1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKxN3z5FPRcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DC69XgVlLuPi"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zl7OaslvLuPo",
        "colab": {}
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1gNfBqguzBu8cHKMPc8C44GbvD443dNC5'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('twitter.csv')  \n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"twitter.csv\")\n",
        "df_pick = df.sample(40,random_state=24)\n",
        "\n",
        "raw_text = df_pick[\"Text\"].tolist()\n",
        "raw_label = df_pick[\"Label\"].tolist()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "text_train,text_test,label_train,label_test = train_test_split(raw_text,raw_label,test_size=0.25,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i92SvLGOLuPv"
      },
      "source": [
        "## Preprocessing [Complete this section]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xfp_5N2fLuPw"
      },
      "source": [
        "**Case Folding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9LggrTtLuPx",
        "colab": {}
      },
      "source": [
        "text_train = [s.lower() for s in text_train]\n",
        "text_test = [s.lower() for s in text_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hb4GVZYCLuP1"
      },
      "source": [
        "**Remove punctuations [Please complete this section]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vl4MgL1XLuP2",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def remove_punctuation_re(x):\n",
        "    x = re.sub(r'[^\\w\\s]','',x)\n",
        "    return x\n",
        "    \n",
        "text_train = [remove_punctuation_re(s) for s in text_train]\n",
        "text_test = [remove_punctuation_re(s) for s in text_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YvIc4FCRLuQC"
      },
      "source": [
        "**Tokenization [Please complete this section]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V3I1iHIWLuQD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "01bb8327-4698-4c82-bf1d-2d7e7c9a926a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#Please complete this\n",
        "text_train = [word_tokenize(s) for s in text_train]\n",
        "text_test = [word_tokenize(s) for s in text_test]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ucmu2pWTLuQG"
      },
      "source": [
        "**Remove stopwords [Please complete this section]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4blOZaW7LuQH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2db4f0bd-7cf2-4831-f7d9-de1d51d3feb3"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "\n",
        "text_train_ns=[]\n",
        "for tokens in text_train:\n",
        "    filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "    text_train_ns.append(filtered_sentence)\n",
        "\n",
        "text_test_ns=[]\n",
        "for tokens in text_test:\n",
        "    filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "    text_test_ns.append(filtered_sentence)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QqFS0duZLuQM"
      },
      "source": [
        "**Lemmatisation [Please complete this section]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6BTXGVwjLuQN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "80f507a0-9f1a-4241-d202-a191ee2feffe"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text_train_le = []\n",
        "for tokens in text_train_ns:\n",
        "    lemma_sentence = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "    text_train_le.append(lemma_sentence)\n",
        "\n",
        "text_test_le = []\n",
        "for tokens in text_test_ns:\n",
        "    lemma_sentence = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "    text_test_le.append(lemma_sentence)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MpQvfCJHLuQQ"
      },
      "source": [
        "**Label Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FkcRYxaZLuQQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "61bab79e-4037-4581-928b-2fc0bf1c62e8"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labels = np.unique(label_train)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(labels)\n",
        "label_train_n = lEnc.transform(label_train)\n",
        "label_test_n = lEnc.transform(label_test)\n",
        "numClass = len(labels)\n",
        "\n",
        "print(labels)\n",
        "print(lEnc.transform(labels))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['none' 'racism' 'sexism']\n",
            "[0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "brgyXIhALuQT"
      },
      "source": [
        "## Embeddings [Complete this section]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ltZvof0NLuQX"
      },
      "source": [
        "**Padding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O1V_W5PvLuQa",
        "colab": {}
      },
      "source": [
        "len_list = [len(s) for s in text_train_ns]\n",
        "seq_length = max(len_list)\n",
        "\n",
        "def add_padding(corpus, seq_length):\n",
        "    output = []\n",
        "    for sentence in corpus:\n",
        "        if len(sentence)>seq_length:\n",
        "            output.append(sentence[:seq_length])\n",
        "        else:\n",
        "            for j in range(seq_length-len(sentence)):\n",
        "                sentence.append(\"<PAD>\")\n",
        "            output.append(sentence)\n",
        "    return output\n",
        "\n",
        "text_train_pad = add_padding(text_train_le,seq_length )\n",
        "text_test_pad = add_padding(text_test_le,seq_length )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rUA2H0-HLuQe"
      },
      "source": [
        "**Download Embeddings [Please try other embeddings]**\n",
        "\n",
        "You can find the details from https://github.com/RaRe-Technologies/gensim-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "856alejrLuQe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8ee48551-8b40-458d-875d-f4da9b2ac00e"
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-25\") #this is only example"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCZMaK0zLuQj"
      },
      "source": [
        "**Get embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bdSLmjkSLuQk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d1064913-8abb-44d2-d805-8c644966e1cd"
      },
      "source": [
        "def get_embeddings(corpus,word_emb_model):\n",
        "    emb_dim = word_emb_model.vector_size\n",
        "    out = []\n",
        "    for sentence in corpus:\n",
        "        out_temp = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                out_temp.append(word_emb_model.wv[word])\n",
        "            except:\n",
        "                out_temp.append([0]*emb_dim)\n",
        "    \n",
        "        out.append(out_temp)\n",
        "    return np.array(out)\n",
        "\n",
        "train_emb = get_embeddings(text_train_pad,word_emb_model)\n",
        "test_emb = get_embeddings(text_test_pad,word_emb_model)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yJ96KxTcLuQo"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jCZG8wAXLuQp",
        "colab": {}
      },
      "source": [
        "n_input = train_emb.shape[2]\n",
        "n_hidden = 50\n",
        "n_class = len(labels)\n",
        "total_epoch = 100\n",
        "learning_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NRrSAS-DLuQs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6cc19388-3612-475a-8d2e-5aa6d9942012"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, num_layers=2, batch_first =True, dropout=0.2)\n",
        "        self.linear = nn.Linear(n_hidden,n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x,_ = self.lstm(x)\n",
        "        x = self.linear(x[:,-1,:])\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net().to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "input_batch_torch = torch.from_numpy(np.array(train_emb)).float().to(device)\n",
        "target_batch_torch = torch.from_numpy(np.array(label_train_n)).view(-1).to(device)\n",
        "\n",
        "\n",
        "for epoch in range(total_epoch):  \n",
        "    \n",
        "    net.train()\n",
        "    outputs = net(input_batch_torch) \n",
        "    loss = criterion(outputs, target_batch_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    net.eval()\n",
        "    outputs = net(input_batch_torch) \n",
        "    \n",
        "    if epoch%10 == 9:\n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        acc= accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "\n",
        "        print('Epoch: %d, loss: %.5f, train_acc: %.2f' %(epoch + 1, loss.item(), acc))\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n",
            "Epoch: 10, loss: 0.53801, train_acc: 0.73\n",
            "Epoch: 20, loss: 0.25101, train_acc: 0.87\n",
            "Epoch: 30, loss: 0.00583, train_acc: 1.00\n",
            "Epoch: 40, loss: 0.00143, train_acc: 1.00\n",
            "Epoch: 50, loss: 0.39852, train_acc: 0.93\n",
            "Epoch: 60, loss: 0.42728, train_acc: 0.87\n",
            "Epoch: 70, loss: 0.19730, train_acc: 0.93\n",
            "Epoch: 80, loss: 0.12919, train_acc: 0.97\n",
            "Epoch: 90, loss: 0.10252, train_acc: 0.97\n",
            "Epoch: 100, loss: 0.05393, train_acc: 0.97\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWTw2QWfN0Xc",
        "colab_type": "text"
      },
      "source": [
        "## Save and Load the model [Complete this section]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abct4WjhO4Bq",
        "colab_type": "text"
      },
      "source": [
        "**Save the model [Complete this part]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPRrmNS5ONQB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dff96851-a184-42fd-fe02-7cbd9a533402"
      },
      "source": [
        "torch.save(net, 'lab5.pt')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75fQ4jx5ON-t",
        "colab_type": "text"
      },
      "source": [
        "**Load the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1AUVT1uOQm6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b351ccb2-5155-4cc9-f96d-c12c27f3c458"
      },
      "source": [
        "model2 = torch.load('lab5.pt')\n",
        "model2.eval()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (lstm): LSTM(25, 50, num_layers=2, batch_first=True, dropout=0.2)\n",
              "  (linear): Linear(in_features=50, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnsgFMGYKrPA",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yntT6vpAKXCt",
        "colab_type": "code",
        "outputId": "f45da036-7a72-4c07-d2e2-51205732fcd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "input_batch_torch = torch.from_numpy(np.array(test_emb)).float().to(device)\n",
        "\n",
        "outputs = model2(input_batch_torch) \n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(label_test_n,predicted.cpu().numpy()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86         7\n",
            "           1       1.00      1.00      1.00         2\n",
            "           2       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.80        10\n",
            "   macro avg       0.62      0.62      0.62        10\n",
            "weighted avg       0.80      0.80      0.80        10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVytyTdS0VQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}