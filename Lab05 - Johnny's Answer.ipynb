{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab05.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/Lab05%20-%20Johnny's%20Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JX8CSelGdJf",
        "colab_type": "text"
      },
      "source": [
        "# Lab 05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9dPCF1BMNpv",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "In this exercise, you are to preprocess the train and test data, and apply different pre-trained embeddings.\n",
        "\n",
        "**Note**: We won't mark your exercise based on the test set performance, we will only check whether the preprocessing part and embedding part are correct.\n",
        "\n",
        "**Important**: This exercise is very important to your assignment1 since you can use most of the codes here in your assignment1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKxN3z5FPRcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "#You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DC69XgVlLuPi"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zl7OaslvLuPo",
        "colab": {}
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '1gNfBqguzBu8cHKMPc8C44GbvD443dNC5'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('twitter.csv')  \n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"twitter.csv\")\n",
        "df_pick = df.sample(40,random_state=24)\n",
        "\n",
        "raw_text = df_pick[\"Text\"].tolist()\n",
        "raw_label = df_pick[\"Label\"].tolist()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "text_train,text_test,label_train,label_test = train_test_split(raw_text,raw_label,test_size=0.25,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i92SvLGOLuPv"
      },
      "source": [
        "## Preprocessing [Complete this section]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xfp_5N2fLuPw"
      },
      "source": [
        "**Case Folding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9LggrTtLuPx",
        "colab": {}
      },
      "source": [
        "text_train = [s.lower() for s in text_train]\n",
        "text_test = [s.lower() for s in text_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hb4GVZYCLuP1"
      },
      "source": [
        "**Remove punctuations [Please complete this section]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vl4MgL1XLuP2",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def remove_punctuation_re(x):\n",
        "    x = re.sub(r'[^\\w\\s]','',x)\n",
        "    return x\n",
        "    \n",
        "text_train = [remove_punctuation_re(s) for s in text_train]\n",
        "text_test = [remove_punctuation_re(s) for s in text_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YvIc4FCRLuQC"
      },
      "source": [
        "**Tokenization [Please complete this section]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V3I1iHIWLuQD",
        "outputId": "4ce43b23-cb7a-4f6c-a041-f457c56f3193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#Please complete this\n",
        "text_train = [word_tokenize(s) for s in text_train]\n",
        "text_test = [word_tokenize(s) for s in text_test]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ucmu2pWTLuQG"
      },
      "source": [
        "**Remove stopwords [Please complete this section]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4blOZaW7LuQH",
        "outputId": "1e9c697c-7372-4bd3-9ca2-fd7f4986a86b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "stop_words = sw.words()\n",
        "\n",
        "text_train_ns=[]\n",
        "for tokens in text_train:\n",
        "    filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "    text_train_ns.append(filtered_sentence)\n",
        "\n",
        "text_test_ns=[]\n",
        "for tokens in text_test:\n",
        "    filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "    text_test_ns.append(filtered_sentence)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QqFS0duZLuQM"
      },
      "source": [
        "**Lemmatisation [Please complete this section]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6BTXGVwjLuQN",
        "outputId": "c826a93d-7bbf-4c29-c980-a75a0bd1d0f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "text_train_le = []\n",
        "for tokens in text_train_ns:\n",
        "    lemma_sentence = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "    text_train_le.append(lemma_sentence)\n",
        "\n",
        "text_test_le = []\n",
        "for tokens in text_test_ns:\n",
        "    lemma_sentence = [lemmatizer.lemmatize(w) for w in tokens ]\n",
        "    text_test_le.append(lemma_sentence)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MpQvfCJHLuQQ"
      },
      "source": [
        "**Label Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FkcRYxaZLuQQ",
        "outputId": "131436f0-b78b-49a5-cc74-da0f53d941a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np \n",
        "\n",
        "labels = np.unique(label_train)\n",
        "\n",
        "lEnc = LabelEncoder()\n",
        "lEnc.fit(labels)\n",
        "label_train_n = lEnc.transform(label_train)\n",
        "label_test_n = lEnc.transform(label_test)\n",
        "numClass = len(labels)\n",
        "\n",
        "print(labels)\n",
        "print(lEnc.transform(labels))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['none' 'racism' 'sexism']\n",
            "[0 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "brgyXIhALuQT"
      },
      "source": [
        "## Embeddings [Complete this section]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ltZvof0NLuQX"
      },
      "source": [
        "**Padding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O1V_W5PvLuQa",
        "colab": {}
      },
      "source": [
        "len_list = [len(s) for s in text_train_ns]\n",
        "seq_length = max(len_list)\n",
        "\n",
        "def add_padding(corpus, seq_length):\n",
        "    output = []\n",
        "    for sentence in corpus:\n",
        "        if len(sentence)>seq_length:\n",
        "            output.append(sentence[:seq_length])\n",
        "        else:\n",
        "            for j in range(seq_length-len(sentence)):\n",
        "                sentence.append(\"<PAD>\")\n",
        "            output.append(sentence)\n",
        "    return output\n",
        "\n",
        "text_train_pad = add_padding(text_train_le,seq_length )\n",
        "text_test_pad = add_padding(text_test_le,seq_length )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rUA2H0-HLuQe"
      },
      "source": [
        "**Download Embeddings [Please try other embeddings]**\n",
        "\n",
        "You can find the details from https://github.com/RaRe-Technologies/gensim-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "856alejrLuQe",
        "outputId": "1e07a7b6-43f2-4a73-f3f6-8edc6e829b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_emb_model = api.load(\"glove-twitter-25\") #this is only example"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sCZMaK0zLuQj"
      },
      "source": [
        "**Get embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bdSLmjkSLuQk",
        "outputId": "b337962f-4c2d-4568-e9e5-0cbf58bec52d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def get_embeddings(corpus,word_emb_model):\n",
        "    emb_dim = word_emb_model.vector_size\n",
        "    out = []\n",
        "    for sentence in corpus:\n",
        "        out_temp = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                out_temp.append(word_emb_model.wv[word])\n",
        "            except:\n",
        "                out_temp.append([0]*emb_dim)\n",
        "    \n",
        "        out.append(out_temp)\n",
        "    return np.array(out)\n",
        "\n",
        "train_emb = get_embeddings(text_train_pad,word_emb_model)\n",
        "test_emb = get_embeddings(text_test_pad,word_emb_model)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yJ96KxTcLuQo"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jCZG8wAXLuQp",
        "colab": {}
      },
      "source": [
        "n_input = train_emb.shape[2]\n",
        "n_hidden = 50\n",
        "n_class = len(labels)\n",
        "total_epoch = 100\n",
        "learning_rate = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NRrSAS-DLuQs",
        "outputId": "82fc3985-7391-416f-f3ac-5180ccf33cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.lstm = nn.LSTM(n_input, n_hidden, num_layers=2, batch_first =True, dropout=0.2)\n",
        "        self.linear = nn.Linear(n_hidden,n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x,_ = self.lstm(x)\n",
        "        x = self.linear(x[:,-1,:])\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net().to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "input_batch_torch = torch.from_numpy(np.array(train_emb)).float().to(device)\n",
        "target_batch_torch = torch.from_numpy(np.array(label_train_n)).view(-1).to(device)\n",
        "\n",
        "\n",
        "for epoch in range(total_epoch):  \n",
        "    \n",
        "    net.train()\n",
        "    outputs = net(input_batch_torch) \n",
        "    loss = criterion(outputs, target_batch_torch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    net.eval()\n",
        "    outputs = net(input_batch_torch) \n",
        "    \n",
        "    if epoch%10 == 9:\n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        acc= accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
        "\n",
        "        print('Epoch: %d, loss: %.5f, train_acc: %.2f' %(epoch + 1, loss.item(), acc))\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, loss: 0.43264, train_acc: 0.80\n",
            "Epoch: 20, loss: 0.08970, train_acc: 0.97\n",
            "Epoch: 30, loss: 0.20755, train_acc: 0.93\n",
            "Epoch: 40, loss: 0.00410, train_acc: 1.00\n",
            "Epoch: 50, loss: 0.00202, train_acc: 1.00\n",
            "Epoch: 60, loss: 0.00103, train_acc: 1.00\n",
            "Epoch: 70, loss: 0.00062, train_acc: 1.00\n",
            "Epoch: 80, loss: 0.00047, train_acc: 1.00\n",
            "Epoch: 90, loss: 0.00039, train_acc: 1.00\n",
            "Epoch: 100, loss: 0.00034, train_acc: 1.00\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWTw2QWfN0Xc",
        "colab_type": "text"
      },
      "source": [
        "## Save and Load the model [Complete this section]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abct4WjhO4Bq",
        "colab_type": "text"
      },
      "source": [
        "**Save the model [Complete this part]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPRrmNS5ONQB",
        "colab_type": "code",
        "outputId": "9a6fac95-a654-47dd-9043-8a1155257236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "torch.save(net, 'lab5.pt')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75fQ4jx5ON-t",
        "colab_type": "text"
      },
      "source": [
        "**Load the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1AUVT1uOQm6",
        "colab_type": "code",
        "outputId": "19c0d13d-be9e-4b1b-e797-0af9b52d791e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model2 = torch.load('lab5.pt')\n",
        "model2.eval()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (lstm): LSTM(25, 50, num_layers=2, batch_first=True, dropout=0.2)\n",
              "  (linear): Linear(in_features=50, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnsgFMGYKrPA",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yntT6vpAKXCt",
        "colab_type": "code",
        "outputId": "67292f65-9ef4-4512-f728-13837652e3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "input_batch_torch = torch.from_numpy(np.array(test_emb)).float().to(device)\n",
        "\n",
        "outputs = model2(input_batch_torch) \n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(label_test_n,predicted.cpu().numpy()))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.86      0.80         7\n",
            "           1       1.00      0.50      0.67         2\n",
            "           2       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.70        10\n",
            "   macro avg       0.58      0.45      0.49        10\n",
            "weighted avg       0.72      0.70      0.69        10\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVytyTdS0VQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}