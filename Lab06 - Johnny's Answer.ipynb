{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“Lab06.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/Lab06%20-%20Johnny's%20Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siAIwRRGUtWW",
        "colab_type": "text"
      },
      "source": [
        "# Lab06"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3fc37f1c-65f7-47db-b49e-af9153a105ab",
        "id": "Dy7Mer9rbNe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        " "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1a2fe95a-515e-483f-e0a5-fa5d0e344fd1",
        "id": "v8UAJz4qbMQz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Retrieve tagged sentences from treebank corpus\n",
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
        " \n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
        "#tagged_words(): list of (str,str) tuple"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n",
            "Tagged sentences:  3914\n",
            "Tagged words: 100676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8da1760f-6195-4743-abfd-210822ddde9b",
        "id": "T0gJFq0SbLkZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "sentences, sentence_tags =[], [] \n",
        "for tagged_sentence in tagged_sentences:\n",
        "    #The zip() function returns a zip object, which is an iterator of tuples where the first item in each passed iterator is paired together, \n",
        "    #and then the second item in each passed iterator are paired together etc.\n",
        "    sentence, tags = zip(*tagged_sentence)\n",
        "    sentences.append(np.array(sentence))\n",
        "    sentence_tags.append(np.array(tags))\n",
        " \n",
        "print(sentences[5])\n",
        "print(sentence_tags[5])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Lorillard' 'Inc.' ',' 'the' 'unit' 'of' 'New' 'York-based' 'Loews'\n",
            " 'Corp.' 'that' '*T*-2' 'makes' 'Kent' 'cigarettes' ',' 'stopped' 'using'\n",
            " 'crocidolite' 'in' 'its' 'Micronite' 'cigarette' 'filters' 'in' '1956'\n",
            " '.']\n",
            "['NNP' 'NNP' ',' 'DT' 'NN' 'IN' 'JJ' 'JJ' 'NNP' 'NNP' 'WDT' '-NONE-' 'VBZ'\n",
            " 'NNP' 'NNS' ',' 'VBD' 'VBG' 'NN' 'IN' 'PRP$' 'NN' 'NN' 'NNS' 'IN' 'CD'\n",
            " '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J4FLJnsWbKCL",
        "colab": {}
      },
      "source": [
        "(train_sentences, \n",
        " test_sentences, \n",
        " train_tags, \n",
        " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SEE2BKqGbDOc"
      },
      "source": [
        "### Making vocabs with special tokens\n",
        "\n",
        "*PAD: Padding*\n",
        "*OOV: Out Of Vocabulary*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NdMbH_QbbC_V",
        "colab": {}
      },
      "source": [
        "words, tags = set([]), set([])\n",
        " \n",
        "for s in train_sentences:\n",
        "    for w in s:\n",
        "        words.add(w.lower())\n",
        "\n",
        "for ts in train_tags:\n",
        "    for t in ts:\n",
        "        tags.add(t)\n",
        "\n",
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0  # The special value used for padding\n",
        "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
        " \n",
        "tag2index = {t: i + 2 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0  # The special value used to padding\n",
        "tag2index['-OOV-'] = 1  # The special value used for OOVs\n",
        "\n",
        "def tag_to_index(tag):\n",
        "    if tag in tag2index:\n",
        "        return tag2index[tag]\n",
        "    else:\n",
        "        return tag2index['-OOV-']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a692fa36-ba19-4a7f-b510-8d6f1c27e276",
        "id": "8T80cH9fbCbj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
        "\n",
        "for s in train_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "\n",
        "    train_sentences_X.append(s_int)\n",
        "\n",
        "for s in test_sentences:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "\n",
        "    test_sentences_X.append(s_int)\n",
        "\n",
        "for s in train_tags:\n",
        "    train_tags_y.append([tag_to_index(t) for t in s])\n",
        "\n",
        "for s in test_tags:\n",
        "    test_tags_y.append([tag_to_index(t) for t in s])\n",
        "\n",
        "print(train_sentences_X[0])\n",
        "print(test_sentences_X[0])\n",
        "print(train_tags_y[0])\n",
        "print(test_tags_y[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9897, 7075, 1464, 4207, 8312, 2043, 6184, 6852, 6118, 9823, 8929, 6469, 10075, 7075, 4306, 9839, 8222, 507, 6133, 7357, 6680, 2583, 1464, 4119, 5978, 196, 7018, 4628, 9762, 1667]\n",
            "[1511, 2760, 5580, 2415, 8425, 3567, 1511, 5615, 3171, 1856, 719, 8961, 9625, 6800, 250, 1, 4050, 1667]\n",
            "[42, 18, 29, 36, 44, 44, 44, 44, 12, 44, 44, 31, 44, 19, 45, 43, 20, 6, 7, 6, 20, 18, 29, 36, 2, 42, 6, 6, 20, 4]\n",
            "[42, 20, 10, 30, 34, 17, 42, 5, 6, 20, 20, 11, 7, 35, 6, 6, 20, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w4V4WQ1fbCO4"
      },
      "source": [
        "### Getting max length of sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f0c4e7db-cf3d-41ad-a784-9aee69357085",
        "id": "WGo8Z857bBkb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
        "print(MAX_LENGTH) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8jsEKMyw1aV",
        "colab_type": "text"
      },
      "source": [
        "### Add PAD by using torch pad_sequence\n",
        "Due to the limitation of Pytorch pad_sequence, we can't assign the max_length, the max_length is calculated by pad_sequence itself. You can try to add padding manually by using for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d5WoxwabuYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "new_train_sentences_X = [torch.from_numpy(np.array(l)) for l in train_sentences_X]\n",
        "new_test_sentences_X = [torch.from_numpy(np.array(l)) for l in test_sentences_X]\n",
        "new_train_tags_y = [torch.from_numpy(np.array(l)) for l in train_tags_y]\n",
        "new_test_tags_y = [torch.from_numpy(np.array(l)) for l in test_tags_y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7Nit7arXRYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "after_pad = pad_sequence(new_train_sentences_X+new_test_sentences_X+new_train_tags_y+new_test_tags_y,batch_first=True)\n",
        "train_sentences_X_pad = after_pad[:len(new_train_sentences_X)]\n",
        "test_sentences_X_pad = after_pad[len(new_train_sentences_X):len(new_train_sentences_X)+len(new_test_sentences_X)]\n",
        "train_tags_y_pad = after_pad[len(new_train_sentences_X)+len(new_test_sentences_X):-len(new_test_tags_y)]\n",
        "test_tags_y_pad = after_pad[-len(new_test_tags_y):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJE3qT2Vi0hL",
        "colab_type": "text"
      },
      "source": [
        "### Build Dataset and Dataloader for training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLmjfFnVi3vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#More detailed info about the TensorDataset, https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataset.html#TensorDataset\n",
        "from torch.utils.data import TensorDataset\n",
        "train_data = TensorDataset(train_sentences_X_pad, train_tags_y_pad)\n",
        "\n",
        "batch_size = 128\n",
        "#More detailed info about the dataLoader, https://pytorch.org/docs/1.1.0/_modules/torch/utils/data/dataloader.html\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjTDYN3sFtet",
        "colab_type": "text"
      },
      "source": [
        "## Pytorch Model (Bidirectional LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U7kYumQjagT",
        "colab_type": "code",
        "outputId": "4be9d632-6c11-480c-bda3-55af02e3ff0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,batch_first=True, bidirectional=True)  \n",
        "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out) \n",
        "        tag_scores = F.log_softmax(tag_space, dim=-1)     \n",
        "        return tag_scores\n",
        "\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word2index), len(tag2index)).cuda()\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "for epoch in range(40):  \n",
        "    loss_now = 0.0\n",
        "    acc = 0\n",
        "\n",
        "    for sentence,targets in train_loader:\n",
        "        sentence = sentence.cuda()\n",
        "        targets = targets.cuda()\n",
        "\n",
        "        model.zero_grad()\n",
        "        model.train()       \n",
        "        tag_scores = model(sentence)\n",
        "\n",
        "        # loss = loss_function(tag_scores, targets)\n",
        "        loss = loss_function(tag_scores.view(-1,tag_scores.size(-1)), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_now+=loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        tag_scores = model(sentence)\n",
        "        _, predicted = torch.max(tag_scores, -1)\n",
        "        prediction = predicted.view(-1).cpu().numpy()\n",
        "        t = targets.view(-1).cpu().numpy()\n",
        "        # Note: The training accuracy here is calculated with \"PAD\", which means most of pos tag will be \"0\".\n",
        "        acc = acc+accuracy_score(prediction,t)*len(prediction) \n",
        "    print('Epoch: %d, training loss: %.4f, training acc: %.2f%%'%(epoch+1,loss_now,100*acc/len(train_sentences_X)/MAX_LENGTH))\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, training loss: 18.7865, training acc: 91.00%\n",
            "Epoch: 2, training loss: 7.3675, training acc: 92.88%\n",
            "Epoch: 3, training loss: 6.0638, training acc: 94.27%\n",
            "Epoch: 4, training loss: 5.0721, training acc: 94.93%\n",
            "Epoch: 5, training loss: 4.2304, training acc: 95.49%\n",
            "Epoch: 6, training loss: 3.5371, training acc: 96.30%\n",
            "Epoch: 7, training loss: 2.9649, training acc: 96.86%\n",
            "Epoch: 8, training loss: 2.5145, training acc: 97.32%\n",
            "Epoch: 9, training loss: 2.1576, training acc: 97.68%\n",
            "Epoch: 10, training loss: 1.8668, training acc: 98.00%\n",
            "Epoch: 11, training loss: 1.6254, training acc: 98.27%\n",
            "Epoch: 12, training loss: 1.4224, training acc: 98.50%\n",
            "Epoch: 13, training loss: 1.2502, training acc: 98.69%\n",
            "Epoch: 14, training loss: 1.1030, training acc: 98.85%\n",
            "Epoch: 15, training loss: 0.9765, training acc: 98.99%\n",
            "Epoch: 16, training loss: 0.8672, training acc: 99.11%\n",
            "Epoch: 17, training loss: 0.7721, training acc: 99.21%\n",
            "Epoch: 18, training loss: 0.6890, training acc: 99.31%\n",
            "Epoch: 19, training loss: 0.6161, training acc: 99.39%\n",
            "Epoch: 20, training loss: 0.5519, training acc: 99.46%\n",
            "Epoch: 21, training loss: 0.4951, training acc: 99.52%\n",
            "Epoch: 22, training loss: 0.4447, training acc: 99.58%\n",
            "Epoch: 23, training loss: 0.4000, training acc: 99.62%\n",
            "Epoch: 24, training loss: 0.3602, training acc: 99.67%\n",
            "Epoch: 25, training loss: 0.3255, training acc: 99.70%\n",
            "Epoch: 26, training loss: 0.3040, training acc: 99.72%\n",
            "Epoch: 27, training loss: 0.2800, training acc: 99.75%\n",
            "Epoch: 28, training loss: 0.2471, training acc: 99.79%\n",
            "Epoch: 29, training loss: 0.2242, training acc: 99.81%\n",
            "Epoch: 30, training loss: 0.2055, training acc: 99.84%\n",
            "Epoch: 31, training loss: 0.1869, training acc: 99.86%\n",
            "Epoch: 32, training loss: 0.1732, training acc: 99.87%\n",
            "Epoch: 33, training loss: 0.1547, training acc: 99.89%\n",
            "Epoch: 34, training loss: 0.1437, training acc: 99.90%\n",
            "Epoch: 35, training loss: 0.1392, training acc: 99.91%\n",
            "Epoch: 36, training loss: 0.1403, training acc: 99.90%\n",
            "Epoch: 37, training loss: 0.1387, training acc: 99.89%\n",
            "Epoch: 38, training loss: 0.1051, training acc: 99.93%\n",
            "Epoch: 39, training loss: 0.0903, training acc: 99.95%\n",
            "Epoch: 40, training loss: 0.0814, training acc: 99.96%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w8nzr_Kq_oA",
        "colab_type": "text"
      },
      "source": [
        "## Test with the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw0HMLN7ou6A",
        "colab_type": "code",
        "outputId": "c09d1d9d-6ac7-4114-fe7e-6e709c0bca69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.eval()\n",
        "sentence = test_sentences_X_pad.cuda()\n",
        "tag_scores = model(sentence)\n",
        "_, predicted = torch.max(tag_scores, -1)\n",
        "predicted = predicted.cpu().numpy()\n",
        "\n",
        "# cut off the PAD part\n",
        "test_len_list = [len(s) for s in test_sentences_X]\n",
        "actual_predicted_list= []\n",
        "for i in range(predicted.shape[0]):\n",
        "    actual_predicted_list+=list(predicted[i])[:test_len_list[i]]\n",
        "\n",
        "# get actual tag list\n",
        "actual_tags = sum(test_tags_y, [])\n",
        "\n",
        "print('Test Accuracy: %.2f%%'%(accuracy_score(actual_predicted_list,actual_tags)*100))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 91.14%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4no4rxxG8ur",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "In this exercise, you are to classify part-of-speech(pos) tags on user-defined sentences using the Bi-LSTM model trained right before the exercise. You should complete the below \"Prediction Result to PoS Tags\" section\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpkj0iSKDSMr",
        "colab_type": "text"
      },
      "source": [
        "## Testing with the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOgn09IucXXA",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess and Predcition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r_eNYQ-0CAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_samples = [\n",
        "    word_tokenize(\"This race is awesome, I want to race too.\"),\n",
        "    word_tokenize(\"That race is silly, I do not want to race.\")\n",
        "]\n",
        "\n",
        "# Converting sentence (tokens) word to index\n",
        "test_samples_X = []\n",
        "for s in test_samples:\n",
        "    s_int = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            s_int.append(word2index[w.lower()])\n",
        "        except KeyError:\n",
        "            s_int.append(word2index['-OOV-'])\n",
        "    test_samples_X.append(s_int)\n",
        "\n",
        "# manually add PAD\n",
        "test_samples_X_pad = []\n",
        "for l in test_samples_X:\n",
        "    test_samples_X_pad.append(l+[0]*(MAX_LENGTH-len(l)))\n",
        "\n",
        "index2tag = {i: t for t, i in tag2index.items()}\n",
        "\n",
        "model.eval()\n",
        "sentence = torch.from_numpy(np.array(test_samples_X_pad)).cuda()\n",
        "predictions = model(sentence)\n",
        "_, predictions = torch.max(predictions, -1)\n",
        "predictions = predictions.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78bmjGiBccFX",
        "colab_type": "text"
      },
      "source": [
        "### Prediction Result to PoS Tags [Complete this part]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiK9WwAdcqwv",
        "colab_type": "code",
        "outputId": "395da1a3-763b-487d-c220-821a5ff85686",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#decode the result to have actual tags\n",
        "def decode_result(predictions, test_samples_X, index2tag):\n",
        "    token_sequences = []\n",
        "    ## write your codes here\n",
        "    # cut off the PAD part\n",
        "    test_len_list = [len(s) for s in test_samples_X]\n",
        "    actual_predicted_list= []\n",
        "    for j in range(len(predictions)):\n",
        "      token_sequences_temp = []\n",
        "      actual_predicted_list.append(list(predictions[j])[:test_len_list[j]])\n",
        "      for i in range(len(actual_predicted_list[j])):\n",
        "         token_sequences_temp.append(sorted(index2tag.items())[actual_predicted_list[j][i]][1])\n",
        "      token_sequences.append(token_sequences_temp)\n",
        "    return token_sequences\n",
        "\n",
        "print(test_samples)\n",
        "print(decode_result(predictions, test_samples_X, index2tag))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['This', 'race', 'is', 'awesome', ',', 'I', 'want', 'to', 'race', 'too', '.'], ['That', 'race', 'is', 'silly', ',', 'I', 'do', 'not', 'want', 'to', 'race', '.']]\n",
            "[['DT', 'NN', 'VBZ', 'VBG', ',', 'PRP', 'VBP', 'TO', 'VB', 'RB', '.'], ['DT', 'NN', 'VBZ', 'VBG', ',', 'PRP', 'VBP', 'RB', 'VB', 'TO', 'CD', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC8eu2Z7cbhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}