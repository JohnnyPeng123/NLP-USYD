{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“Lab08.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/Lab08%20-%20Johnny's%20Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezc9rjh_asVG",
        "colab_type": "text"
      },
      "source": [
        "# Lab08\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kg_YnAymt4i",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "You are required to modify the below example code that can be working with beam search (k > 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zRivr820ULSk"
      },
      "source": [
        "## Neural Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egk2F3luQI5g",
        "colab_type": "text"
      },
      "source": [
        "Now, let's see how to build a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. The objective of this model is to generate new text, given that some input text is present. Lets start building the architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bKTEsdU_ULSm",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIkyodwQO6d",
        "colab_type": "text"
      },
      "source": [
        "Lets use a popular nursery rhyme — “Cat and Her Kittens” as our corpus. A corpus is defined as the collection of text documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhy-wko0TdkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "# Pad sequences to the max length\n",
        "def pad_sequences_pre(input_sequences, maxlen):\n",
        "    output = []\n",
        "    for inp in input_sequences:\n",
        "        if len(inp)< maxlen:\n",
        "            output.append([0]*(maxlen-len(inp)) + inp)\n",
        "        else:\n",
        "            output.append(inp[:maxlen])\n",
        "    return output\n",
        "\n",
        "# Prepare the data\n",
        "def dataset_preparation(data):\n",
        "    corpus = data.lower().split(\"\\n\")\n",
        "    normalized_text=[]\n",
        "    for string in corpus:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "    tokenized_sentences=[sentence.strip().split(\" \") for sentence in normalized_text]\n",
        "\n",
        "    word_list_dict ={}\n",
        "    for sent in tokenized_sentences:\n",
        "        for word in sent:\n",
        "            if word != \"\":\n",
        "                word_list_dict[word] = 1\n",
        "    word_list = list(word_list_dict.keys())\n",
        "    word_to_index = {word:word_list.index(word) for word in word_list}\n",
        "\n",
        "    total_words = len(word_list)+1\n",
        "\n",
        "    # create input sequences using list of tokens\n",
        "    input_sequences = []\n",
        "    for line in tokenized_sentences:\n",
        "        token_list = []\n",
        "        for word in line:\n",
        "            if word!=\"\":\n",
        "                token_list.append(word_to_index[word])\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    # pad sequences \n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences_pre(input_sequences, maxlen=max_sequence_len))\n",
        "\n",
        "    # create predictors and label\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "    return predictors, np.array(label), max_sequence_len, total_words, word_list, word_to_index\n",
        "\n",
        "data = '''The cat and her kittens\n",
        "They put on their mittens\n",
        "To eat a christmas pie\n",
        "The poor little kittens\n",
        "They lost their mittens\n",
        "And then they began to cry.\n",
        "\n",
        "O mother dear, we sadly fear\n",
        "We cannot go to-day,\n",
        "For we have lost our mittens\n",
        "If it be so, ye shall not go\n",
        "For ye are naughty kittens'''\n",
        "\n",
        "predictors, label, max_sequence_len, total_words, word_list, word_to_index = dataset_preparation(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCDUDPold5T8",
        "colab_type": "code",
        "outputId": "8cf5c8d5-4731-4e80-a7ed-8b13b9d0a1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the model\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, total_words):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim_1 = hidden_dim_1\n",
        "        self.hidden_dim_2 = hidden_dim_2\n",
        "        self.word_embeddings = nn.Embedding(total_words, embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim_1, batch_first=True)  \n",
        "        self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)  \n",
        "        self.hidden2tag = nn.Linear(hidden_dim_2, total_words)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out_1, _ = self.lstm1(embeds)\n",
        "        lstm_out_2, _ = self.lstm2(lstm_out_1)\n",
        "        tag_space = self.hidden2tag(lstm_out_2[:,-1,:])                    \n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)      \n",
        "        return tag_scores\n",
        "\n",
        "# Parameter setting\n",
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM_1 = 150\n",
        "HIDDEN_DIM_2 = 100\n",
        "batch_size=predictors.shape[0]\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, total_words).cuda()\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "sentence =torch.from_numpy(predictors).cuda().to(torch.int64)\n",
        "targets = torch.from_numpy(label).cuda().to(torch.int64)\n",
        "\n",
        "\n",
        "# Training\n",
        "for epoch in range(100):  \n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()       \n",
        "    tag_scores = model(sentence)\n",
        "    loss = loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        model.eval()\n",
        "        _, predicted = torch.max(tag_scores, 1)\n",
        "        prediction = predicted.view(-1).cpu().numpy()\n",
        "        t = targets.view(-1).cpu().numpy()\n",
        "        acc = accuracy_score(prediction,t)\n",
        "        print('Epoch: %d, training loss: %.4f, training acc: %.2f%%'%(epoch+1,loss.item(),100*acc))\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, training loss: 3.6694, training acc: 6.25%\n",
            "Epoch: 20, training loss: 3.4622, training acc: 12.50%\n",
            "Epoch: 30, training loss: 3.0190, training acc: 20.83%\n",
            "Epoch: 40, training loss: 2.6104, training acc: 27.08%\n",
            "Epoch: 50, training loss: 2.2231, training acc: 47.92%\n",
            "Epoch: 60, training loss: 1.8765, training acc: 60.42%\n",
            "Epoch: 70, training loss: 1.6554, training acc: 58.33%\n",
            "Epoch: 80, training loss: 1.3766, training acc: 83.33%\n",
            "Epoch: 90, training loss: 1.1904, training acc: 89.58%\n",
            "Epoch: 100, training loss: 1.0295, training acc: 91.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy_B1EX4QRPS",
        "colab_type": "text"
      },
      "source": [
        "The code below only works with k=1, it does not store the candidates. You need to modify the code to make it working with k > 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOpunEmZNoLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b73a1c31-9a2c-4d26-ed72-57be289825c6"
      },
      "source": [
        "# convert index to word\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results\n",
        "def get_topK(predicted, k=1):\n",
        "    \n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "\n",
        "# generate text, currently only works with k=1 \n",
        "\n",
        "# To-Do: modify this function\n",
        "###\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    \n",
        "    for _ in range(next_words):\n",
        "        all_candidates = []\n",
        "\n",
        "        for l in range(len(seed_candidates)):\n",
        "          seed_text, score = seed_candidates[l]\n",
        "          token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "          token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "          \n",
        "          seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "          predicted = model(seed_input).cpu().detach().numpy()\n",
        "          \n",
        "          predicitons = []\n",
        "          for i in range(1,k+1):\n",
        "            id, log_p = get_topK(predicted, k)[i-1]\n",
        "            predicitons.append((id, log_p))\n",
        "          \n",
        "          for j in range(len(predicitons)):\n",
        "            candidate = [seed_text + ' ' + ind_to_word(predicitons[j][0]), score - predicitons[j][1]]  #we are summing up the negative log, so we need to find the minimum score(which is the highest prob)\n",
        "            all_candidates.append(candidate)\n",
        "        \n",
        "        # order all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "        print(ordered)\n",
        "        seed_candidates = ordered[:1]\n",
        "    \n",
        "    return seed_candidates\n",
        "\n",
        "# you can add more function if you want to\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=3))\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['we naughty her', 1.9356861114501953], ['we naughty lost', 2.0805208683013916], ['we naughty are', 2.538965940475464]]\n",
            "[['we naughty her so', 3.3954014778137207], ['we naughty her kittens', 3.825654983520508], ['we naughty her mittens', 3.9835870265960693]]\n",
            "[['we naughty her so mittens', 4.440878868103027], ['we naughty her so our', 4.935371398925781], ['we naughty her so sadly', 5.779910564422607]]\n",
            "[['we naughty her so mittens', 4.440878868103027]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYf-gtT-Uiuh",
        "colab_type": "text"
      },
      "source": [
        "**Sample Output** (Your output would be different, it is based on the trained model)\n",
        "\n",
        "\n",
        "```\n",
        "we naughty lost their mittens\n",
        "```\n",
        "\n"
      ]
    }
  ]
}