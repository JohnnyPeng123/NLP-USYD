{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/Lab08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezc9rjh_asVG",
        "colab_type": "text"
      },
      "source": [
        "# Lab08\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nA_uGHieevZ",
        "colab_type": "text"
      },
      "source": [
        "# Statistical Language Model (SLM)\n",
        "\n",
        "A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability P(w1, ..., wn) to the whole sequence. One solution is to make the assumption that the probability of a word only depends on the previous n words. This is known as an n-gram model or unigram model when n = 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfT2RsFvE3Lc",
        "colab_type": "text"
      },
      "source": [
        "## Bigrams and Trigrams\n",
        "\n",
        "An n-gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a (n − 1)–order Markov model. Using Latin numerical prefixes, an n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\"; size 3 is a \"trigram\". English cardinal numbers are sometimes used, e.g., \"four-gram\", \"five-gram\", and so on.\n",
        "\n",
        "For example, the frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4yjU_UcstUy",
        "colab_type": "text"
      },
      "source": [
        "Let's see how to build a such a model with NLTK "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GJ6LDdq-dB6",
        "colab_type": "code",
        "outputId": "1ef93c4f-5817-4355-e2e0-9819c640c4a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCMRUipsE1q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.corpus import reuters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PmoQ501E_Tz",
        "colab_type": "code",
        "outputId": "3494e70c-5b3a-4627-8217-229bb84d881b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "first_sentence = reuters.sents()[0]\n",
        "first_sentence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASIAN',\n",
              " 'EXPORTERS',\n",
              " 'FEAR',\n",
              " 'DAMAGE',\n",
              " 'FROM',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.-',\n",
              " 'JAPAN',\n",
              " 'RIFT',\n",
              " 'Mounting',\n",
              " 'trade',\n",
              " 'friction',\n",
              " 'between',\n",
              " 'the',\n",
              " 'U',\n",
              " '.',\n",
              " 'S',\n",
              " '.',\n",
              " 'And',\n",
              " 'Japan',\n",
              " 'has',\n",
              " 'raised',\n",
              " 'fears',\n",
              " 'among',\n",
              " 'many',\n",
              " 'of',\n",
              " 'Asia',\n",
              " \"'\",\n",
              " 's',\n",
              " 'exporting',\n",
              " 'nations',\n",
              " 'that',\n",
              " 'the',\n",
              " 'row',\n",
              " 'could',\n",
              " 'inflict',\n",
              " 'far',\n",
              " '-',\n",
              " 'reaching',\n",
              " 'economic',\n",
              " 'damage',\n",
              " ',',\n",
              " 'businessmen',\n",
              " 'and',\n",
              " 'officials',\n",
              " 'said',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97SmMUjnFKPc",
        "colab_type": "code",
        "outputId": "598e47f6-4e0f-47f3-b99a-b8dc935671f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(\"bigrams without pad: \", list(bigrams(first_sentence)))\n",
        "\n",
        "print(\"bigrams with pad: \", list(bigrams(first_sentence, pad_left=True, pad_right=True)))\n",
        "\n",
        "print(\"trigrams without pad: \", list(trigrams(first_sentence)))\n",
        "\n",
        "print(\"trigrams with pad: \", list(trigrams(first_sentence, pad_left=True, pad_right=True)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bigrams without pad:  [('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n",
            "bigrams with pad:  [(None, 'ASIAN'), ('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.'), ('.', None)]\n",
            "trigrams without pad:  [('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n",
            "trigrams with pad:  [(None, None, 'ASIAN'), (None, 'ASIAN', 'EXPORTERS'), ('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.'), ('said', '.', None), ('.', None, None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvl2w18dB_Js",
        "colab_type": "text"
      },
      "source": [
        "NLTK [bigrams()](https://www.nltk.org/api/nltk.html#nltk.util.bigrams),  [trigrams()](https://www.nltk.org/api/nltk.html#nltk.util.trigrams),  [ngrams()](https://www.nltk.org/api/nltk.html#nltk.util.ngrams)\n",
        "\n",
        "Now, let's build a trigram model using the Reuters corpus. Building a bigram model is completely analogous and easier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OBW2sORF1sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a model which contains the trigram counts  \n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzZFpU5-GD66",
        "colab_type": "code",
        "outputId": "7765c7c2-5b8f-4ff8-ccc6-66f48ac7f103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(model[\"what\", \"the\"][\"economists\"])\n",
        "\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "\n",
        "# counts of the sentence starting with \"The\"\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0\n",
            "8839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9AhXXaHGR-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9VxFRs3GVQy",
        "colab_type": "code",
        "outputId": "1cb3c3ee-7688-4a59-d0e1-b0cf3703fe8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(model[\"what\", \"the\"][\"economists\"] )\n",
        "\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "\n",
        "# probabilities of the sentence starting with \"The\"\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.043478260869565216\n",
            "0.0\n",
            "0.16154324146501936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZalKlJptvh33",
        "colab_type": "text"
      },
      "source": [
        "Now you have a tri-gram language model. Let's generate some text. The output text is actually really readable!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBv-NscGe9d",
        "colab_type": "code",
        "outputId": "ff8e43dc-4139-40c3-b828-3b802edfa794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import random\n",
        "\n",
        "text = [None, None]\n",
        " \n",
        "sentence_finished = False\n",
        "\n",
        "# Keep generating the next word until reaching the end of the setence\n",
        "while not sentence_finished:\n",
        "    # Randomly select a probability threshold r\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    # Go through the posible w3 conditioned on current w1 and w2\n",
        "    for word in model[tuple(text[-2:])].keys():\n",
        "        # Accumulate the probability\n",
        "        accumulator += model[tuple(text[-2:])][word]\n",
        " \n",
        "        # When the threshold is reached, use the current w3 as the next word to be generated\n",
        "        if accumulator >= r:\n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "    # If the last two words are None, it will reach the end and stop generating\n",
        "    if text[-2:] == [None, None]:\n",
        "        sentence_finished = True\n",
        "\n",
        "# The generated sentence are as follows\n",
        "' '.join([t for t in text if t])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The semi - processed leather , the oldest site currently operated by Michaels , who is under - utilised because of different countries .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ9YDhpza29T",
        "colab_type": "text"
      },
      "source": [
        "# Decoding Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgP7L9aowkD1",
        "colab_type": "text"
      },
      "source": [
        "NLP tasks, such as chatbot, text summarization, and machine translation, the prediction required is a sequence of words.\n",
        "\n",
        "It is common for models developed for these types of problems to output a probability distribution over each word in the vocabulary for each word in the output sequence. It is then left to a decoder process to transform the probabilities into a final sequence of words.\n",
        "\n",
        "Decoding the most likely output sequence involves searching through all the possible output sequences based on their likelihood. The size of the vocabulary is often tens or hundreds of thousands of words, or even millions of words. Therefore, the search problem is exponential in the length of the output sequence and is intractable (NP-complete) to search completely.\n",
        "\n",
        "In practice, heuristic search methods are used to return one or more approximate or “good enough” decoded output sequences for a given prediction.\n",
        "\n",
        "Candidate sequences of words are scored based on their likelihood. It is common to use a greedy search or a beam search to locate candidate sequences of text. We will look at both of these decoding algorithms in this post."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3q9CKB5arfc",
        "colab_type": "text"
      },
      "source": [
        "## Greedy Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRs0dBDXv-3U",
        "colab_type": "text"
      },
      "source": [
        "A simple approximation is to use a greedy search that selects the most likely word at each step in the output sequence. This approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal.\n",
        "\n",
        "We can demonstrate the greedy search approach to decoding with a small contrived example in Python. We can start off with a prediction problem that involves a sequence of 10 words. Each word is predicted as a probability distribution over a vocabulary of 5 words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9bVjguBJksg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SthjwprJfaE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a sequence of 10 words over a vocab of 5 words\n",
        "data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
        "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "        [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
        "data = array(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82tjZn5SwHks",
        "colab_type": "text"
      },
      "source": [
        "We will assume that the words have been integer encoded, such that the column index can be used to look-up the associated word in the vocabulary. Therefore, the task of decoding becomes the task of selecting a sequence of integers from the probability distributions.\n",
        "\n",
        "The argmax() mathematical function can be used to select the index of an array that has the largest value. We can use this function to select the word index that is most likely at each step in the sequence. This function is provided directly in numpy.\n",
        "\n",
        "The greedy_decoder() function below implements this decoder strategy using the argmax function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9o2t8bjJveU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# greedy decoder, pick only \n",
        "def greedy_decoder(data):\n",
        "    # index for largest probability each row\n",
        "    return [argmax(s) for s in data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ccJuMoYwVFR",
        "colab_type": "text"
      },
      "source": [
        "Running the example outputs a sequence of integers that could then be mapped back to words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbo4v9KeJ6um",
        "colab_type": "code",
        "outputId": "82b6929f-7105-4f82-e90b-49bbe990fea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#decode seqeunce\n",
        "result = greedy_decoder(data)\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 0, 4, 0, 4, 0, 4, 0, 4, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o46exoAnKN80",
        "colab_type": "text"
      },
      "source": [
        "## Beamsearch Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz5tkvgQw7_K",
        "colab_type": "text"
      },
      "source": [
        "Another popular heuristic is the beam search that expands upon the greedy search and returns a list of most likely output sequences.\n",
        "\n",
        "Instead of greedily choosing the most likely next step as the sequence is constructed, the beam search expands all possible next steps and keeps the k most likely, where k is a user-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities.\n",
        "\n",
        "We do not need to start with random states; instead, we start with the k most likely words as the first step in the sequence. Common beam width values are 1 for a greedy search and values of 5 or 10 for common benchmark problems in machine translation. Larger beam widths result in better performance of a model as the multiple candidate sequences increase the likelihood of better matching a target sequence. This increased performance results in a decrease in decoding speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX8CkPBLKZ3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4qTw0jQxVJV",
        "colab_type": "text"
      },
      "source": [
        "The beam_search_decoder() function below implements the beam search decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ5EFT3nKQSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# beam search\n",
        "def beam_search_decoder(data, k):\n",
        "    sequences = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for step,row in enumerate(data):\n",
        "        all_candidates = list()\n",
        "        # expand each current candidate\n",
        "        for i in range(len(sequences)):\n",
        "            seq, score = sequences[i]\n",
        "            for j in range(len(row)):\n",
        "                candidate = [seq + [j], score + (-log(row[j])) ]  #we are summing up the negative log, so we need to find the minimum score(which is the highest prob)\n",
        "                all_candidates.append(candidate)\n",
        "        # order all candidates by score\n",
        "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "\n",
        "        # select k best\n",
        "        sequences = ordered[:k]\n",
        "        \n",
        "        # display the k-best sequences\n",
        "        print(\"The\", str(k), \"best sequences at step \", str(step), \": \")\n",
        "        print(sequences)\n",
        "        print()\n",
        "\n",
        "    return sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-Rro25Dxbu0",
        "colab_type": "text"
      },
      "source": [
        "We can tie this together with the sample data from the previous section and this time return the 3 most likely sequences. Running the example prints both the integer sequences and their log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO-UQOwuKUh6",
        "colab_type": "code",
        "outputId": "3798605f-0a12-4d1e-b88e-1de8e44bab1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "# decode sequence\n",
        "result = beam_search_decoder(data, 3)\n",
        "\n",
        "\n",
        "print()\n",
        "print(\"The final decoded 3 best sequences: \")\n",
        "for seq in result:\n",
        "    print(seq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The 3 best sequences at step  0 : \n",
            "[[[4], 0.6931471805599453], [[3], 0.916290731874155], [[2], 1.2039728043259361]]\n",
            "\n",
            "The 3 best sequences at step  1 : \n",
            "[[[4, 0], 1.3862943611198906], [[4, 1], 1.6094379124341003], [[3, 0], 1.6094379124341003]]\n",
            "\n",
            "The 3 best sequences at step  2 : \n",
            "[[[4, 0, 4], 2.0794415416798357], [[4, 0, 3], 2.3025850929940455], [[4, 1, 4], 2.3025850929940455]]\n",
            "\n",
            "The 3 best sequences at step  3 : \n",
            "[[[4, 0, 4, 0], 2.772588722239781], [[4, 0, 4, 1], 2.995732273553991], [[4, 0, 3, 0], 2.995732273553991]]\n",
            "\n",
            "The 3 best sequences at step  4 : \n",
            "[[[4, 0, 4, 0, 4], 3.4657359027997265], [[4, 0, 4, 0, 3], 3.6888794541139363], [[4, 0, 4, 1, 4], 3.6888794541139363]]\n",
            "\n",
            "The 3 best sequences at step  5 : \n",
            "[[[4, 0, 4, 0, 4, 0], 4.1588830833596715], [[4, 0, 4, 0, 4, 1], 4.382026634673881], [[4, 0, 4, 0, 3, 0], 4.382026634673881]]\n",
            "\n",
            "The 3 best sequences at step  6 : \n",
            "[[[4, 0, 4, 0, 4, 0, 4], 4.852030263919617], [[4, 0, 4, 0, 4, 0, 3], 5.075173815233827], [[4, 0, 4, 0, 4, 1, 4], 5.075173815233827]]\n",
            "\n",
            "The 3 best sequences at step  7 : \n",
            "[[[4, 0, 4, 0, 4, 0, 4, 0], 5.545177444479562], [[4, 0, 4, 0, 4, 0, 4, 1], 5.768320995793772], [[4, 0, 4, 0, 4, 0, 3, 0], 5.768320995793772]]\n",
            "\n",
            "The 3 best sequences at step  8 : \n",
            "[[[4, 0, 4, 0, 4, 0, 4, 0, 4], 6.238324625039508], [[4, 0, 4, 0, 4, 0, 4, 0, 3], 6.461468176353717], [[4, 0, 4, 0, 4, 0, 4, 1, 4], 6.461468176353717]]\n",
            "\n",
            "The 3 best sequences at step  9 : \n",
            "[[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.931471805599453], [[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154615356913663], [[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154615356913663]]\n",
            "\n",
            "\n",
            "The final decoded 3 best sequences: \n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 0], 6.931471805599453]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 4, 1], 7.154615356913663]\n",
            "[[4, 0, 4, 0, 4, 0, 4, 0, 3, 0], 7.154615356913663]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kg_YnAymt4i",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "You are required to modify the below example code that can be working with beam search (k > 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zRivr820ULSk"
      },
      "source": [
        "## Neural Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egk2F3luQI5g",
        "colab_type": "text"
      },
      "source": [
        "Now, let's see how to build a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. The objective of this model is to generate new text, given that some input text is present. Lets start building the architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bKTEsdU_ULSm",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFIkyodwQO6d",
        "colab_type": "text"
      },
      "source": [
        "Lets use a popular nursery rhyme — “Cat and Her Kittens” as our corpus. A corpus is defined as the collection of text documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhy-wko0TdkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "# Pad sequences to the max length\n",
        "def pad_sequences_pre(input_sequences, maxlen):\n",
        "    output = []\n",
        "    for inp in input_sequences:\n",
        "        if len(inp)< maxlen:\n",
        "            output.append([0]*(maxlen-len(inp)) + inp)\n",
        "        else:\n",
        "            output.append(inp[:maxlen])\n",
        "    return output\n",
        "\n",
        "# Prepare the data\n",
        "def dataset_preparation(data):\n",
        "    corpus = data.lower().split(\"\\n\")\n",
        "    normalized_text=[]\n",
        "    for string in corpus:\n",
        "        tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "        normalized_text.append(tokens)\n",
        "    tokenized_sentences=[sentence.strip().split(\" \") for sentence in normalized_text]\n",
        "\n",
        "    word_list_dict ={}\n",
        "    for sent in tokenized_sentences:\n",
        "        for word in sent:\n",
        "            if word != \"\":\n",
        "                word_list_dict[word] = 1\n",
        "    word_list = list(word_list_dict.keys())\n",
        "    word_to_index = {word:word_list.index(word) for word in word_list}\n",
        "\n",
        "    total_words = len(word_list)+1\n",
        "\n",
        "    # create input sequences using list of tokens\n",
        "    input_sequences = []\n",
        "    for line in tokenized_sentences:\n",
        "        token_list = []\n",
        "        for word in line:\n",
        "            if word!=\"\":\n",
        "                token_list.append(word_to_index[word])\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    # pad sequences \n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences_pre(input_sequences, maxlen=max_sequence_len))\n",
        "\n",
        "    # create predictors and label\n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "    return predictors, np.array(label), max_sequence_len, total_words, word_list, word_to_index\n",
        "\n",
        "data = '''The cat and her kittens\n",
        "They put on their mittens\n",
        "To eat a christmas pie\n",
        "The poor little kittens\n",
        "They lost their mittens\n",
        "And then they began to cry.\n",
        "\n",
        "O mother dear, we sadly fear\n",
        "We cannot go to-day,\n",
        "For we have lost our mittens\n",
        "If it be so, ye shall not go\n",
        "For ye are naughty kittens'''\n",
        "\n",
        "predictors, label, max_sequence_len, total_words, word_list, word_to_index = dataset_preparation(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCDUDPold5T8",
        "colab_type": "code",
        "outputId": "90bc6498-eaa4-4170-e133-c1553bbcae68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the model\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim_1, hidden_dim_2, total_words):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim_1 = hidden_dim_1\n",
        "        self.hidden_dim_2 = hidden_dim_2\n",
        "        self.word_embeddings = nn.Embedding(total_words, embedding_dim)\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim_1, batch_first=True)  \n",
        "        self.lstm2 = nn.LSTM(hidden_dim_1, hidden_dim_2, batch_first=True)  \n",
        "        self.hidden2tag = nn.Linear(hidden_dim_2, total_words)\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out_1, _ = self.lstm1(embeds)\n",
        "        lstm_out_2, _ = self.lstm2(lstm_out_1)\n",
        "        tag_space = self.hidden2tag(lstm_out_2[:,-1,:])                    \n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)      \n",
        "        return tag_scores\n",
        "\n",
        "# Parameter setting\n",
        "EMBEDDING_DIM = 10\n",
        "HIDDEN_DIM_1 = 150\n",
        "HIDDEN_DIM_2 = 100\n",
        "batch_size=predictors.shape[0]\n",
        "\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM_1, HIDDEN_DIM_2, total_words).cuda()\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "sentence =torch.from_numpy(predictors).cuda().to(torch.int64)\n",
        "targets = torch.from_numpy(label).cuda().to(torch.int64)\n",
        "\n",
        "\n",
        "# Training\n",
        "for epoch in range(100):  \n",
        "\n",
        "    model.train()\n",
        "    model.zero_grad()       \n",
        "    tag_scores = model(sentence)\n",
        "    loss = loss_function(tag_scores, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        model.eval()\n",
        "        _, predicted = torch.max(tag_scores, 1)\n",
        "        prediction = predicted.view(-1).cpu().numpy()\n",
        "        t = targets.view(-1).cpu().numpy()\n",
        "        acc = accuracy_score(prediction,t)\n",
        "        print('Epoch: %d, training loss: %.4f, training acc: %.2f%%'%(epoch+1,loss.item(),100*acc))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, training loss: 3.6649, training acc: 8.33%\n",
            "Epoch: 20, training loss: 3.4763, training acc: 12.50%\n",
            "Epoch: 30, training loss: 3.0723, training acc: 12.50%\n",
            "Epoch: 40, training loss: 2.6599, training acc: 33.33%\n",
            "Epoch: 50, training loss: 2.2659, training acc: 52.08%\n",
            "Epoch: 60, training loss: 1.9666, training acc: 50.00%\n",
            "Epoch: 70, training loss: 1.6831, training acc: 72.92%\n",
            "Epoch: 80, training loss: 1.4315, training acc: 81.25%\n",
            "Epoch: 90, training loss: 1.2313, training acc: 85.42%\n",
            "Epoch: 100, training loss: 1.0517, training acc: 91.67%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy_B1EX4QRPS",
        "colab_type": "text"
      },
      "source": [
        "The code below only works with k=1, it does not store the candidates. You need to modify the code to make it working with k > 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOpunEmZNoLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert index to word\n",
        "def ind_to_word(predicted_ind):\n",
        "    for word, index in word_to_index.items():\n",
        "        if index == predicted_ind:\n",
        "            return word\n",
        "    return \"\"    \n",
        "\n",
        "\n",
        "# get the top k most predicted results\n",
        "def get_topK(predicted, k=1):\n",
        "    \n",
        "    top_k = np.argsort(predicted[0])[-k:]\n",
        "\n",
        "    return [(id, predicted[0][id]) for id in top_k]\n",
        "\n",
        "\n",
        "# generate text, currently only works with k=1 \n",
        "\n",
        "# To-Do: modify this function\n",
        "###\n",
        "def generate_text(seed_text, next_words, max_sequence_len, k=1):\n",
        "\n",
        "    \n",
        "    seed_candidates = [(seed_text, .0)]\n",
        "    for _ in range(next_words):\n",
        "        successives = []\n",
        "\n",
        "        seed_text, score = seed_candidates[0]\n",
        "        token_list = [word_to_index[word] for word in seed_text.split()]\n",
        "        token_list = pad_sequences_pre([token_list], maxlen=max_sequence_len-1)\n",
        "        \n",
        "        seed_input = torch.from_numpy(np.array(token_list)).cuda().to(torch.int64)\n",
        "        predicted = model(seed_input).cpu().detach().numpy()\n",
        "\n",
        "        id, log_p = get_topK(predicted, k)[0]\n",
        "        output_word = ind_to_word(id)\n",
        "        successives.append((seed_text + ' ' + output_word, score - log_p))    \n",
        "        seed_candidates = successives\n",
        "    return seed_candidates[0][0]\n",
        "\n",
        "\n",
        "\n",
        "# you can add more function if you want to\n",
        "\n",
        "print(generate_text(\"we naughty\", 3, max_sequence_len, k=1))\n",
        "# print(generate_text(\"we naughty\", 3, max_sequence_len, k=3))\n",
        "\n",
        "# Please note that it can happen that k=1 and k=3 have the same output because this is only a small dataset.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYf-gtT-Uiuh",
        "colab_type": "text"
      },
      "source": [
        "**Sample Output** (Your output would be different, it is based on the trained model)\n",
        "\n",
        "\n",
        "```\n",
        "we naughty lost their mittens\n",
        "```\n",
        "\n"
      ]
    }
  ]
}