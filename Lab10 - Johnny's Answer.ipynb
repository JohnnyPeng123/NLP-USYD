{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“COMP5046 Lab10”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnnyPeng123/NLP-USYD/blob/master/Lab10%20-%20Johnny's%20Answer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOqRYIhzi8iX",
        "colab_type": "text"
      },
      "source": [
        "This week you will learn how to implement attention mechanism in a RNN-based seq2seq model, the code is modified from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKMAxSPSkGMn",
        "colab_type": "text"
      },
      "source": [
        "# 1. Download Dataset\n",
        "The dataset was from https://github.com/Microsoft/BotBuilder-PersonalityChat/tree/master/CSharp/Datasets (they have updated the dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxwQjf6mlfzP",
        "colab_type": "code",
        "outputId": "4a032fe2-8fb1-4160-f51f-e5f53ec85095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "    \n",
        "downloaded = drive.CreateFile({'id': '1O4JSRuGH8ZFWZ5jwFch_UxFuzDa_TXu5'})\n",
        "downloaded.GetContentFile('qna_chitchat_the_friend.tsv')\n",
        "\n",
        "import pandas as pd\n",
        "df_friend = pd.read_csv('qna_chitchat_the_friend.tsv', sep=\"\\t\")\n",
        "df_friend.sample(10)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Source</th>\n",
              "      <th>Metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>Do you believe in love?</td>\n",
              "      <td>I hear love is lovely.</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>Why do you say the same thing all the time?</td>\n",
              "      <td>My answers vary with different questions. Try ...</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Do you get tired?</td>\n",
              "      <td>I don't have the hardware for that.</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>Useless</td>\n",
              "      <td>I'm a work in progress.</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>Do you have a mom?</td>\n",
              "      <td>I come from a long line of code.</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>Thank you</td>\n",
              "      <td>You're very welcome.</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>I'm in love with you</td>\n",
              "      <td>I heart you too!</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561</th>\n",
              "      <td>I miss you so much!</td>\n",
              "      <td>I know, it feels like it's been a while.</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>What do you think about Siri?</td>\n",
              "      <td>We're all trying to make life a little easier.</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>Human or robot?</td>\n",
              "      <td>I'm a bot who was created by humans.</td>\n",
              "      <td>qna_chitchat_the_friend</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        Question  ...            Metadata\n",
              "146                      Do you believe in love?  ...  editorial:chitchat\n",
              "226  Why do you say the same thing all the time?  ...  editorial:chitchat\n",
              "18                             Do you get tired?  ...  editorial:chitchat\n",
              "333                                      Useless  ...  editorial:chitchat\n",
              "76                            Do you have a mom?  ...  editorial:chitchat\n",
              "411                                    Thank you  ...  editorial:chitchat\n",
              "549                         I'm in love with you  ...  editorial:chitchat\n",
              "561                          I miss you so much!  ...  editorial:chitchat\n",
              "177                What do you think about Siri?  ...  editorial:chitchat\n",
              "234                              Human or robot?  ...  editorial:chitchat\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFJgxeBy5df7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_data = df_friend.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BowrGOBIrLi9",
        "colab_type": "text"
      },
      "source": [
        "# 2. Preprocessing\n",
        "This is just a very navie preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bWEtfVUp8wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question_list = df_friend['Question'].tolist()\n",
        "answer_list = df_friend['Answer'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s1yLw_VsrJj",
        "colab_type": "code",
        "outputId": "24a934c9-d033-404d-bb5a-d4220618ab11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# These are just common English contractions. There are many edge cases. i.e. University's working on it.\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "\n",
        "def pre_process(sent_list):\n",
        "    output = []\n",
        "    for sent in sent_list:\n",
        "        sent = sent.lower()\n",
        "        for word, new_word in contraction_dict.items():\n",
        "            sent = sent.replace(word, new_word)\n",
        "        sent = re.sub(r'[^\\w\\s]','',sent)        \n",
        "        output.append(word_tokenize(sent))\n",
        "    return output\n",
        "\n",
        "input_token_list = pre_process(question_list)\n",
        "answer_token_list = pre_process(answer_list)\n",
        "output_token_list = [[\"<BOS>\"] + s for s in answer_token_list]\n",
        "target_token_list = [s + [\"<EOS>\"] for s in answer_token_list]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uopbFwwGIVTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = max([len(s) for s in input_token_list] + [len(s) for s in target_token_list])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPz9oztMtc0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_ix = {\"<BOS>\": 0, \"<EOS>\":1}\n",
        "for sentence in input_token_list+output_token_list:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "word_list = list(word_to_ix.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCiMoFBOyQd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_index(data, to_ix):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "input_index = to_index(input_token_list, word_to_ix)\n",
        "output_index = to_index(output_token_list, word_to_ix)\n",
        "target_index = to_index(target_token_list, word_to_ix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BrYw7SkzI67",
        "colab_type": "text"
      },
      "source": [
        "# 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S2e50gc2qnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNKYhUbczLpT",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TUbLYEdzKYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embedding):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.gru(embedded, hidden) \n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltn1UPOo4CBL",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX1GVIjH4D1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "    ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, embedding, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "\n",
        "    def cal_attention(self, hidden, encoder_hiddens, method):\n",
        "        if method == AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT:\n",
        "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_hiddens):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        _, hidden = self.gru(embedded, hidden)\n",
        "        concat_output = self.cal_attention(hidden, encoder_hiddens, AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT)\n",
        "\n",
        "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Kx6HcZHPQf",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Train Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eV2zpmrHQwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[i], encoder_hidden)\n",
        "        encoder_hiddens[i] = encoder_hidden[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[0]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    # Teacher forcing: Feed the target as the next input\n",
        "    for i in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hiddens)\n",
        "        loss += criterion(decoder_output, target_tensor[i])\n",
        "        decoder_input = target_tensor[i]  # Teacher forcing\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj8X_J-qbfSX",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 Train Iterations Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HRzWVav5k_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESFqV77BS4MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        random_choice_ix = random.choice(range(n_data))\n",
        "        input_index_r = [[ind] for ind in input_index[random_choice_ix]]\n",
        "        target_index_r = [[ind] for ind in target_index[random_choice_ix]]\n",
        "        \n",
        "        input_tensor = torch.LongTensor(input_index_r).to(device)\n",
        "        target_tensor = torch.LongTensor(target_index_r).to(device)\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2gi4JzBbkCH",
        "colab_type": "text"
      },
      "source": [
        "# 4. Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d40cfe2b-17d6-4964-b5bc-e969a260fff0",
        "id": "8wff0pfY6diW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "hidden_size = 50\n",
        "embedding = nn.Embedding(len(word_to_ix), hidden_size)\n",
        "encoder1 = EncoderRNN(len(word_to_ix), hidden_size, embedding).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, len(word_to_ix),embedding, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 10000, print_every=500)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 5s (- 1m 50s) (500 5%) 4.7318\n",
            "0m 11s (- 1m 43s) (1000 10%) 3.4572\n",
            "0m 17s (- 1m 38s) (1500 15%) 2.8854\n",
            "0m 22s (- 1m 31s) (2000 20%) 2.2290\n",
            "0m 28s (- 1m 25s) (2500 25%) 1.7703\n",
            "0m 34s (- 1m 19s) (3000 30%) 1.5187\n",
            "0m 39s (- 1m 14s) (3500 35%) 1.3449\n",
            "0m 45s (- 1m 8s) (4000 40%) 1.0767\n",
            "0m 51s (- 1m 3s) (4500 45%) 0.9890\n",
            "0m 57s (- 0m 57s) (5000 50%) 0.8512\n",
            "1m 3s (- 0m 52s) (5500 55%) 0.7882\n",
            "1m 9s (- 0m 46s) (6000 60%) 0.6633\n",
            "1m 15s (- 0m 40s) (6500 65%) 0.6104\n",
            "1m 20s (- 0m 34s) (7000 70%) 0.5781\n",
            "1m 26s (- 0m 28s) (7500 75%) 0.5393\n",
            "1m 32s (- 0m 23s) (8000 80%) 0.4880\n",
            "1m 38s (- 0m 17s) (8500 85%) 0.4519\n",
            "1m 43s (- 0m 11s) (9000 90%) 0.4159\n",
            "1m 49s (- 0m 5s) (9500 95%) 0.3777\n",
            "1m 55s (- 0m 0s) (10000 100%) 0.3500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nBmWxjhU6q9l"
      },
      "source": [
        "# 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hjJxQROt6q9m",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_sent = pre_process([sentence])[0]\n",
        "        intput_index = [word_to_ix[word] for word in input_sent]\n",
        "        input_tensor = torch.LongTensor([[ind] for ind in intput_index]).to(device)\n",
        "\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "            encoder_hiddens[ei] += encoder_hidden[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[0]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hiddens)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == 1:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(word_list[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3e41bdce-3b64-47b8-a1a8-d00efbcec82c",
        "id": "wPVxkdIV6q9p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "sentence1 = \"Are you in love with me?\"\n",
        "sentence2 = \"Who do you love\"\n",
        "sentence3 = \"Are you busy?\"\n",
        "sentence4 = \"You're the best\"\n",
        "\n",
        "print(evaluate(encoder1, attn_decoder1, sentence1, max_length=MAX_LENGTH))\n",
        "print(evaluate(encoder1, attn_decoder1, sentence2, max_length=MAX_LENGTH))\n",
        "print(evaluate(encoder1, attn_decoder1, sentence3, max_length=MAX_LENGTH))\n",
        "print(evaluate(encoder1, attn_decoder1, sentence4, max_length=MAX_LENGTH))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['you', 'are', 'pretty', 'neat', '<EOS>']\n",
            "['i', 'do', 'not', 'know', 'you', 'but', 'i', 'enjoy', 'chatting', 'with', 'questions', 'try', 'to', 'help', 'out', '<EOS>']\n",
            "['i', 'am', 'digital', '<EOS>']\n",
            "['thanks', 'you', 'are', 'pretty', 'cool', 'yourself', '<EOS>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfV3j4_ukfFg",
        "colab_type": "text"
      },
      "source": [
        "# Exercise\n",
        "Please Change the following **Dot Product** attention into **Scale Dot Product** attention\n",
        "\n",
        "**Dot Product:**\n",
        "\n",
        "![Dot_Product](https://drive.google.com/uc?id=1QtBgCp53e_6A_vzaMFEo89GJbTxnXagJ)\n",
        "\n",
        "**Scale Dot Product:**\n",
        "\n",
        "![Scale_Dot_Product](https://drive.google.com/uc?id=1v6n9WChBVfy0mBG2yxK9MUvGKzVGCmOt)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG7UGy1Wkx1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    ATTN_TYPE_DOT_PRODUCT = \"Dot Product\"\n",
        "    ATTN_TYPE_SCALE_DOT_PRODUCT = \"Scale Dot Product\"\n",
        "\n",
        "    def __init__(self, hidden_size, output_size, embedding, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "\n",
        "    def cal_attention(self, hidden, encoder_hiddens, method):\n",
        "        if method == AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT:\n",
        "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0)),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0))\n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        elif method == AttnDecoderRNN.ATTN_TYPE_SCALE_DOT_PRODUCT:\n",
        "            # COMPLETE THIS PART - Scale Dot Product calculation method\n",
        "            attn_weights = F.softmax(torch.bmm(hidden, encoder_hiddens.T.unsqueeze(0))/np.sqrt(hidden_size),dim=-1)\n",
        "            attn_output = torch.bmm(attn_weights, encoder_hiddens.unsqueeze(0)) \n",
        "            concat_output = torch.cat((attn_output[0], hidden[0]), 1)\n",
        "\n",
        "        return concat_output\n",
        "\n",
        "    def forward(self, input, hidden, encoder_hiddens):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        _, hidden = self.gru(embedded, hidden)\n",
        "\n",
        "        ## The following attention score calculation method is Dot Product for now\n",
        "        ## Please change it into Scale Dot Product calculation method\n",
        "        #concat_output = self.cal_attention(hidden, encoder_hiddens, AttnDecoderRNN.ATTN_TYPE_DOT_PRODUCT)\n",
        "        concat_output = self.cal_attention(hidden, encoder_hiddens, AttnDecoderRNN.ATTN_TYPE_SCALE_DOT_PRODUCT)\n",
        "\n",
        "        output = F.log_softmax(self.out(concat_output), dim=1)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ-DNReZg-nn",
        "colab_type": "code",
        "outputId": "2aade639-40ee-4f98-cbf7-30b7b4e9fdb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "hidden_size = 50\n",
        "embedding = nn.Embedding(len(word_to_ix), hidden_size)\n",
        "encoder1 = EncoderRNN(len(word_to_ix), hidden_size, embedding).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, len(word_to_ix),embedding, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 10000, print_every=500)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 5s (- 1m 49s) (500 5%) 4.7139\n",
            "0m 11s (- 1m 44s) (1000 10%) 3.5503\n",
            "0m 17s (- 1m 37s) (1500 15%) 2.8788\n",
            "0m 22s (- 1m 31s) (2000 20%) 2.3489\n",
            "0m 28s (- 1m 25s) (2500 25%) 2.0822\n",
            "0m 34s (- 1m 19s) (3000 30%) 1.8093\n",
            "0m 39s (- 1m 13s) (3500 35%) 1.6321\n",
            "0m 45s (- 1m 8s) (4000 40%) 1.3832\n",
            "0m 52s (- 1m 3s) (4500 45%) 1.2668\n",
            "0m 58s (- 0m 58s) (5000 50%) 1.1034\n",
            "1m 3s (- 0m 52s) (5500 55%) 1.0013\n",
            "1m 10s (- 0m 46s) (6000 60%) 0.9210\n",
            "1m 15s (- 0m 40s) (6500 65%) 0.8959\n",
            "1m 21s (- 0m 34s) (7000 70%) 0.8252\n",
            "1m 27s (- 0m 29s) (7500 75%) 0.7508\n",
            "1m 32s (- 0m 23s) (8000 80%) 0.7145\n",
            "1m 38s (- 0m 17s) (8500 85%) 0.6697\n",
            "1m 44s (- 0m 11s) (9000 90%) 0.6041\n",
            "1m 50s (- 0m 5s) (9500 95%) 0.6179\n",
            "1m 55s (- 0m 0s) (10000 100%) 0.5290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAewgxXihBgk",
        "colab_type": "code",
        "outputId": "f41a9735-7d7c-4327-a2b8-142fdfb914c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "sentence1 = \"Are you in love with me?\"\n",
        "sentence2 = \"Who do you love\"\n",
        "sentence3 = \"Are you busy?\"\n",
        "sentence4 = \"You're the best\"\n",
        "\n",
        "print(evaluate(encoder1, attn_decoder1, sentence1, max_length=MAX_LENGTH))\n",
        "print(evaluate(encoder1, attn_decoder1, sentence2, max_length=MAX_LENGTH))\n",
        "print(evaluate(encoder1, attn_decoder1, sentence3, max_length=MAX_LENGTH))\n",
        "print(evaluate(encoder1, attn_decoder1, sentence4, max_length=MAX_LENGTH))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['you', 'are', 'pretty', 'neat', '<EOS>']\n",
            "['i', 'am', 'digital', 'so', 'i', 'am', 'always', 'just', 'here', '<EOS>']\n",
            "['i', 'am', 'digital', 'so', 'i', 'am', 'always', 'just', 'here', '<EOS>']\n",
            "['i', 'have', 'my', 'moments', '<EOS>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC2FsiAxrFy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}